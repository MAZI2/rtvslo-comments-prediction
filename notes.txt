time dilemma!!!!


lstm

contextual relationships ... attention ... last 6 hours ...

lstms as article sequence is not necessarily independent.

3. Create special time-conditional tokens (for text)




-----

TODO: batch size
TODO: embedding token size / other embeddings
TODO: koren stevila komentarjev ali pa kaj drugega

How to ensemble this with the log-linear model for even better performance?

Ensemble 5 trained models (diff random seeds) and average predictions — it will reduce MAE


----
1. Target Transformation (you're already doing this)

Why it works:

You observed overdispersion + skew.
Applying log1p(y) compresses large values and improves gradient flow.
This exploits the heavy-tailed distribution of your data.
✅ Already helping you — you are exploiting distribution here.

Priors

baseline

a sem jaz log1p napovedoval

visualization

weighting

how were the splits performed

✅ Saved best model for cluster 0 (Val MAE: 24.56)
✅ Saved best model for cluster 1 (Val MAE: 31.52)
✅ Saved best model for cluster 2 (Val MAE: 4.35)
✅ Saved best model for cluster 3 (Val MAE: 19.04)
✅ Saved best model for cluster 4 (Val MAE: 40.53)
✅ Saved best model for cluster 5 (Val MAE: 25.34)
✅ Saved best model for cluster 6 (Val MAE: 42.24)
✅ Saved best model for cluster 7 (Val MAE: 15.04)
✅ Saved best model for cluster 8 (Val MAE: 2.44)
✅ Saved best model for cluster 9 (Val MAE: 7.94)
✅ Saved best model for cluster 10 (Val MAE: 19.78)
✅ Saved best model for cluster 11 (Val MAE: 24.13)
✅ Saved best model for cluster 12 (Val MAE: 13.16)
✅ Saved best model for cluster 13 (Val MAE: 8.68)
✅ Saved best model for cluster 14 (Val MAE: 15.76)


You're using UMAP → KMeans to create semantically-coherent clusters. However:

    Effect: Some clusters may:

        Group articles semantically, but not behaviorally (e.g., no relation in comment patterns).

        Be too small or too broad, harming predictive power.