
CLUSTERING
You're using UMAP → KMeans to create semantically-coherent clusters. However:
    Effect: Some clusters may:
        Group articles semantically, but not behaviorally (e.g., no relation in comment patterns).
        Be too small or too broad, harming predictive power.

✅ Saved best model for cluster 0 (Val MAE: 24.56)
✅ Saved best model for cluster 1 (Val MAE: 31.52)
✅ Saved best model for cluster 2 (Val MAE: 4.35)
✅ Saved best model for cluster 3 (Val MAE: 19.04)
✅ Saved best model for cluster 4 (Val MAE: 40.53)
✅ Saved best model for cluster 5 (Val MAE: 25.34)
✅ Saved best model for cluster 6 (Val MAE: 42.24)
✅ Saved best model for cluster 7 (Val MAE: 15.04)
✅ Saved best model for cluster 8 (Val MAE: 2.44)
✅ Saved best model for cluster 9 (Val MAE: 7.94)
✅ Saved best model for cluster 10 (Val MAE: 19.78)
✅ Saved best model for cluster 11 (Val MAE: 24.13)
✅ Saved best model for cluster 12 (Val MAE: 13.16)
✅ Saved best model for cluster 13 (Val MAE: 8.68)
✅ Saved best model for cluster 14 (Val MAE: 15.76)





HYPERPARAMETERS:
batch size
embedding token size / other embeddings
koren stevila komentarjev ali pa kaj drugega

VZORCI V PODATKIH
- podatki dovolj noisy da katerkoly steer NNja s predobdelavami škoduje
- značilnosti se skrivajo v zlo specifičnih encodingih. Preclustering škoduje. Za podatke niso značilne semantične skupine ... so pa značilne tematsko razdeljene
- priors škodujejo
- You observed overdispersion + skew.
    - Applying log1p(y) compresses large values and improves gradient flow.
        This exploits the heavy-tailed distribution of your data.


TODO:
    try topic multimodel
    try outlier avoidance?

- how were the splits performed

ZINB
learned time embeddings not working
time is very non descriptive here. The unnatural scaling helps

on best predictions the adj helps quite a lot. On 26.15 model it doesnt. Compare the distribution
quantile smoothing