time dilemma!!!!


lstm

contextual relationships ... attention ... last 6 hours ...

lstms as article sequence is not necessarily independent.

3. Create special time-conditional tokens (for text)




-----

TODO: batch size
TODO: embedding token size / other embeddings
TODO: koren stevila komentarjev ali pa kaj drugega

How to ensemble this with the log-linear model for even better performance?

Ensemble 5 trained models (diff random seeds) and average predictions — it will reduce MAE


----
1. Target Transformation (you're already doing this)

Why it works:

You observed overdispersion + skew.
Applying log1p(y) compresses large values and improves gradient flow.
This exploits the heavy-tailed distribution of your data.
✅ Already helping you — you are exploiting distribution here.

Priors

baseline

a sem jaz log1p napovedoval

visualization

weighting

how were the splits performed

