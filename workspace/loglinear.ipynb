{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a259f115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29201/3253867428.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert_vectors = torch.load(\"train/sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train MAE: 30.59 | Val MAE: 26.73\n",
      "Epoch 2 - Train MAE: 27.59 | Val MAE: 26.55\n",
      "Epoch 3 - Train MAE: 26.77 | Val MAE: 25.23\n",
      "Epoch 4 - Train MAE: 26.33 | Val MAE: 24.75\n",
      "Epoch 5 - Train MAE: 25.67 | Val MAE: 24.91\n",
      "Epoch 6 - Train MAE: 25.43 | Val MAE: 24.48\n",
      "Epoch 7 - Train MAE: 25.01 | Val MAE: 24.37\n",
      "Epoch 8 - Train MAE: 25.01 | Val MAE: 24.12\n",
      "Epoch 9 - Train MAE: 24.52 | Val MAE: 23.98\n",
      "Epoch 10 - Train MAE: 24.36 | Val MAE: 24.21\n",
      "Epoch 11 - Train MAE: 24.18 | Val MAE: 23.57\n",
      "Epoch 12 - Train MAE: 24.04 | Val MAE: 23.66\n",
      "Epoch 13 - Train MAE: 23.88 | Val MAE: 23.30\n",
      "Epoch 14 - Train MAE: 23.66 | Val MAE: 23.54\n",
      "Epoch 15 - Train MAE: 23.56 | Val MAE: 23.95\n",
      "Epoch 16 - Train MAE: 23.34 | Val MAE: 23.34\n",
      "Epoch 17 - Train MAE: 23.27 | Val MAE: 23.48\n",
      "Epoch 18 - Train MAE: 23.26 | Val MAE: 23.48\n",
      "Epoch 19 - Train MAE: 22.99 | Val MAE: 23.24\n",
      "Epoch 20 - Train MAE: 22.90 | Val MAE: 23.26\n",
      "Epoch 21 - Train MAE: 22.82 | Val MAE: 23.78\n",
      "Epoch 22 - Train MAE: 22.80 | Val MAE: 23.25\n",
      "Epoch 23 - Train MAE: 22.75 | Val MAE: 23.28\n",
      "Epoch 24 - Train MAE: 22.42 | Val MAE: 23.42\n",
      "Epoch 25 - Train MAE: 22.29 | Val MAE: 23.10\n",
      "Epoch 26 - Train MAE: 22.24 | Val MAE: 23.60\n",
      "Epoch 27 - Train MAE: 22.29 | Val MAE: 23.44\n",
      "Epoch 28 - Train MAE: 22.12 | Val MAE: 23.64\n",
      "Epoch 29 - Train MAE: 22.08 | Val MAE: 23.15\n",
      "Epoch 30 - Train MAE: 21.85 | Val MAE: 23.00\n",
      "Epoch 31 - Train MAE: 21.96 | Val MAE: 22.91\n",
      "Epoch 32 - Train MAE: 21.72 | Val MAE: 23.24\n",
      "Epoch 33 - Train MAE: 21.48 | Val MAE: 23.17\n",
      "Epoch 34 - Train MAE: 21.66 | Val MAE: 23.45\n",
      "Epoch 35 - Train MAE: 21.43 | Val MAE: 25.35\n",
      "Epoch 36 - Train MAE: 21.31 | Val MAE: 22.86\n",
      "Epoch 37 - Train MAE: 21.41 | Val MAE: 22.98\n",
      "Epoch 38 - Train MAE: 20.96 | Val MAE: 25.35\n",
      "Epoch 39 - Train MAE: 21.13 | Val MAE: 23.24\n",
      "Epoch 40 - Train MAE: 21.17 | Val MAE: 23.02\n",
      "Epoch 41 - Train MAE: 20.95 | Val MAE: 22.85\n",
      "Epoch 42 - Train MAE: 21.00 | Val MAE: 23.23\n",
      "Epoch 43 - Train MAE: 20.84 | Val MAE: 23.10\n",
      "Epoch 44 - Train MAE: 20.75 | Val MAE: 23.56\n",
      "Epoch 45 - Train MAE: 20.71 | Val MAE: 23.52\n",
      "Epoch 46 - Train MAE: 20.70 | Val MAE: 23.01\n",
      "Epoch 47 - Train MAE: 20.60 | Val MAE: 23.67\n",
      "Epoch 48 - Train MAE: 19.81 | Val MAE: 23.12\n",
      "Epoch 49 - Train MAE: 19.84 | Val MAE: 22.96\n",
      "Epoch 50 - Train MAE: 19.78 | Val MAE: 22.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29201/3253867428.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert_vectors_validation = torch.load(\"validation/sloberta_embeddings_val.pt\", map_location=\"cpu\").numpy()\n",
      "/tmp/ipykernel_29201/3253867428.py:234: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert_vectors_test = torch.load(\"test_final/sloberta_embeddings_final.pt\", map_location=\"cpu\").numpy()\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Imports ---\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 2. Utility functions ---\n",
    "def load(fn):\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_date_features(articles):\n",
    "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
    "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
    "    day_of_week = np.array([a['day_of_week'] for a in articles])\n",
    "    hour = np.array([a['hour'] for a in articles])\n",
    "\n",
    "    year_scaler = StandardScaler()\n",
    "    month_scaler = StandardScaler()\n",
    "    years_scaled = year_scaler.fit_transform(years)\n",
    "    months_scaled = month_scaler.fit_transform(months)\n",
    "\n",
    "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
    "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
    "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
    "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
    "\n",
    "    date_feats = np.concatenate([years_scaled, months_scaled, day_sin[:, None], day_cos[:, None], hour_sin[:, None], hour_cos[:, None]], axis=1)\n",
    "    return date_feats\n",
    "\n",
    "def extract_topics_from_url(url):\n",
    "    parts = url.split('/')\n",
    "    topic = parts[3] if len(parts) > 3 else 'none'\n",
    "    subtopic = parts[4] if len(parts) > 4 else 'none'\n",
    "    return topic, subtopic\n",
    "\n",
    "# --- 3. Dataset and Model ---\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, bert_vectors, topic_ids, subtopic_ids, date_feats, targets):\n",
    "        self.X = torch.tensor(bert_vectors, dtype=torch.float32)\n",
    "        self.topic_ids = torch.tensor(topic_ids, dtype=torch.long)\n",
    "        self.subtopic_ids = torch.tensor(subtopic_ids, dtype=torch.long)\n",
    "        self.date_feats = torch.tensor(date_feats, dtype=torch.float32)\n",
    "        self.y = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.topic_ids[idx], self.subtopic_ids[idx], self.date_feats[idx], self.y[idx]\n",
    "\n",
    "class MLPWithEmbeddings(nn.Module):\n",
    "    def __init__(self, input_dim, num_topics, num_subtopics):\n",
    "        super().__init__()\n",
    "        self.topic_embedding = nn.Embedding(num_topics, 16)\n",
    "        self.subtopic_embedding = nn.Embedding(num_subtopics, 24)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + 16 + 24 + 6, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_text, topic_ids, subtopic_ids, date_feats):\n",
    "        topic_embed = self.topic_embedding(topic_ids)\n",
    "        subtopic_embed = self.subtopic_embedding(subtopic_ids)\n",
    "        x = torch.cat([x_text, topic_embed, subtopic_embed, date_feats], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# --- 4. Main Model Trainer ---\n",
    "class RTVSloBERT:\n",
    "    def __init__(self, batch_size=64, epochs=10, learning_rate=1e-3, l2_lambda=1e-4, eval_split=0.2):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.eval_split = eval_split\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        for a in train_data:\n",
    "            dt = pd.to_datetime(a['date'])\n",
    "            a['year'] = dt.year\n",
    "            a['month'] = dt.month\n",
    "            a['day_of_week'] = dt.weekday()\n",
    "            a['hour'] = dt.hour\n",
    "\n",
    "        raw_targets = [a['n_comments'] for a in train_data]\n",
    "        targets = [np.log1p(t) for t in raw_targets]\n",
    "        bert_vectors = torch.load(\"train/sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n",
    "\n",
    "        topics = []\n",
    "        subtopics = []\n",
    "        for a in train_data:\n",
    "            topic, subtopic = extract_topics_from_url(a['url'])\n",
    "            topics.append(topic)\n",
    "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
    "\n",
    "        date_feats = process_date_features(train_data)\n",
    "\n",
    "        self.topic_encoder = LabelEncoder().fit(topics)\n",
    "        self.subtopic_encoder = LabelEncoder().fit(subtopics)\n",
    "\n",
    "        topic_ids = self.topic_encoder.transform(topics)\n",
    "        subtopic_ids = self.subtopic_encoder.transform(subtopics)\n",
    "\n",
    "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, date_train, date_val, y_train, y_val = train_test_split(\n",
    "            bert_vectors, topic_ids, subtopic_ids, date_feats, targets, test_size=self.eval_split, random_state=42\n",
    "        )\n",
    "\n",
    "        train_dataset = NewsDataset(X_train, topic_train, subtopic_train, date_train, y_train)\n",
    "        val_dataset = NewsDataset(X_val, topic_val, subtopic_val, date_val, y_val)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "        input_dim = bert_vectors.shape[1]\n",
    "        self.model = MLPWithEmbeddings(input_dim, len(self.topic_encoder.classes_), len(self.subtopic_encoder.classes_))\n",
    "\n",
    "        self._train(self.model, train_loader, val_loader)\n",
    "\n",
    "    def _train(self, model, train_loader, val_loader):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate, weight_decay=self.l2_lambda)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "        criterion = nn.HuberLoss(delta=5.0)\n",
    "\n",
    "        best_val_mae = float('inf')\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train()\n",
    "            train_mae_list = []\n",
    "            for x_batch, topic_ids, subtopic_ids, date_feats, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats).squeeze()\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                y_pred_clamped = torch.clamp(y_pred, -10, 10)\n",
    "                y_batch_clamped = torch.clamp(y_batch, -10, 10)\n",
    "                mae = torch.mean(torch.abs(torch.expm1(y_pred_clamped) - torch.expm1(y_batch_clamped))).item()\n",
    "                train_mae_list.append(mae)\n",
    "\n",
    "            model.eval()\n",
    "            val_mae_list = []\n",
    "            with torch.no_grad():\n",
    "                for x_batch, topic_ids, subtopic_ids, date_feats, y_batch in val_loader:\n",
    "                    y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats).squeeze()\n",
    "                    y_pred_clamped = torch.clamp(y_pred, -10, 10)\n",
    "                    y_batch_clamped = torch.clamp(y_batch, -10, 10)\n",
    "                    mae = torch.mean(torch.abs(torch.expm1(y_pred_clamped) - torch.expm1(y_batch_clamped))).item()\n",
    "                    val_mae_list.append(mae)\n",
    "\n",
    "            train_mae = np.mean(train_mae_list)\n",
    "            val_mae = np.mean(val_mae_list)\n",
    "            scheduler.step(val_mae)\n",
    "\n",
    "            if val_mae < best_val_mae:\n",
    "                best_val_mae = val_mae\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "            print(f\"Epoch {epoch+1} - Train MAE: {train_mae:.2f} | Val MAE: {val_mae:.2f}\")\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "    def predict(self, test_data, bert_vectors):\n",
    "        for a in test_data:\n",
    "            dt = pd.to_datetime(a['date'])\n",
    "            a['year'] = dt.year\n",
    "            a['month'] = dt.month\n",
    "            a['day_of_week'] = dt.weekday()\n",
    "            a['hour'] = dt.hour\n",
    "\n",
    "        topics = []\n",
    "        subtopics = []\n",
    "        for a in test_data:\n",
    "            topic, subtopic = extract_topics_from_url(a['url'])\n",
    "            topics.append(topic)\n",
    "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
    "\n",
    "        date_feats = process_date_features(test_data)\n",
    "\n",
    "        topic_ids = [self.topic_encoder.classes_[0] if t not in self.topic_encoder.classes_ else t for t in topics]\n",
    "        subtopic_ids = [self.subtopic_encoder.classes_[0] if s not in self.subtopic_encoder.classes_ else s for s in subtopics]\n",
    "\n",
    "        topic_ids = self.topic_encoder.transform(topic_ids)\n",
    "        subtopic_ids = self.subtopic_encoder.transform(subtopic_ids)\n",
    "\n",
    "        X = torch.tensor(bert_vectors, dtype=torch.float32)\n",
    "        topic_ids = torch.tensor(topic_ids, dtype=torch.long)\n",
    "        subtopic_ids = torch.tensor(subtopic_ids, dtype=torch.long)\n",
    "        date_feats = torch.tensor(date_feats, dtype=torch.float32)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(X, topic_ids, subtopic_ids, date_feats).squeeze().numpy()\n",
    "            return np.clip(np.expm1(preds), 0, None)\n",
    "\n",
    "# --- 5. Main ---\n",
    "if __name__ == '__main__':\n",
    "    train = load(\"../data/rtvslo_train.json\")\n",
    "    validation = load(\"../data/rtvslo_validation.json\")\n",
    "    test = load(\"../data/rtvslo_test.json\")\n",
    "\n",
    "    m = RTVSloBERT(\n",
    "        eval_split=0.05,\n",
    "        batch_size=170,\n",
    "        epochs=50,\n",
    "        learning_rate=1e-4,\n",
    "        l2_lambda=1e-3\n",
    "    )\n",
    "\n",
    "    m.fit(train)\n",
    "\n",
    "    bert_vectors_validation = torch.load(\"validation/sloberta_embeddings_val.pt\", map_location=\"cpu\").numpy()\n",
    "    p = m.predict(validation, bert_vectors_validation)\n",
    "    np.savetxt(\"predictions_val.txt\", p, fmt=\"%.4f\")\n",
    "\n",
    "    bert_vectors_test = torch.load(\"test_final/sloberta_embeddings_final.pt\", map_location=\"cpu\").numpy()\n",
    "    p = m.predict(test, bert_vectors_test)\n",
    "    np.savetxt(\"predictions_test.txt\", p, fmt=\"%.4f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "92b5b940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Error Analysis by Prediction Size Bucket:\n",
      "Bucket     |  N Samples |  Mean Pred |  Mean True |      MAE\n",
      "------------------------------------------------------------\n",
      "0â€“5        |        731 |        1.8 |        4.5 |     3.82\n",
      "5â€“20       |        585 |       10.9 |       18.1 |    13.07\n",
      "20â€“50      |        404 |       31.8 |       42.3 |    29.45\n",
      "50â€“100     |        239 |       72.2 |       84.8 |    50.52\n",
      "100â€“300    |        233 |      166.7 |      192.2 |    86.05\n",
      "300â€“1000   |         26 |      354.7 |      342.8 |   116.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Load ---\n",
    "preds = np.loadtxt(\"final_predictions_val_postprocessed.txt\")\n",
    "\n",
    "# Assuming you have the true labels:\n",
    "with open(\"../data/rtvslo_validation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_articles = json.load(f)\n",
    "\n",
    "y_true = np.array([a['n_comments'] for a in val_articles], dtype=np.float32)\n",
    "\n",
    "# --- 2. Quick sanity check ---\n",
    "assert len(preds) == len(y_true), \"Length mismatch between preds and ground truth.\"\n",
    "\n",
    "# --- 3. Define Buckets ---\n",
    "bucket_edges = [0, 5, 20, 50, 100, 300, 1000, np.inf]\n",
    "bucket_names = [\n",
    "    \"0â€“5\",\n",
    "    \"5â€“20\",\n",
    "    \"20â€“50\",\n",
    "    \"50â€“100\",\n",
    "    \"100â€“300\",\n",
    "    \"300â€“1000\",\n",
    "    \"1000+\"\n",
    "]\n",
    "\n",
    "# --- 4. Bucket analysis ---\n",
    "buckets = {name: [] for name in bucket_names}\n",
    "\n",
    "for p, y in zip(preds, y_true):\n",
    "    for i in range(len(bucket_edges)-1):\n",
    "        if bucket_edges[i] <= p < bucket_edges[i+1]:\n",
    "            buckets[bucket_names[i]].append((p, y))\n",
    "            break\n",
    "\n",
    "# --- 5. Report stats per bucket ---\n",
    "print(\"\\nðŸ“Š Error Analysis by Prediction Size Bucket:\")\n",
    "print(f\"{'Bucket':<10} | {'N Samples':>10} | {'Mean Pred':>10} | {'Mean True':>10} | {'MAE':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "bucket_maes = []\n",
    "\n",
    "for name, samples in buckets.items():\n",
    "    if len(samples) == 0:\n",
    "        continue\n",
    "    preds_b, trues_b = zip(*samples)\n",
    "    preds_b = np.array(preds_b)\n",
    "    trues_b = np.array(trues_b)\n",
    "    mae = np.mean(np.abs(preds_b - trues_b))\n",
    "    bucket_maes.append(mae)\n",
    "    print(f\"{name:<10} | {len(samples):>10} | {np.mean(preds_b):>10.1f} | {np.mean(trues_b):>10.1f} | {mae:>8.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7bb2b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved adj_predictions.txt with quantile smoothing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Load ---\n",
    "preds = np.loadtxt(\"final_predictions_test.txt\")\n",
    "\n",
    "# --- 2. Postprocessing: Quantile Smoothing ---\n",
    "\n",
    "# Step 1: Clip extreme values\n",
    "max_allowed = 3000\n",
    "preds = np.clip(preds, 0, max_allowed)\n",
    "\n",
    "# Step 2: Quantile smoothing\n",
    "q_low = np.percentile(preds, 1)\n",
    "q_high = np.percentile(preds, 99)\n",
    "\n",
    "# Linearly squash very small and very large predictions\n",
    "def quantile_smooth(x):\n",
    "    if x < q_low:\n",
    "        return x * 0.7  # shrink low values\n",
    "    elif x > q_high:\n",
    "        return q_high # shrink heavy tails\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "vectorized_smooth = np.vectorize(quantile_smooth)\n",
    "smoothed_preds = vectorized_smooth(preds)\n",
    "\n",
    "# --- 3. Save ---\n",
    "np.savetxt(\"adj_predictions.txt\", smoothed_preds, fmt=\"%.4f\")\n",
    "\n",
    "print(\"âœ… Saved adj_predictions.txt with quantile smoothing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9298e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š MAE between predictions and true values: 25.85\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. Load your predictions\n",
    "preds = np.loadtxt(\"adj_predictions.txt\")\n",
    "\n",
    "# 2. Load your true y-values from dataset_val.json\n",
    "with open(\"../data/rtvslo_validation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_articles = json.load(f)\n",
    "\n",
    "# 3. Extract true n_comments\n",
    "y_true = np.array([a[\"n_comments\"] for a in val_articles], dtype=np.float32)\n",
    "\n",
    "# 4. Check lengths\n",
    "assert len(preds) == len(y_true), f\"Length mismatch: preds={len(preds)}, y_true={len(y_true)}\"\n",
    "\n",
    "# 5. Calculate MAE\n",
    "mae = mean_absolute_error(y_true, preds)\n",
    "\n",
    "print(f\"ðŸ“Š MAE between predictions and true values: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09206ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ MAE on zero-comment articles: 1.91 (should be close to 0)\n",
      "Average predicted comments for zero-articles: 1.91\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load predictions\n",
    "preds = np.loadtxt(\"predictions.txt\")\n",
    "\n",
    "# Load validation set\n",
    "with open(\"../data/rtvslo_validation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "# Extract true values\n",
    "y_true = np.array([a[\"n_comments\"] for a in val_data], dtype=np.float32)\n",
    "\n",
    "# Filter to just articles with 0 true comments\n",
    "zero_mask = (y_true == 0)\n",
    "zero_preds = preds[zero_mask]\n",
    "zero_true = y_true[zero_mask]\n",
    "\n",
    "# Evaluate\n",
    "mae_zero = mean_absolute_error(zero_true, zero_preds)\n",
    "print(f\"ðŸ“‰ MAE on zero-comment articles: {mae_zero:.2f} (should be close to 0)\")\n",
    "print(f\"Average predicted comments for zero-articles: {np.mean(zero_preds):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ffd43",
   "metadata": {},
   "source": [
    "## ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a4560",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'dopolnitev-strategije-drzavnih-nalozb-cilj-invalidskih-podjetij-tudi-druzbeno-odgovorno-upravljanje'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/sklearn/utils/_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/sklearn/utils/_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dopolnitev-strategije-drzavnih-nalozb-cilj-invalidskih-podjetij-tudi-druzbeno-odgovorno-upravljanje'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Encode topics/subtopics using the SAME encoders!\u001b[39;00m\n\u001b[1;32m     17\u001b[0m topic_ids_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(topic_encoder\u001b[38;5;241m.\u001b[39mtransform(val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m---> 18\u001b[0m subtopic_ids_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43msubtopic_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubtopic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# --- 3. TensorDataset and DataLoader ---\u001b[39;00m\n\u001b[1;32m     21\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(text_embeddings_val, time_features_val, topic_ids_val, subtopic_ids_val, targets_val)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/sklearn/utils/_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'dopolnitev-strategije-drzavnih-nalozb-cilj-invalidskih-podjetij-tudi-druzbeno-odgovorno-upravljanje'"
     ]
    }
   ],
   "source": [
    "# --- 1. Load best model ---\n",
    "model.load_state_dict(torch.load(\"best_model_topic.pt\"))  # âš¡ Match training model name!\n",
    "model.eval()\n",
    "\n",
    "# --- 2. Prepare validation data ---\n",
    "val_df = pd.read_json(\"../data/rtvslo_validation.json\")\n",
    "val_df = enrich_articles_with_time_features(val_df)\n",
    "\n",
    "# Load sloberta validation embeddings\n",
    "text_embeddings_val = torch.load(\"sloberta_embeddings_val.pt\", weights_only=True)\n",
    "targets_val = torch.load(\"targets_val.pt\", weights_only=True)\n",
    "\n",
    "# Process time features\n",
    "time_features_val, _ = process_time_features(val_df, scaler=time_scaler)\n",
    "\n",
    "# Encode topics/subtopics using the SAME encoders!\n",
    "topic_ids_val = torch.tensor(topic_encoder.transform(val_df['topic']), dtype=torch.long)\n",
    "subtopic_ids_val = torch.tensor(subtopic_encoder.transform(val_df['subtopic']), dtype=torch.long)\n",
    "\n",
    "# --- 3. TensorDataset and DataLoader ---\n",
    "val_dataset = TensorDataset(text_embeddings_val, time_features_val, topic_ids_val, subtopic_ids_val, targets_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "# --- 4. Evaluation loop ---\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, time, topic_id, subtopic_id, y in val_loader:\n",
    "        text, time, topic_id, subtopic_id, y = text.to(device), time.to(device), topic_id.to(device), subtopic_id.to(device), y.to(device)\n",
    "        y_pred = model(text, time, topic_id, subtopic_id)\n",
    "        y_preds.append(torch.expm1(y_pred).cpu())  # ðŸ”¥ Reverse log1p\n",
    "        y_trues.append(torch.expm1(y).cpu())\n",
    "\n",
    "y_preds = torch.cat(y_preds).numpy()\n",
    "y_trues = torch.cat(y_trues).numpy()\n",
    "\n",
    "# --- 5. Metrics ---\n",
    "mae = mean_absolute_error(y_trues, y_preds)\n",
    "rmse = np.sqrt(mean_squared_error(y_trues, y_preds))\n",
    "r2 = r2_score(y_trues, y_preds)\n",
    "\n",
    "print(\"\\nðŸ“Š Validation Results with Topics:\")\n",
    "print(f\"  MAE : {mae:.2f}\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  RÂ²  : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec8abe",
   "metadata": {},
   "source": [
    "### Test set output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fecd8416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2218\n",
      "2218\n",
      "âœ… Saved predictions to final_predictions.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load your saved model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LinearRegressionPredictor(\n",
    "    input_dim_text=text_embeddings.shape[1],  # match your trained model\n",
    "    input_dim_time=6,                         # if you use only days_since\n",
    "    hidden_dim=128,\n",
    "    dropout_prob=0.5\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"best_model_linear_log.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 2. Load new articles\n",
    "df = pd.read_json(\"../data/rtvslo_test.json\")  # or whatever your test set is\n",
    "\n",
    "# 3. Preprocess time features\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "# Extract raw features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.weekday  # Monday = 0\n",
    "df['hour'] = df['date'].dt.hour\n",
    "\n",
    "df['year_scaled'] = year_scaler.transform(df['year'].values.reshape(-1, 1))\n",
    "df['month_scaled'] = month_scaler.transform(df['month'].values.reshape(-1, 1))\n",
    "\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# Only using 'days_since' here\n",
    "time_features_new = torch.tensor(\n",
    "    df[['year_scaled', 'month_scaled', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos']].values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# 4. Load the corresponding SloBERTa embeddings for new articles\n",
    "text_embeddings_new = torch.load(\"sloberta_embeddings_final.pt\", weights_only=True)\n",
    "\n",
    "# 5. Predict\n",
    "model.eval()\n",
    "preds = []\n",
    "\n",
    "batch_size = 128\n",
    "dataset = torch.utils.data.TensorDataset(text_embeddings_new, time_features_new)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "print(len(dataset))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, time in loader:\n",
    "        text, time = text.to(device), time.to(device)\n",
    "        y_pred = model(text, time)\n",
    "        y_pred = torch.expm1(y_pred)  # ðŸ”¥ reverse log1p to real counts\n",
    "        preds.append(y_pred.cpu())\n",
    "\n",
    "# Stack predictions\n",
    "preds = torch.cat(preds).numpy()\n",
    "print(len(preds))\n",
    "\n",
    "# 6. Save predictions to .txt\n",
    "np.savetxt(\"final_predictions.txt\", preds, fmt=\"%.3f\")  # or \"%.0f\" if you want integer predictions\n",
    "\n",
    "print(\"âœ… Saved predictions to final_predictions.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
