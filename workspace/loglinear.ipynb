{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8164ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_subtopic(url):\n",
    "    try:\n",
    "        # Remove protocol (https://) and split by slashes\n",
    "        parts = url.split(\"//\", 1)[-1].split(\"/\")\n",
    "        \n",
    "        topic = parts[1] if len(parts) > 1 else \"unknown\"\n",
    "        subtopic = parts[2] if len(parts) > 2 and not parts[2].isdigit() else \"none\"  # if 2nd part exists and is NOT an article ID\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing URL {url}: {e}\")\n",
    "        topic = \"unknown\"\n",
    "        subtopic = \"none\"\n",
    "    \n",
    "    return topic, subtopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_articles_with_topics(df):\n",
    "    topics = []\n",
    "    subtopics = []\n",
    "\n",
    "    for url in df['url']:\n",
    "        topic, subtopic = extract_topic_subtopic(url)\n",
    "        topics.append(topic)\n",
    "        subtopics.append(subtopic)\n",
    "\n",
    "    df['topic'] = topics\n",
    "    df['subtopic'] = subtopics\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af874f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport\n",
      "preostali-sporti\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.rtvslo.si/sport/preostali-sporti/anka-pogacnik-pot-v-dohi-koncala-v-drugem-krogu/667810\"\n",
    "topic, subtopic = extract_topic_subtopic(url)\n",
    "print(topic)     # â†’ \"slovenija\"\n",
    "print(subtopic)  # â†’ \"razsodisce-nekdanja-novinarka-in-urednica-tv-slovenija-krsila-novinarski-kodeks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a259f115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train MAE: 30.77 | Val MAE: 26.56\n",
      "Epoch 2 - Train MAE: 27.64 | Val MAE: 25.81\n",
      "Epoch 3 - Train MAE: 26.93 | Val MAE: 25.44\n",
      "Epoch 4 - Train MAE: 26.33 | Val MAE: 24.92\n",
      "Epoch 5 - Train MAE: 26.04 | Val MAE: 24.73\n",
      "Epoch 6 - Train MAE: 25.54 | Val MAE: 25.43\n",
      "Epoch 7 - Train MAE: 25.48 | Val MAE: 24.44\n",
      "Epoch 8 - Train MAE: 24.94 | Val MAE: 24.13\n",
      "Epoch 9 - Train MAE: 24.85 | Val MAE: 24.34\n",
      "Epoch 10 - Train MAE: 24.75 | Val MAE: 23.75\n",
      "Epoch 11 - Train MAE: 24.34 | Val MAE: 23.95\n",
      "Epoch 12 - Train MAE: 24.34 | Val MAE: 23.77\n",
      "Epoch 13 - Train MAE: 24.04 | Val MAE: 23.96\n",
      "Epoch 14 - Train MAE: 24.05 | Val MAE: 23.35\n",
      "Epoch 15 - Train MAE: 23.85 | Val MAE: 24.17\n",
      "Epoch 16 - Train MAE: 23.66 | Val MAE: 23.61\n",
      "Epoch 17 - Train MAE: 23.58 | Val MAE: 24.25\n",
      "Epoch 18 - Train MAE: 23.55 | Val MAE: 23.40\n",
      "Epoch 19 - Train MAE: 23.42 | Val MAE: 23.47\n",
      "Epoch 20 - Train MAE: 23.20 | Val MAE: 23.86\n",
      "Epoch 21 - Train MAE: 22.73 | Val MAE: 22.98\n",
      "Epoch 22 - Train MAE: 22.70 | Val MAE: 22.73\n",
      "Epoch 23 - Train MAE: 22.50 | Val MAE: 22.85\n",
      "Epoch 24 - Train MAE: 22.60 | Val MAE: 22.76\n",
      "Epoch 25 - Train MAE: 22.60 | Val MAE: 22.84\n",
      "Epoch 26 - Train MAE: 22.46 | Val MAE: 22.53\n",
      "Epoch 27 - Train MAE: 22.35 | Val MAE: 22.80\n",
      "Epoch 28 - Train MAE: 22.43 | Val MAE: 22.78\n",
      "Epoch 29 - Train MAE: 22.13 | Val MAE: 23.08\n",
      "Epoch 30 - Train MAE: 22.16 | Val MAE: 22.87\n",
      "Epoch 31 - Train MAE: 22.15 | Val MAE: 23.03\n",
      "Epoch 32 - Train MAE: 22.16 | Val MAE: 22.75\n",
      "Epoch 33 - Train MAE: 21.66 | Val MAE: 22.79\n",
      "Epoch 34 - Train MAE: 21.81 | Val MAE: 22.65\n",
      "Epoch 35 - Train MAE: 21.79 | Val MAE: 22.65\n",
      "Epoch 36 - Train MAE: 21.71 | Val MAE: 22.84\n",
      "Epoch 37 - Train MAE: 21.77 | Val MAE: 22.67\n",
      "Epoch 38 - Train MAE: 21.72 | Val MAE: 22.59\n",
      "Epoch 39 - Train MAE: 21.45 | Val MAE: 22.60\n",
      "Epoch 40 - Train MAE: 21.48 | Val MAE: 22.61\n",
      "Epoch 41 - Train MAE: 21.46 | Val MAE: 22.73\n",
      "Epoch 42 - Train MAE: 21.49 | Val MAE: 22.68\n",
      "Epoch 43 - Train MAE: 21.41 | Val MAE: 22.77\n",
      "Epoch 44 - Train MAE: 21.35 | Val MAE: 22.78\n",
      "Epoch 45 - Train MAE: 21.37 | Val MAE: 22.72\n",
      "Epoch 46 - Train MAE: 21.18 | Val MAE: 22.70\n",
      "Epoch 47 - Train MAE: 21.26 | Val MAE: 22.75\n",
      "Epoch 48 - Train MAE: 21.25 | Val MAE: 22.70\n",
      "Epoch 49 - Train MAE: 21.15 | Val MAE: 22.80\n",
      "Epoch 50 - Train MAE: 21.21 | Val MAE: 22.69\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Imports ---\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 2. Utility functions ---\n",
    "def load(fn):\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_date_features(articles):\n",
    "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
    "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
    "    day_of_week = np.array([a['day_of_week'] for a in articles])\n",
    "    hour = np.array([a['hour'] for a in articles])\n",
    "\n",
    "    year_scaler = StandardScaler()\n",
    "    month_scaler = StandardScaler()\n",
    "    years_scaled = year_scaler.fit_transform(years)\n",
    "    months_scaled = month_scaler.fit_transform(months)\n",
    "\n",
    "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
    "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
    "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
    "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
    "\n",
    "    date_feats = np.concatenate([years_scaled, months_scaled, day_sin[:, None], day_cos[:, None], hour_sin[:, None], hour_cos[:, None]], axis=1)\n",
    "    return date_feats\n",
    "\n",
    "def extract_topics_from_url(url):\n",
    "    parts = url.split('/')\n",
    "    topic = parts[3] if len(parts) > 3 else 'none'\n",
    "    subtopic = parts[4] if len(parts) > 4 else 'none'\n",
    "    return topic, subtopic\n",
    "\n",
    "# --- 3. Dataset and Model ---\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, bert_vectors, topic_ids, subtopic_ids, date_feats, targets):\n",
    "        self.X = torch.tensor(bert_vectors, dtype=torch.float32)\n",
    "        self.topic_ids = torch.tensor(topic_ids, dtype=torch.long)\n",
    "        self.subtopic_ids = torch.tensor(subtopic_ids, dtype=torch.long)\n",
    "        self.date_feats = torch.tensor(date_feats, dtype=torch.float32)\n",
    "        self.y = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.topic_ids[idx], self.subtopic_ids[idx], self.date_feats[idx], self.y[idx]\n",
    "\n",
    "class MLPWithEmbeddings(nn.Module):\n",
    "    def __init__(self, input_dim, num_topics, num_subtopics):\n",
    "        super().__init__()\n",
    "        self.topic_embedding = nn.Embedding(num_topics, 16)\n",
    "        self.subtopic_embedding = nn.Embedding(num_subtopics, 24)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + 16 + 24 + 6, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_text, topic_ids, subtopic_ids, date_feats):\n",
    "        topic_embed = self.topic_embedding(topic_ids)\n",
    "        subtopic_embed = self.subtopic_embedding(subtopic_ids)\n",
    "        x = torch.cat([x_text, topic_embed, subtopic_embed, date_feats], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# --- 4. Main Model Trainer ---\n",
    "class RTVSloBERT:\n",
    "    def __init__(self, batch_size=64, epochs=10, learning_rate=1e-3, l2_lambda=1e-4, eval_split=0.2):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.eval_split = eval_split\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        for a in train_data:\n",
    "            dt = pd.to_datetime(a['date'])\n",
    "            a['year'] = dt.year\n",
    "            a['month'] = dt.month\n",
    "            a['day_of_week'] = dt.weekday()\n",
    "            a['hour'] = dt.hour\n",
    "\n",
    "        raw_targets = [a['n_comments'] for a in train_data]\n",
    "        targets = [np.log1p(t) for t in raw_targets]\n",
    "        bert_vectors = torch.load(\"sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n",
    "\n",
    "        topics = []\n",
    "        subtopics = []\n",
    "        for a in train_data:\n",
    "            topic, subtopic = extract_topics_from_url(a['url'])\n",
    "            topics.append(topic)\n",
    "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
    "\n",
    "        date_feats = process_date_features(train_data)\n",
    "\n",
    "        self.topic_encoder = LabelEncoder().fit(topics)\n",
    "        self.subtopic_encoder = LabelEncoder().fit(subtopics)\n",
    "\n",
    "        topic_ids = self.topic_encoder.transform(topics)\n",
    "        subtopic_ids = self.subtopic_encoder.transform(subtopics)\n",
    "\n",
    "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, date_train, date_val, y_train, y_val = train_test_split(\n",
    "            bert_vectors, topic_ids, subtopic_ids, date_feats, targets, test_size=self.eval_split, random_state=42\n",
    "        )\n",
    "\n",
    "        train_dataset = NewsDataset(X_train, topic_train, subtopic_train, date_train, y_train)\n",
    "        val_dataset = NewsDataset(X_val, topic_val, subtopic_val, date_val, y_val)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "        input_dim = bert_vectors.shape[1]\n",
    "        self.model = MLPWithEmbeddings(input_dim, len(self.topic_encoder.classes_), len(self.subtopic_encoder.classes_))\n",
    "\n",
    "        self._train(self.model, train_loader, val_loader)\n",
    "\n",
    "    def _train(self, model, train_loader, val_loader):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate, weight_decay=self.l2_lambda)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "        best_val_mae = float('inf')\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train()\n",
    "            train_mae_list = []\n",
    "            for x_batch, topic_ids, subtopic_ids, date_feats, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats).squeeze()\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                y_pred_clamped = torch.clamp(y_pred, -10, 10)\n",
    "                y_batch_clamped = torch.clamp(y_batch, -10, 10)\n",
    "                mae = torch.mean(torch.abs(torch.expm1(y_pred_clamped) - torch.expm1(y_batch_clamped))).item()\n",
    "                train_mae_list.append(mae)\n",
    "\n",
    "            model.eval()\n",
    "            val_mae_list = []\n",
    "            with torch.no_grad():\n",
    "                for x_batch, topic_ids, subtopic_ids, date_feats, y_batch in val_loader:\n",
    "                    y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats).squeeze()\n",
    "                    y_pred_clamped = torch.clamp(y_pred, -10, 10)\n",
    "                    y_batch_clamped = torch.clamp(y_batch, -10, 10)\n",
    "                    mae = torch.mean(torch.abs(torch.expm1(y_pred_clamped) - torch.expm1(y_batch_clamped))).item()\n",
    "                    val_mae_list.append(mae)\n",
    "\n",
    "            train_mae = np.mean(train_mae_list)\n",
    "            val_mae = np.mean(val_mae_list)\n",
    "            scheduler.step(val_mae)\n",
    "\n",
    "            if val_mae < best_val_mae:\n",
    "                best_val_mae = val_mae\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "            print(f\"Epoch {epoch+1} - Train MAE: {train_mae:.2f} | Val MAE: {val_mae:.2f}\")\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        for a in test_data:\n",
    "            dt = pd.to_datetime(a['date'])\n",
    "            a['year'] = dt.year\n",
    "            a['month'] = dt.month\n",
    "            a['day_of_week'] = dt.weekday()\n",
    "            a['hour'] = dt.hour\n",
    "\n",
    "        bert_vectors = torch.load(\"sloberta_embeddings_final.pt\", map_location=\"cpu\").numpy()\n",
    "\n",
    "        topics = []\n",
    "        subtopics = []\n",
    "        for a in test_data:\n",
    "            topic, subtopic = extract_topics_from_url(a['url'])\n",
    "            topics.append(topic)\n",
    "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
    "\n",
    "        date_feats = process_date_features(test_data)\n",
    "\n",
    "        topic_ids = [self.topic_encoder.classes_[0] if t not in self.topic_encoder.classes_ else t for t in topics]\n",
    "        subtopic_ids = [self.subtopic_encoder.classes_[0] if s not in self.subtopic_encoder.classes_ else s for s in subtopics]\n",
    "\n",
    "        topic_ids = self.topic_encoder.transform(topic_ids)\n",
    "        subtopic_ids = self.subtopic_encoder.transform(subtopic_ids)\n",
    "\n",
    "        X = torch.tensor(bert_vectors, dtype=torch.float32)\n",
    "        topic_ids = torch.tensor(topic_ids, dtype=torch.long)\n",
    "        subtopic_ids = torch.tensor(subtopic_ids, dtype=torch.long)\n",
    "        date_feats = torch.tensor(date_feats, dtype=torch.float32)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(X, topic_ids, subtopic_ids, date_feats).squeeze().numpy()\n",
    "            return np.clip(np.expm1(preds), 0, None)\n",
    "\n",
    "# --- 5. Main ---\n",
    "if __name__ == '__main__':\n",
    "    train = load(\"../data/rtvslo_train.json\")\n",
    "    test = load(\"../data/rtvslo_test.json\")\n",
    "\n",
    "    m = RTVSloBERT(\n",
    "        eval_split=0.05,\n",
    "        batch_size=170,\n",
    "        epochs=50,\n",
    "        learning_rate=1e-4,\n",
    "        l2_lambda=1e-3\n",
    "    )\n",
    "\n",
    "    m.fit(train)\n",
    "    p = m.predict(test)\n",
    "    np.savetxt(\"predictions.txt\", p, fmt=\"%.4f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9298e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š MAE between predictions and true values: 27.13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. Load your predictions\n",
    "preds = np.loadtxt(\"predictions.txt\")\n",
    "\n",
    "# 2. Load your true y-values from dataset_val.json\n",
    "with open(\"../data/rtvslo_validation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_articles = json.load(f)\n",
    "\n",
    "# 3. Extract true n_comments\n",
    "y_true = np.array([a[\"n_comments\"] for a in val_articles], dtype=np.float32)\n",
    "\n",
    "# 4. Check lengths\n",
    "assert len(preds) == len(y_true), f\"Length mismatch: preds={len(preds)}, y_true={len(y_true)}\"\n",
    "\n",
    "# 5. Calculate MAE\n",
    "mae = mean_absolute_error(y_true, preds)\n",
    "\n",
    "print(f\"ðŸ“Š MAE between predictions and true values: {mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ffd43",
   "metadata": {},
   "source": [
    "## ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a4560",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'dopolnitev-strategije-drzavnih-nalozb-cilj-invalidskih-podjetij-tudi-druzbeno-odgovorno-upravljanje'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/sklearn/utils/_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/sklearn/utils/_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dopolnitev-strategije-drzavnih-nalozb-cilj-invalidskih-podjetij-tudi-druzbeno-odgovorno-upravljanje'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Encode topics/subtopics using the SAME encoders!\u001b[39;00m\n\u001b[1;32m     17\u001b[0m topic_ids_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(topic_encoder\u001b[38;5;241m.\u001b[39mtransform(val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m---> 18\u001b[0m subtopic_ids_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43msubtopic_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubtopic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# --- 3. TensorDataset and DataLoader ---\u001b[39;00m\n\u001b[1;32m     21\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(text_embeddings_val, time_features_val, topic_ids_val, subtopic_ids_val, targets_val)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/sklearn/utils/_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'dopolnitev-strategije-drzavnih-nalozb-cilj-invalidskih-podjetij-tudi-druzbeno-odgovorno-upravljanje'"
     ]
    }
   ],
   "source": [
    "# --- 1. Load best model ---\n",
    "model.load_state_dict(torch.load(\"best_model_topic.pt\"))  # âš¡ Match training model name!\n",
    "model.eval()\n",
    "\n",
    "# --- 2. Prepare validation data ---\n",
    "val_df = pd.read_json(\"../data/rtvslo_validation.json\")\n",
    "val_df = enrich_articles_with_time_features(val_df)\n",
    "\n",
    "# Load sloberta validation embeddings\n",
    "text_embeddings_val = torch.load(\"sloberta_embeddings_val.pt\", weights_only=True)\n",
    "targets_val = torch.load(\"targets_val.pt\", weights_only=True)\n",
    "\n",
    "# Process time features\n",
    "time_features_val, _ = process_time_features(val_df, scaler=time_scaler)\n",
    "\n",
    "# Encode topics/subtopics using the SAME encoders!\n",
    "topic_ids_val = torch.tensor(topic_encoder.transform(val_df['topic']), dtype=torch.long)\n",
    "subtopic_ids_val = torch.tensor(subtopic_encoder.transform(val_df['subtopic']), dtype=torch.long)\n",
    "\n",
    "# --- 3. TensorDataset and DataLoader ---\n",
    "val_dataset = TensorDataset(text_embeddings_val, time_features_val, topic_ids_val, subtopic_ids_val, targets_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "# --- 4. Evaluation loop ---\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, time, topic_id, subtopic_id, y in val_loader:\n",
    "        text, time, topic_id, subtopic_id, y = text.to(device), time.to(device), topic_id.to(device), subtopic_id.to(device), y.to(device)\n",
    "        y_pred = model(text, time, topic_id, subtopic_id)\n",
    "        y_preds.append(torch.expm1(y_pred).cpu())  # ðŸ”¥ Reverse log1p\n",
    "        y_trues.append(torch.expm1(y).cpu())\n",
    "\n",
    "y_preds = torch.cat(y_preds).numpy()\n",
    "y_trues = torch.cat(y_trues).numpy()\n",
    "\n",
    "# --- 5. Metrics ---\n",
    "mae = mean_absolute_error(y_trues, y_preds)\n",
    "rmse = np.sqrt(mean_squared_error(y_trues, y_preds))\n",
    "r2 = r2_score(y_trues, y_preds)\n",
    "\n",
    "print(\"\\nðŸ“Š Validation Results with Topics:\")\n",
    "print(f\"  MAE : {mae:.2f}\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  RÂ²  : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec8abe",
   "metadata": {},
   "source": [
    "### Test set output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fecd8416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2218\n",
      "2218\n",
      "âœ… Saved predictions to final_predictions.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load your saved model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LinearRegressionPredictor(\n",
    "    input_dim_text=text_embeddings.shape[1],  # match your trained model\n",
    "    input_dim_time=6,                         # if you use only days_since\n",
    "    hidden_dim=128,\n",
    "    dropout_prob=0.5\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"best_model_linear_log.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 2. Load new articles\n",
    "df = pd.read_json(\"../data/rtvslo_test.json\")  # or whatever your test set is\n",
    "\n",
    "# 3. Preprocess time features\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "# Extract raw features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.weekday  # Monday = 0\n",
    "df['hour'] = df['date'].dt.hour\n",
    "\n",
    "df['year_scaled'] = year_scaler.transform(df['year'].values.reshape(-1, 1))\n",
    "df['month_scaled'] = month_scaler.transform(df['month'].values.reshape(-1, 1))\n",
    "\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# Only using 'days_since' here\n",
    "time_features_new = torch.tensor(\n",
    "    df[['year_scaled', 'month_scaled', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos']].values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# 4. Load the corresponding SloBERTa embeddings for new articles\n",
    "text_embeddings_new = torch.load(\"sloberta_embeddings_final.pt\", weights_only=True)\n",
    "\n",
    "# 5. Predict\n",
    "model.eval()\n",
    "preds = []\n",
    "\n",
    "batch_size = 128\n",
    "dataset = torch.utils.data.TensorDataset(text_embeddings_new, time_features_new)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "print(len(dataset))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, time in loader:\n",
    "        text, time = text.to(device), time.to(device)\n",
    "        y_pred = model(text, time)\n",
    "        y_pred = torch.expm1(y_pred)  # ðŸ”¥ reverse log1p to real counts\n",
    "        preds.append(y_pred.cpu())\n",
    "\n",
    "# Stack predictions\n",
    "preds = torch.cat(preds).numpy()\n",
    "print(len(preds))\n",
    "\n",
    "# 6. Save predictions to .txt\n",
    "np.savetxt(\"final_predictions.txt\", preds, fmt=\"%.3f\")  # or \"%.0f\" if you want integer predictions\n",
    "\n",
    "print(\"âœ… Saved predictions to final_predictions.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
