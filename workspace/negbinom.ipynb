{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a2c0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bba395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val MAE: 38.44\n",
      "Epoch 2 - Val MAE: 33.99\n",
      "Epoch 3 - Val MAE: 32.05\n",
      "Epoch 4 - Val MAE: 31.25\n",
      "Epoch 5 - Val MAE: 30.24\n",
      "Epoch 6 - Val MAE: 29.07\n",
      "Epoch 7 - Val MAE: 28.01\n",
      "Epoch 8 - Val MAE: 27.75\n",
      "Epoch 9 - Val MAE: 27.33\n",
      "Epoch 10 - Val MAE: 28.09\n",
      "Epoch 11 - Val MAE: 27.54\n",
      "Epoch 12 - Val MAE: 27.62\n",
      "Epoch 13 - Val MAE: 27.97\n",
      "Epoch 14 - Val MAE: 28.82\n",
      "Epoch 15 - Val MAE: 27.67\n",
      "Epoch 16 - Val MAE: 26.39\n",
      "Epoch 17 - Val MAE: 26.93\n",
      "Epoch 18 - Val MAE: 26.42\n",
      "Epoch 19 - Val MAE: 26.76\n",
      "Epoch 20 - Val MAE: 26.46\n",
      "Epoch 21 - Val MAE: 25.74\n",
      "Epoch 22 - Val MAE: 25.34\n",
      "Epoch 23 - Val MAE: 26.18\n",
      "Epoch 24 - Val MAE: 26.58\n",
      "Epoch 25 - Val MAE: 26.16\n",
      "Epoch 26 - Val MAE: 26.76\n",
      "Epoch 27 - Val MAE: 25.56\n",
      "Epoch 28 - Val MAE: 26.41\n",
      "Epoch 29 - Val MAE: 25.09\n",
      "Epoch 30 - Val MAE: 24.92\n",
      "Epoch 31 - Val MAE: 24.80\n",
      "Epoch 32 - Val MAE: 25.73\n",
      "Epoch 33 - Val MAE: 24.97\n",
      "Epoch 34 - Val MAE: 25.77\n",
      "Epoch 35 - Val MAE: 27.84\n",
      "Epoch 36 - Val MAE: 26.24\n",
      "Epoch 37 - Val MAE: 23.48\n",
      "Epoch 38 - Val MAE: 23.79\n",
      "Epoch 39 - Val MAE: 24.13\n",
      "Epoch 40 - Val MAE: 23.52\n",
      "Epoch 41 - Val MAE: 23.56\n",
      "Epoch 42 - Val MAE: 24.72\n",
      "Epoch 43 - Val MAE: 24.95\n",
      "Epoch 44 - Val MAE: 25.58\n",
      "Epoch 45 - Val MAE: 24.21\n",
      "Epoch 46 - Val MAE: 25.99\n",
      "Epoch 47 - Val MAE: 24.81\n",
      "Epoch 48 - Val MAE: 26.56\n",
      "Epoch 49 - Val MAE: 25.74\n",
      "Epoch 50 - Val MAE: 24.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ld/fxnflmcx12s8gnksx3ltycv80000gn/T/ipykernel_60586/2349513113.py:243: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(bert_vectors, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Imports ---\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 2. Utility functions ---\n",
    "def load(fn):\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def enrich_articles_with_time_features(articles):\n",
    "    for a in articles:\n",
    "        dt = pd.to_datetime(a['date'])\n",
    "        a['year'] = dt.year\n",
    "        a['month'] = dt.month\n",
    "        a['day_of_week'] = dt.weekday()\n",
    "        a['hour'] = dt.hour\n",
    "    return articles\n",
    "\n",
    "def process_date_features(articles):\n",
    "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
    "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
    "    day_of_week = np.array([a['day_of_week'] for a in articles])\n",
    "    hour = np.array([a['hour'] for a in articles])\n",
    "\n",
    "    year_scaler = StandardScaler()\n",
    "    month_scaler = StandardScaler()\n",
    "    years_scaled = year_scaler.fit_transform(years)\n",
    "    months_scaled = month_scaler.fit_transform(months)\n",
    "\n",
    "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
    "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
    "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
    "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
    "\n",
    "    date_feats = np.concatenate([years_scaled, months_scaled, day_sin[:, None], day_cos[:, None], hour_sin[:, None], hour_cos[:, None]], axis=1)\n",
    "    return date_feats\n",
    "\n",
    "def extract_topics_from_url(url):\n",
    "    parts = url.split('/')\n",
    "    topic = parts[3] if len(parts) > 3 else 'none'\n",
    "    subtopic = parts[4] if len(parts) > 4 else 'none'\n",
    "    return topic, subtopic\n",
    "\n",
    "# --- 3. Dataset ---\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, text_embed, topic_ids, subtopic_ids, date_feats, targets):\n",
    "        self.text_embed = text_embed\n",
    "        self.topic_ids = topic_ids\n",
    "        self.subtopic_ids = subtopic_ids\n",
    "        self.date_feats = date_feats\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.text_embed[idx],\n",
    "            self.topic_ids[idx],\n",
    "            self.subtopic_ids[idx],\n",
    "            self.date_feats[idx],\n",
    "            self.targets[idx]\n",
    "        )\n",
    "\n",
    "# --- 4. Model ---\n",
    "class NegativeBinomialLoss(nn.Module):\n",
    "    def forward(self, y_true, mu, alpha):\n",
    "        eps = 1e-8\n",
    "        mu = torch.clamp(mu, min=eps)\n",
    "        alpha = torch.clamp(alpha, min=eps)\n",
    "        r = 1.0 / (alpha + eps)\n",
    "        p = r / (r + mu + eps)\n",
    "\n",
    "        log_prob = (\n",
    "            torch.lgamma(y_true + r)\n",
    "            - torch.lgamma(r)\n",
    "            - torch.lgamma(y_true + 1)\n",
    "            + r * torch.log(p + eps)\n",
    "            + y_true * torch.log(1 - p + eps)\n",
    "        )\n",
    "        return -torch.mean(log_prob)\n",
    "\n",
    "class NBRegressionWithTopics(nn.Module):\n",
    "    def __init__(self, input_dim_text, num_topics, num_subtopics, time_dim=6, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.topic_embed = nn.Embedding(num_topics, 16)\n",
    "        self.subtopic_embed = nn.Embedding(num_subtopics, 24)\n",
    "\n",
    "        total_input_dim = input_dim_text + 16 + 24 + time_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(total_input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.mu_head = nn.Linear(hidden_dim, 1)\n",
    "        self.alpha_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, text_embed, topic_ids, subtopic_ids, date_feats):\n",
    "        topic_emb = self.topic_embed(topic_ids)\n",
    "        subtopic_emb = self.subtopic_embed(subtopic_ids)\n",
    "        x = torch.cat([text_embed, topic_emb, subtopic_emb, date_feats], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        mu = torch.exp(self.mu_head(x))\n",
    "        alpha = F.softplus(self.alpha_head(x))\n",
    "        return mu.squeeze(1), alpha.squeeze(1)\n",
    "\n",
    "# --- 5. Main Trainer Class ---\n",
    "class RTVSloNB:\n",
    "    def __init__(self, batch_size=64, epochs=50, learning_rate=1e-3, eval_split=0.2):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eval_split = eval_split\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        train_data = enrich_articles_with_time_features(train_data)\n",
    "\n",
    "        raw_targets = [a['n_comments'] for a in train_data]\n",
    "        targets = torch.tensor(raw_targets, dtype=torch.float32)\n",
    "\n",
    "        bert_vectors = torch.load(\"sloberta_embeddings.pt\", map_location=\"cpu\").to(torch.float32)\n",
    "\n",
    "        topics = []\n",
    "        subtopics = []\n",
    "        for a in train_data:\n",
    "            topic, subtopic = extract_topics_from_url(a['url'])\n",
    "            topics.append(topic)\n",
    "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
    "\n",
    "        date_feats = torch.tensor(process_date_features(train_data), dtype=torch.float32)\n",
    "\n",
    "        self.topic_encoder = LabelEncoder().fit(topics)\n",
    "        self.subtopic_encoder = LabelEncoder().fit(subtopics)\n",
    "\n",
    "        topic_ids = torch.tensor(self.topic_encoder.transform(topics), dtype=torch.long)\n",
    "        subtopic_ids = torch.tensor(self.subtopic_encoder.transform(subtopics), dtype=torch.long)\n",
    "\n",
    "        # >>> FIX: split numpy arrays\n",
    "        X_np = bert_vectors.cpu().numpy()\n",
    "        topic_np = topic_ids.cpu().numpy()\n",
    "        subtopic_np = subtopic_ids.cpu().numpy()\n",
    "        date_np = date_feats.cpu().numpy()\n",
    "        target_np = targets.cpu().numpy()\n",
    "\n",
    "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, date_train, date_val, y_train, y_val = train_test_split(\n",
    "            X_np, topic_np, subtopic_np, date_np, target_np, test_size=self.eval_split, random_state=42\n",
    "        )\n",
    "\n",
    "        # >>> FIX: rewrap into torch tensors\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "        topic_train = torch.tensor(topic_train, dtype=torch.long)\n",
    "        topic_val = torch.tensor(topic_val, dtype=torch.long)\n",
    "        subtopic_train = torch.tensor(subtopic_train, dtype=torch.long)\n",
    "        subtopic_val = torch.tensor(subtopic_val, dtype=torch.long)\n",
    "        date_train = torch.tensor(date_train, dtype=torch.float32)\n",
    "        date_val = torch.tensor(date_val, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "        y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "        train_dataset = NewsDataset(X_train, topic_train, subtopic_train, date_train, y_train)\n",
    "        val_dataset = NewsDataset(X_val, topic_val, subtopic_val, date_val, y_val)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "        input_dim = bert_vectors.shape[1]\n",
    "        self.model = NBRegressionWithTopics(input_dim, len(self.topic_encoder.classes_), len(self.subtopic_encoder.classes_))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self._train(self.model, train_loader, val_loader)\n",
    "\n",
    "\n",
    "    def _train(self, model, train_loader, val_loader):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "        criterion = NegativeBinomialLoss()\n",
    "\n",
    "        best_val_mae = float('inf')\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train()\n",
    "            for text, topic_id, subtopic_id, time_feat, y in train_loader:\n",
    "                text, topic_id, subtopic_id, time_feat, y = text.to(self.device), topic_id.to(self.device), subtopic_id.to(self.device), time_feat.to(self.device), y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                mu, alpha = model(text, topic_id, subtopic_id, time_feat)\n",
    "                loss = criterion(y, mu, alpha)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            with torch.no_grad():\n",
    "                for text, topic_id, subtopic_id, time_feat, y in val_loader:\n",
    "                    text, topic_id, subtopic_id, time_feat, y = text.to(self.device), topic_id.to(self.device), subtopic_id.to(self.device), time_feat.to(self.device), y.to(self.device)\n",
    "                    mu, alpha = model(text, topic_id, subtopic_id, time_feat)\n",
    "                    val_preds.append(mu.cpu())\n",
    "                    val_targets.append(y.cpu())\n",
    "\n",
    "            val_preds = torch.cat(val_preds).numpy()\n",
    "            val_targets = torch.cat(val_targets).numpy()\n",
    "            val_mae = np.mean(np.abs(val_preds - val_targets))\n",
    "\n",
    "            if val_mae < best_val_mae:\n",
    "                best_val_mae = val_mae\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "            print(f\"Epoch {epoch+1} - Val MAE: {val_mae:.2f}\")\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        test_data = enrich_articles_with_time_features(test_data)\n",
    "\n",
    "        bert_vectors = torch.load(\"sloberta_embeddings_val.pt\", map_location=\"cpu\").to(torch.float32)\n",
    "\n",
    "        topics = []\n",
    "        subtopics = []\n",
    "        for a in test_data:\n",
    "            topic, subtopic = extract_topics_from_url(a['url'])\n",
    "            topics.append(topic)\n",
    "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
    "\n",
    "        date_feats = process_date_features(test_data)\n",
    "\n",
    "        topic_ids = [self.topic_encoder.classes_[0] if t not in self.topic_encoder.classes_ else t for t in topics]\n",
    "        subtopic_ids = [self.subtopic_encoder.classes_[0] if s not in self.subtopic_encoder.classes_ else s for s in subtopics]\n",
    "\n",
    "        topic_ids = self.topic_encoder.transform(topic_ids)\n",
    "        subtopic_ids = self.subtopic_encoder.transform(subtopic_ids)\n",
    "\n",
    "        X = torch.tensor(bert_vectors, dtype=torch.float32)\n",
    "        topic_ids = torch.tensor(topic_ids, dtype=torch.long)\n",
    "        subtopic_ids = torch.tensor(subtopic_ids, dtype=torch.long)\n",
    "        date_feats = torch.tensor(date_feats, dtype=torch.float32)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X, topic_ids, subtopic_ids, date_feats = X.to(self.device), topic_ids.to(self.device), subtopic_ids.to(self.device), date_feats.to(self.device)\n",
    "            preds, _ = self.model(X, topic_ids, subtopic_ids, date_feats)\n",
    "            return np.clip(preds.cpu().numpy(), 0, None)\n",
    "\n",
    "# --- 6. Main ---\n",
    "if __name__ == '__main__':\n",
    "    train = load(\"../data/rtvslo_train.json\")\n",
    "    test = load(\"../data/rtvslo_test.json\")\n",
    "\n",
    "    m = RTVSloNB(\n",
    "        eval_split=0.05,\n",
    "        batch_size=170,\n",
    "        epochs=50,\n",
    "        learning_rate=1e-4\n",
    "    )\n",
    "\n",
    "    m.fit(train)\n",
    "    p = m.predict(test)\n",
    "    np.savetxt(\"final_predictions.txt\", p, fmt=\"%.4f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "489335d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š MAE between predictions and true values: 26.36\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. Load your predictions\n",
    "preds = np.loadtxt(\"predictions.txt\")\n",
    "\n",
    "# 2. Load your true y-values from dataset_val.json\n",
    "with open(\"../data/rtvslo_validation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_articles = json.load(f)\n",
    "\n",
    "# 3. Extract true n_comments\n",
    "y_true = np.array([a[\"n_comments\"] for a in val_articles], dtype=np.float32)\n",
    "\n",
    "# 4. Check lengths\n",
    "assert len(preds) == len(y_true), f\"Length mismatch: preds={len(preds)}, y_true={len(y_true)}\"\n",
    "\n",
    "# 5. Calculate MAE\n",
    "mae = mean_absolute_error(y_true, preds)\n",
    "\n",
    "print(f\"ðŸ“Š MAE between predictions and true values: {mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ba405a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final predictions saved to final_predictions_nb.txt\n"
     ]
    }
   ],
   "source": [
    "# Load your new sloberta embeddings and time features\n",
    "text_embeddings_test = torch.load(\"sloberta_embeddings_final.pt\", weights_only=True, map_location='cpu')\n",
    "# 3. Preprocess time features\n",
    "df = pd.read_json(\"../data/rtvslo_test.json\")  # or whatever your test set is\n",
    "\n",
    "# 3. Preprocess time features\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "# Extract raw features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.weekday  # Monday = 0\n",
    "df['hour'] = df['date'].dt.hour\n",
    "\n",
    "df['year_scaled'] = year_scaler.transform(df['year'].values.reshape(-1, 1))\n",
    "df['month_scaled'] = month_scaler.transform(df['month'].values.reshape(-1, 1))\n",
    "\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# Only using 'days_since' here\n",
    "time_features_test = torch.tensor(\n",
    "    df[['year_scaled', 'month_scaled', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos']].values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Move to TensorDataset and Loader\n",
    "test_dataset = TensorDataset(text_embeddings_test, time_features_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, time in test_loader:\n",
    "        text, time = text.to(device), time.to(device)\n",
    "        mu, _ = model(text, time)\n",
    "        preds.append(mu.cpu())\n",
    "\n",
    "# Stack predictions\n",
    "final_preds = torch.cat(preds).numpy()\n",
    "\n",
    "# Save to txt\n",
    "np.savetxt(\"final_predictions_nb.txt\", final_preds, fmt=\"%.3f\")\n",
    "\n",
    "print(\"âœ… Final predictions saved to final_predictions_nb.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
