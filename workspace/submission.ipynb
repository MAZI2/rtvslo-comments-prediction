{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Data Loading and Processing ===\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def prepare_date_features(articles):\n",
    "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
    "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
    "    weekdays = np.array([a['day_of_week'] for a in articles])\n",
    "    hours = np.array([a['hour'] for a in articles])\n",
    "\n",
    "    year_scaler = StandardScaler()\n",
    "    month_scaler = StandardScaler()\n",
    "    years_scaled = year_scaler.fit_transform(years)\n",
    "    months_scaled = month_scaler.fit_transform(months)\n",
    "\n",
    "    weekday_sin = np.sin(2 * np.pi * weekdays / 7)\n",
    "    weekday_cos = np.cos(2 * np.pi * weekdays / 7)\n",
    "    hour_sin = np.sin(2 * np.pi * hours / 24)\n",
    "    hour_cos = np.cos(2 * np.pi * hours / 24)\n",
    "\n",
    "    return np.hstack([years_scaled, months_scaled, weekday_sin[:, None], weekday_cos[:, None], hour_sin[:, None], hour_cos[:, None]])\n",
    "\n",
    "\n",
    "def extract_topics_from_urls(urls):\n",
    "    topics, subtopics = [], []\n",
    "    for url in urls:\n",
    "        parts = url.split('/')\n",
    "        topic = parts[3] if len(parts) > 3 else 'none'\n",
    "        subtopic = parts[4] if len(parts) > 4 else 'none'\n",
    "        subtopic = 'none' if subtopic == 'NO_SUBTOPIC' else subtopic\n",
    "        topics.append(topic)\n",
    "        subtopics.append(subtopic)\n",
    "    return topics, subtopics\n",
    "\n",
    "# === Dataset and Model Definitions ===\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, bert, topic, subtopic, date, target):\n",
    "        self.bert = bert\n",
    "        self.topic = topic\n",
    "        self.subtopic = subtopic\n",
    "        self.date = date\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.bert[idx], self.topic[idx], self.subtopic[idx], self.date[idx], self.target[idx]\n",
    "\n",
    "class MLPWithEmbeddings(nn.Module):\n",
    "    def __init__(self, input_dim, n_topics, n_subtopics):\n",
    "        super().__init__()\n",
    "        self.topic_emb = nn.Embedding(n_topics, 16)\n",
    "        self.subtopic_emb = nn.Embedding(n_subtopics, 24)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + 16 + 24 + 6, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, st, d):\n",
    "        t_emb = self.topic_emb(t)\n",
    "        st_emb = self.subtopic_emb(st)\n",
    "        return self.net(torch.cat([x, t_emb, st_emb, d], dim=1))\n",
    "\n",
    "# === Trainer and Predictor ===\n",
    "\n",
    "class RTVPredictor:\n",
    "    def __init__(self, model_id=0, batch_size=170, epochs=150, lr=1e-4, weight_decay=1e-3):\n",
    "        self.model_id = model_id\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def prepare(self, articles):\n",
    "        for a in articles:\n",
    "            dt = pd.to_datetime(a['date'])\n",
    "            a['year'] = dt.year\n",
    "            a['month'] = dt.month\n",
    "            a['day_of_week'] = dt.weekday()\n",
    "            a['hour'] = dt.hour\n",
    "\n",
    "    def train(self, train_articles, bert_vectors, save_path):\n",
    "        self.prepare(train_articles)\n",
    "\n",
    "        targets = np.log1p([a['n_comments'] for a in train_articles])\n",
    "        topics, subtopics = extract_topics_from_urls([a['url'] for a in train_articles])\n",
    "\n",
    "        self.topic_enc = LabelEncoder().fit(topics)\n",
    "        self.subtopic_enc = LabelEncoder().fit(subtopics)\n",
    "\n",
    "        topic_ids = self.topic_enc.transform(topics)\n",
    "        subtopic_ids = self.subtopic_enc.transform(subtopics)\n",
    "        date_feats = prepare_date_features(train_articles)\n",
    "\n",
    "\n",
    "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, date_train, date_val, y_train, y_val = train_test_split(\n",
    "            bert_vectors, topic_ids, subtopic_ids, date_feats, targets, test_size=0.05, random_state=42\n",
    "        )\n",
    "\n",
    "        train_data = NewsDataset(\n",
    "            torch.tensor(X_train).float(), torch.tensor(topic_train).long(),\n",
    "            torch.tensor(subtopic_train).long(), torch.tensor(date_train).float(),\n",
    "            torch.tensor(y_train).float()\n",
    "        )\n",
    "        val_data = NewsDataset(\n",
    "            torch.tensor(X_val).float(), torch.tensor(topic_val).long(),\n",
    "            torch.tensor(subtopic_val).long(), torch.tensor(date_val).float(),\n",
    "            torch.tensor(y_val).float()\n",
    "        )\n",
    "\n",
    "        loader_train = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        loader_val = DataLoader(val_data, batch_size=self.batch_size)\n",
    "\n",
    "        model = MLPWithEmbeddings(X_train.shape[1], len(self.topic_enc.classes_), len(self.subtopic_enc.classes_)).to(self.device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5)\n",
    "        loss_fn = nn.L1Loss()\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience = 15\n",
    "        no_improve = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train()\n",
    "            for x, t, st, d, y in loader_train:\n",
    "                opt.zero_grad()\n",
    "                y_hat = model(x.to(self.device), t.to(self.device), st.to(self.device), d.to(self.device)).squeeze()\n",
    "                loss = loss_fn(y_hat, y.to(self.device))\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            model.eval()\n",
    "            losses = []\n",
    "            with torch.no_grad():\n",
    "                for x, t, st, d, y in loader_val:\n",
    "                    y_hat = model(x.to(self.device), t.to(self.device), st.to(self.device), d.to(self.device)).squeeze()\n",
    "                    losses.append(loss_fn(y_hat, y.to(self.device)).item())\n",
    "            val_loss = np.mean(losses)\n",
    "            scheduler.step(val_loss)\n",
    "            print(f\"Epoch {epoch+1:03d} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            if val_loss < best_loss - 1e-4:\n",
    "                best_loss = val_loss\n",
    "                no_improve = 0\n",
    "                best_state = model.state_dict()\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        self.model = model\n",
    "        torch.save(best_state, save_path)\n",
    "\n",
    "    def load(self, path, bert_dim):\n",
    "        self.model = MLPWithEmbeddings(bert_dim, len(self.topic_enc.classes_), len(self.subtopic_enc.classes_)).to(self.device)\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "    def predict(self, articles, bert_vectors):\n",
    "        self.prepare(articles)\n",
    "        topics, subtopics = extract_topics_from_urls([a['url'] for a in articles])\n",
    "        topic_ids = [t if t in self.topic_enc.classes_ else self.topic_enc.classes_[0] for t in topics]\n",
    "        subtopic_ids = [s if s in self.subtopic_enc.classes_ else self.subtopic_enc.classes_[0] for s in subtopics]\n",
    "\n",
    "        date_feats = prepare_date_features(articles)\n",
    "\n",
    "        x = torch.tensor(bert_vectors).float().to(self.device)\n",
    "        t = torch.tensor(self.topic_enc.transform(topic_ids)).long().to(self.device)\n",
    "        st = torch.tensor(self.subtopic_enc.transform(subtopic_ids)).long().to(self.device)\n",
    "        d = torch.tensor(date_feats).float().to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(x, t, st, d).squeeze().cpu().numpy()\n",
    "        return np.clip(np.expm1(preds), 0, None)\n",
    "\n",
    "# === Main ===\n",
    "\n",
    "def main():\n",
    "    TRAIN_MODEL = True\n",
    "\n",
    "    model_path = f\"snapshots/model_000.pt\"\n",
    "    os.makedirs(\"snapshots\", exist_ok=True)\n",
    "\n",
    "    train_data = load_json(\"rtvslo_train.json\")\n",
    "    val_data = load_json(\"rtvslo_validation.json\")\n",
    "    test_data = load_json(\"rtvslo_test.json\")\n",
    "\n",
    "    bert_train = torch.load(\"sloberta_embeddings.pt\").numpy()\n",
    "    bert_val = torch.load(\"sloberta_embeddings_val.pt\").numpy()\n",
    "    bert_test = torch.load(\"sloberta_embeddings_final.pt\").numpy()\n",
    "\n",
    "    model = RTVPredictor(model_id=0)\n",
    "\n",
    "    if TRAIN_MODEL:\n",
    "        model.train(train_data, bert_train, model_path)\n",
    "    else:\n",
    "        topics, subtopics = extract_topics_from_urls([a['url'] for a in train_data])\n",
    "        model.topic_enc = LabelEncoder().fit(topics)\n",
    "        model.subtopic_enc = LabelEncoder().fit(subtopics)\n",
    "        for a in train_data:\n",
    "            dt = pd.to_datetime(a['date'])\n",
    "            a['year'] = dt.year\n",
    "            a['month'] = dt.month\n",
    "            a['day_of_week'] = dt.weekday()\n",
    "            a['hour'] = dt.hour\n",
    "        _, model.year_scaler, model.month_scaler = prepare_date_features(train_data)\n",
    "        model.load(model_path, bert_train.shape[1])\n",
    "\n",
    "    preds_val = model.predict(val_data, bert_val)\n",
    "    preds_test = model.predict(test_data, bert_test)\n",
    "\n",
    "    np.savetxt(\"final_predictions_val.txt\", preds_val, fmt=\"%.4f\")\n",
    "    np.savetxt(\"final_predictions_test.txt\", preds_test, fmt=\"%.4f\")\n",
    "    print(\"✅ Predictions saved.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
