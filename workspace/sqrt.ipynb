{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2d5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38801/2826453792.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert_vectors = torch.load(\"train/sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train MAE: 30.96 | Val MAE: 27.81\n",
      "Epoch 2 - Train MAE: 28.03 | Val MAE: 26.51\n",
      "Epoch 3 - Train MAE: 27.24 | Val MAE: 26.29\n",
      "Epoch 4 - Train MAE: 26.65 | Val MAE: 26.04\n",
      "Epoch 5 - Train MAE: 26.16 | Val MAE: 25.43\n",
      "Epoch 6 - Train MAE: 25.78 | Val MAE: 25.35\n",
      "Epoch 7 - Train MAE: 25.56 | Val MAE: 25.95\n",
      "Epoch 8 - Train MAE: 25.15 | Val MAE: 24.94\n",
      "Epoch 9 - Train MAE: 24.83 | Val MAE: 24.25\n",
      "Epoch 10 - Train MAE: 24.49 | Val MAE: 24.62\n",
      "Epoch 11 - Train MAE: 24.28 | Val MAE: 24.85\n",
      "Epoch 12 - Train MAE: 23.90 | Val MAE: 24.39\n",
      "Epoch 13 - Train MAE: 23.75 | Val MAE: 24.22\n",
      "Epoch 14 - Train MAE: 23.50 | Val MAE: 24.23\n",
      "Epoch 15 - Train MAE: 23.18 | Val MAE: 24.42\n",
      "Epoch 16 - Train MAE: 22.97 | Val MAE: 23.88\n",
      "Epoch 17 - Train MAE: 22.71 | Val MAE: 24.01\n",
      "Epoch 18 - Train MAE: 22.49 | Val MAE: 24.73\n",
      "Epoch 19 - Train MAE: 22.09 | Val MAE: 23.78\n",
      "Epoch 20 - Train MAE: 21.94 | Val MAE: 24.09\n",
      "Epoch 21 - Train MAE: 21.61 | Val MAE: 25.15\n",
      "Epoch 22 - Train MAE: 21.27 | Val MAE: 24.29\n",
      "Epoch 23 - Train MAE: 21.11 | Val MAE: 23.88\n",
      "Epoch 24 - Train MAE: 20.89 | Val MAE: 24.08\n",
      "Epoch 25 - Train MAE: 20.60 | Val MAE: 24.10\n",
      "Epoch 26 - Train MAE: 19.78 | Val MAE: 23.85\n",
      "Epoch 27 - Train MAE: 19.53 | Val MAE: 23.87\n",
      "Epoch 28 - Train MAE: 19.38 | Val MAE: 23.77\n",
      "Epoch 29 - Train MAE: 19.16 | Val MAE: 23.50\n",
      "Epoch 30 - Train MAE: 19.00 | Val MAE: 24.02\n",
      "Epoch 31 - Train MAE: 18.88 | Val MAE: 24.91\n",
      "Epoch 32 - Train MAE: 18.57 | Val MAE: 23.49\n",
      "Epoch 33 - Train MAE: 18.46 | Val MAE: 23.64\n",
      "Epoch 34 - Train MAE: 18.17 | Val MAE: 24.28\n",
      "Epoch 35 - Train MAE: 18.14 | Val MAE: 23.54\n",
      "Epoch 36 - Train MAE: 18.03 | Val MAE: 23.79\n",
      "Epoch 37 - Train MAE: 17.68 | Val MAE: 23.23\n",
      "Epoch 38 - Train MAE: 17.68 | Val MAE: 24.00\n",
      "Epoch 39 - Train MAE: 17.32 | Val MAE: 23.48\n",
      "Epoch 40 - Train MAE: 17.20 | Val MAE: 23.67\n",
      "Epoch 41 - Train MAE: 17.15 | Val MAE: 23.59\n",
      "Epoch 42 - Train MAE: 16.95 | Val MAE: 23.32\n",
      "Epoch 43 - Train MAE: 16.72 | Val MAE: 23.95\n",
      "Epoch 44 - Train MAE: 16.34 | Val MAE: 23.62\n",
      "Epoch 45 - Train MAE: 16.18 | Val MAE: 23.50\n",
      "Epoch 46 - Train MAE: 16.06 | Val MAE: 23.76\n",
      "Epoch 47 - Train MAE: 15.99 | Val MAE: 23.42\n",
      "Epoch 48 - Train MAE: 15.99 | Val MAE: 23.50\n",
      "Epoch 49 - Train MAE: 15.82 | Val MAE: 23.55\n",
      "Epoch 50 - Train MAE: 15.64 | Val MAE: 23.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38801/2826453792.py:203: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert_vectors_validation = torch.load(\"validation/sloberta_embeddings_val.pt\", map_location=\"cpu\").numpy()\n",
      "/tmp/ipykernel_38801/2826453792.py:207: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert_vectors_test = torch.load(\"test_final/sloberta_embeddings_final.pt\", map_location=\"cpu\").numpy()\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Imports ---\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 2. Utility Functions ---\n",
    "def load(fn):\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_date_features(articles):\n",
    "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
    "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
    "    day_of_week = np.array([a['day_of_week'] for a in articles])\n",
    "    hour = np.array([a['hour'] for a in articles])\n",
    "\n",
    "    year_scaler = StandardScaler()\n",
    "    month_scaler = StandardScaler()\n",
    "    years_scaled = year_scaler.fit_transform(years)\n",
    "    months_scaled = month_scaler.fit_transform(months)\n",
    "\n",
    "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
    "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
    "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
    "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
    "\n",
    "    date_feats = np.concatenate([years_scaled, months_scaled, day_sin[:, None], day_cos[:, None], hour_sin[:, None], hour_cos[:, None]], axis=1)\n",
    "    return date_feats\n",
    "\n",
    "def extract_topics_from_url(url):\n",
    "    parts = url.split('/')\n",
    "    topic = parts[3] if len(parts) > 3 else 'none'\n",
    "    subtopic = parts[4] if len(parts) > 4 else 'none'\n",
    "    return topic, subtopic\n",
    "\n",
    "# --- 3. Dataset ---\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, bert_vectors, topic_ids, subtopic_ids, date_feats, targets):\n",
    "        self.X = torch.tensor(bert_vectors, dtype=torch.float32)\n",
    "        self.topic_ids = torch.tensor(topic_ids, dtype=torch.long)\n",
    "        self.subtopic_ids = torch.tensor(subtopic_ids, dtype=torch.long)\n",
    "        self.date_feats = torch.tensor(date_feats, dtype=torch.float32)\n",
    "        self.y = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.topic_ids[idx], self.subtopic_ids[idx], self.date_feats[idx], self.y[idx]\n",
    "\n",
    "# --- 4. Model ---\n",
    "class MLPWithEmbeddings(nn.Module):\n",
    "    def __init__(self, input_dim, num_topics, num_subtopics):\n",
    "        super().__init__()\n",
    "        self.topic_embedding = nn.Embedding(num_topics, 16)\n",
    "        self.subtopic_embedding = nn.Embedding(num_subtopics, 24)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + 16 + 24 + 6, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_text, topic_ids, subtopic_ids, date_feats):\n",
    "        topic_embed = self.topic_embedding(topic_ids)\n",
    "        subtopic_embed = self.subtopic_embedding(subtopic_ids)\n",
    "        x = torch.cat([x_text, topic_embed, subtopic_embed, date_feats], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# --- 5. Main Class ---\n",
    "class RTVSloBERT:\n",
    "    def __init__(self, batch_size=170, epochs=70, learning_rate=1e-4, l2_lambda=1e-3, eval_split=0.05):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.eval_split = eval_split\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        for a in train_data:\n",
    "            dt = pd.to_datetime(a['date'])\n",
    "            a['year'] = dt.year\n",
    "            a['month'] = dt.month\n",
    "            a['day_of_week'] = dt.weekday()\n",
    "            a['hour'] = dt.hour\n",
    "\n",
    "        raw_targets = [a['n_comments'] for a in train_data]\n",
    "        targets = [np.log1p(t) for t in raw_targets]\n",
    "        bert_vectors = torch.load(\"train/sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n",
    "\n",
    "        topics = []\n",
    "        subtopics = []\n",
    "        for a in train_data:\n",
    "            topic, subtopic = extract_topics_from_url(a['url'])\n",
    "            topics.append(topic)\n",
    "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
    "\n",
    "        date_feats = process_date_features(train_data)\n",
    "\n",
    "        self.topic_encoder = LabelEncoder().fit(topics)\n",
    "        self.subtopic_encoder = LabelEncoder().fit(subtopics)\n",
    "\n",
    "        topic_ids = self.topic_encoder.transform(topics)\n",
    "        subtopic_ids = self.subtopic_encoder.transform(subtopics)\n",
    "\n",
    "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, date_train, date_val, y_train, y_val = train_test_split(\n",
    "            bert_vectors, topic_ids, subtopic_ids, date_feats, targets, test_size=self.eval_split, random_state=42\n",
    "        )\n",
    "\n",
    "        train_dataset = NewsDataset(X_train, topic_train, subtopic_train, date_train, y_train)\n",
    "        val_dataset = NewsDataset(X_val, topic_val, subtopic_val, date_val, y_val)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "        input_dim = bert_vectors.shape[1]\n",
    "        self.model = MLPWithEmbeddings(input_dim, len(self.topic_encoder.classes_), len(self.subtopic_encoder.classes_))\n",
    "\n",
    "        self._train(self.model, train_loader, val_loader)\n",
    "\n",
    "    def _train(self, model, train_loader, val_loader):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate, weight_decay=self.l2_lambda)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "        criterion = nn.HuberLoss(delta=5.0)\n",
    "\n",
    "        best_val_mae = float('inf')\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train()\n",
    "            train_mae_list = []\n",
    "            for x_batch, topic_ids, subtopic_ids, date_feats, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats).squeeze()\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                y_pred_clamped = torch.clamp(y_pred, -10, 10)\n",
    "                y_batch_clamped = torch.clamp(y_batch, -10, 10)\n",
    "                mae = torch.mean(torch.abs(torch.expm1(y_pred_clamped) - torch.expm1(y_batch_clamped))).item()\n",
    "                train_mae_list.append(mae)\n",
    "\n",
    "            model.eval()\n",
    "            val_mae_list = []\n",
    "            with torch.no_grad():\n",
    "                for x_batch, topic_ids, subtopic_ids, date_feats, y_batch in val_loader:\n",
    "                    y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats).squeeze()\n",
    "                    y_pred_clamped = torch.clamp(y_pred, -10, 10)\n",
    "                    y_batch_clamped = torch.clamp(y_batch, -10, 10)\n",
    "                    mae = torch.mean(torch.abs(torch.expm1(y_pred_clamped) - torch.expm1(y_batch_clamped))).item()\n",
    "                    val_mae_list.append(mae)\n",
    "\n",
    "            train_mae = np.mean(train_mae_list)\n",
    "            val_mae = np.mean(val_mae_list)\n",
    "            scheduler.step(val_mae)\n",
    "\n",
    "            if val_mae < best_val_mae:\n",
    "                best_val_mae = val_mae\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "            print(f\"Epoch {epoch+1} - Train MAE: {train_mae:.2f} | Val MAE: {val_mae:.2f}\")\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "    def predict(self, test_data, bert_vectors):\n",
    "        for a in test_data:\n",
    "            dt = pd.to_datetime(a['date'])\n",
    "            a['year'] = dt.year\n",
    "            a['month'] = dt.month\n",
    "            a['day_of_week'] = dt.weekday()\n",
    "            a['hour'] = dt.hour\n",
    "\n",
    "        topics = []\n",
    "        subtopics = []\n",
    "        for a in test_data:\n",
    "            topic, subtopic = extract_topics_from_url(a['url'])\n",
    "            topics.append(topic)\n",
    "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
    "\n",
    "        date_feats = process_date_features(test_data)\n",
    "\n",
    "        topic_ids = [self.topic_encoder.classes_[0] if t not in self.topic_encoder.classes_ else t for t in topics]\n",
    "        subtopic_ids = [self.subtopic_encoder.classes_[0] if s not in self.subtopic_encoder.classes_ else s for s in subtopics]\n",
    "\n",
    "        topic_ids = self.topic_encoder.transform(topic_ids)\n",
    "        subtopic_ids = self.subtopic_encoder.transform(subtopic_ids)\n",
    "\n",
    "        X = torch.tensor(bert_vectors, dtype=torch.float32)\n",
    "        topic_ids = torch.tensor(topic_ids, dtype=torch.long)\n",
    "        subtopic_ids = torch.tensor(subtopic_ids, dtype=torch.long)\n",
    "        date_feats = torch.tensor(date_feats, dtype=torch.float32)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(X, topic_ids, subtopic_ids, date_feats).squeeze().numpy()\n",
    "            return np.clip(np.expm1(preds), 0, None)\n",
    "\n",
    "# --- 6. Ensemble Training ---\n",
    "if __name__ == '__main__':\n",
    "    train = load(\"../data/rtvslo_train.json\")\n",
    "    test = load(\"../data/rtvslo_test.json\")\n",
    "\n",
    "    bert_vectors_test = torch.load(\"test_final/sloberta_embeddings_final.pt\", map_location=\"cpu\").numpy()\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for run in range(70):\n",
    "        print(f\"ðŸ”µ Training model {run+1}/70...\")\n",
    "        model = RTVSloBERT(\n",
    "            eval_split=0.05,\n",
    "            batch_size=170,\n",
    "            epochs=70,\n",
    "            learning_rate=1e-4,\n",
    "            l2_lambda=1e-3\n",
    "        )\n",
    "        model.fit(train)\n",
    "        preds = model.predict(test, bert_vectors_test)\n",
    "        all_predictions.append(preds)\n",
    "\n",
    "    print(\"ðŸ”µ Averaging all predictions...\")\n",
    "    avg_preds = np.mean(np.stack(all_predictions), axis=0)\n",
    "    np.savetxt(\"ensemble_predictions.txt\", avg_preds, fmt=\"%.4f\")\n",
    "    print(\"âœ… Saved final averaged predictions!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "654ed5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved adj_predictions.txt with quantile smoothing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Load ---\n",
    "preds = np.loadtxt(\"predictions_val.txt\")\n",
    "\n",
    "# Optional: If you have the true labels available (for evaluation)\n",
    "# e.g., validation set where you know true n_comments\n",
    "# with open(\"../data/rtvslo_validation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     val_articles = json.load(f)\n",
    "# y_true = np.array([a['n_comments'] for a in val_articles], dtype=np.float32)\n",
    "# have_true_labels = True\n",
    "\n",
    "# --- 2. Postprocessing: Quantile Smoothing ---\n",
    "\n",
    "# Step 1: Clip extreme values\n",
    "max_allowed = 3000\n",
    "preds = np.clip(preds, 0, max_allowed)\n",
    "\n",
    "# Step 2: Quantile smoothing\n",
    "q_low = np.percentile(preds, 1)\n",
    "q_high = np.percentile(preds, 99)\n",
    "\n",
    "# Linearly squash very small and very large predictions\n",
    "def quantile_smooth(x):\n",
    "    if x < q_low:\n",
    "        return x * 0.7  # shrink low values\n",
    "    elif x > q_high:\n",
    "        return q_high # shrink heavy tails\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "vectorized_smooth = np.vectorize(quantile_smooth)\n",
    "smoothed_preds = vectorized_smooth(preds)\n",
    "\n",
    "# --- 3. Save ---\n",
    "np.savetxt(\"adj_predictions.txt\", smoothed_preds, fmt=\"%.4f\")\n",
    "\n",
    "print(\"âœ… Saved adj_predictions.txt with quantile smoothing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8027391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š MAE between predictions and true values: 30.16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. Load your predictions\n",
    "preds = np.loadtxt(\"adj_predictions.txt\")\n",
    "\n",
    "# 2. Load your true y-values from dataset_val.json\n",
    "with open(\"../data/rtvslo_validation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_articles = json.load(f)\n",
    "\n",
    "# 3. Extract true n_comments\n",
    "y_true = np.array([a[\"n_comments\"] for a in val_articles], dtype=np.float32)\n",
    "\n",
    "# 4. Check lengths\n",
    "assert len(preds) == len(y_true), f\"Length mismatch: preds={len(preds)}, y_true={len(y_true)}\"\n",
    "\n",
    "# 5. Calculate MAE\n",
    "mae = mean_absolute_error(y_true, preds)\n",
    "\n",
    "print(f\"ðŸ“Š MAE between predictions and true values: {mae:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
