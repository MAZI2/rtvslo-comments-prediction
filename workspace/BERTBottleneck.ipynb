{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b7c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val MAE: 30.73\n",
      "Epoch 2 - Val MAE: 28.48\n",
      "Epoch 3 - Val MAE: 27.38\n",
      "Epoch 4 - Val MAE: 26.75\n",
      "Epoch 5 - Val MAE: 26.53\n",
      "Epoch 6 - Val MAE: 26.55\n",
      "Epoch 7 - Val MAE: 26.46\n",
      "Epoch 8 - Val MAE: 25.62\n",
      "Epoch 9 - Val MAE: 25.82\n",
      "Epoch 10 - Val MAE: 25.46\n",
      "Epoch 11 - Val MAE: 25.44\n",
      "Epoch 12 - Val MAE: 25.38\n",
      "Epoch 13 - Val MAE: 25.61\n",
      "Epoch 14 - Val MAE: 25.47\n",
      "Epoch 15 - Val MAE: 25.04\n",
      "Epoch 16 - Val MAE: 24.92\n",
      "Epoch 17 - Val MAE: 24.99\n",
      "Epoch 18 - Val MAE: 25.05\n",
      "Epoch 19 - Val MAE: 24.78\n",
      "Epoch 20 - Val MAE: 24.77\n",
      "Epoch 21 - Val MAE: 24.76\n",
      "Epoch 22 - Val MAE: 24.61\n",
      "Epoch 23 - Val MAE: 26.01\n",
      "Epoch 24 - Val MAE: 24.55\n",
      "Epoch 25 - Val MAE: 24.59\n",
      "Epoch 26 - Val MAE: 24.52\n",
      "Epoch 27 - Val MAE: 24.56\n",
      "Epoch 28 - Val MAE: 24.37\n",
      "Epoch 29 - Val MAE: 24.31\n",
      "Epoch 30 - Val MAE: 24.45\n",
      "Epoch 31 - Val MAE: 24.37\n",
      "Epoch 32 - Val MAE: 24.08\n",
      "Epoch 33 - Val MAE: 24.40\n",
      "Epoch 34 - Val MAE: 24.22\n",
      "Epoch 35 - Val MAE: 24.13\n",
      "Epoch 36 - Val MAE: 24.11\n",
      "Epoch 37 - Val MAE: 24.06\n",
      "Epoch 38 - Val MAE: 24.47\n",
      "Epoch 39 - Val MAE: 24.10\n",
      "Epoch 40 - Val MAE: 24.00\n",
      "Epoch 41 - Val MAE: 23.92\n",
      "Epoch 42 - Val MAE: 23.86\n",
      "Epoch 43 - Val MAE: 23.91\n",
      "Epoch 44 - Val MAE: 23.93\n",
      "Epoch 45 - Val MAE: 23.97\n",
      "Epoch 46 - Val MAE: 24.01\n",
      "Epoch 47 - Val MAE: 24.09\n",
      "Epoch 48 - Val MAE: 23.73\n",
      "Epoch 49 - Val MAE: 23.88\n",
      "Epoch 50 - Val MAE: 23.87\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Imports ---\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "# --- 2. Utility Functions ---\n",
    "def load(fn):\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_topics_from_url(url):\n",
    "    parts = url.split('/')\n",
    "    topic = parts[3] if len(parts) > 3 else 'none'\n",
    "    subtopic = parts[4] if len(parts) > 4 else 'none'\n",
    "    return topic, subtopic\n",
    "\n",
    "def enrich_articles(articles):\n",
    "    for a in articles:\n",
    "        dt = pd.to_datetime(a['date'])\n",
    "        a['year'] = dt.year\n",
    "        a['month'] = dt.month\n",
    "        a['day_of_week'] = dt.weekday()\n",
    "        a['hour'] = dt.hour\n",
    "    return articles\n",
    "\n",
    "# --- 3. Dataset ---\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, topic_ids, subtopic_ids, hour_ids, weekday_ids, targets):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.topic_ids = torch.tensor(topic_ids, dtype=torch.long)\n",
    "        self.subtopic_ids = torch.tensor(subtopic_ids, dtype=torch.long)\n",
    "        self.hour_ids = torch.tensor(hour_ids, dtype=torch.long)\n",
    "        self.weekday_ids = torch.tensor(weekday_ids, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.X[idx],\n",
    "            self.topic_ids[idx],\n",
    "            self.subtopic_ids[idx],\n",
    "            self.hour_ids[idx],\n",
    "            self.weekday_ids[idx],\n",
    "            self.targets[idx]\n",
    "        )\n",
    "\n",
    "# --- 4. Model ---\n",
    "class MLPWithBottleneckAndTopics(nn.Module):\n",
    "    def __init__(self, input_dim, num_topics, num_subtopics):\n",
    "        super().__init__()\n",
    "\n",
    "        # Bottleneck MLP on top of text embeddings\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Embeddings\n",
    "        self.topic_emb = nn.Embedding(num_topics, 16)\n",
    "        self.subtopic_emb = nn.Embedding(num_subtopics, 24)\n",
    "        self.hour_emb = nn.Embedding(24, 4)\n",
    "        self.weekday_emb = nn.Embedding(7, 3)\n",
    "\n",
    "        # Total input size\n",
    "        total_input = 128 + 16 + 24 + 4 + 3  # bottleneck + topic + subtopic + hour + weekday\n",
    "\n",
    "        # Final prediction MLP\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(total_input, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, topic_id, subtopic_id, hour_id, weekday_id):\n",
    "        x = self.bottleneck(x)  # ðŸ”¥ transform text embeddings\n",
    "\n",
    "        topic_vec = self.topic_emb(topic_id)\n",
    "        subtopic_vec = self.subtopic_emb(subtopic_id)\n",
    "        hour_vec = self.hour_emb(hour_id)\n",
    "        weekday_vec = self.weekday_emb(weekday_id)\n",
    "\n",
    "        all_features = torch.cat([x, topic_vec, subtopic_vec, hour_vec, weekday_vec], dim=1)\n",
    "        return self.net(all_features).squeeze()\n",
    "\n",
    "# --- 5. Trainer ---\n",
    "class RTVSloBottleneck:\n",
    "    def __init__(self, batch_size=128, epochs=50, lr=1e-4, eval_split=0.05):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.eval_split = eval_split\n",
    "\n",
    "    def fit(self, articles, val_articles=None):\n",
    "        articles = enrich_articles(articles)\n",
    "        y = [a['n_comments'] for a in articles]\n",
    "\n",
    "        bert = torch.load(\"sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n",
    "        topics, subtopics = zip(*[extract_topics_from_url(a['url']) for a in articles])\n",
    "        subtopics = [s if s != \"NO_SUBTOPIC\" else \"none\" for s in subtopics]\n",
    "\n",
    "        self.topic_enc = LabelEncoder().fit(topics)\n",
    "        self.subtopic_enc = LabelEncoder().fit(subtopics)\n",
    "        topic_ids = self.topic_enc.transform(topics)\n",
    "        subtopic_ids = self.subtopic_enc.transform(subtopics)\n",
    "\n",
    "        hour_ids = np.array([a['hour'] for a in articles], dtype=np.int64)\n",
    "        weekday_ids = np.array([a['day_of_week'] for a in articles], dtype=np.int64)\n",
    "\n",
    "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, hour_train, hour_val, weekday_train, weekday_val, y_train, y_val = train_test_split(\n",
    "            bert, topic_ids, subtopic_ids, hour_ids, weekday_ids, y, test_size=self.eval_split, random_state=42\n",
    "        )\n",
    "\n",
    "        train_data = NewsDataset(X_train, topic_train, subtopic_train, hour_train, weekday_train, y_train)\n",
    "        val_data = NewsDataset(X_val, topic_val, subtopic_val, hour_val, weekday_val, y_val)\n",
    "\n",
    "        loader_train = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        loader_val = DataLoader(val_data, batch_size=self.batch_size)\n",
    "\n",
    "        self.model = MLPWithBottleneckAndTopics(X_train.shape[1], len(self.topic_enc.classes_), len(self.subtopic_enc.classes_))\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "        best_mae = float('inf')\n",
    "        best_state = None\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            for batch in loader_train:\n",
    "                x, t, st, h, wd, y = batch\n",
    "                optimizer.zero_grad()\n",
    "                pred = self.model(x, t, st, h, wd)\n",
    "                loss = criterion(pred, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_preds, val_true = [], []\n",
    "                for batch in loader_val:\n",
    "                    x, t, st, h, wd, y = batch\n",
    "                    pred = self.model(x, t, st, h, wd)\n",
    "                    val_preds.append(pred)\n",
    "                    val_true.append(y)\n",
    "                val_preds = torch.cat(val_preds).numpy()\n",
    "                val_true = torch.cat(val_true).numpy()\n",
    "                val_mae = mean_absolute_error(val_true, val_preds)\n",
    "                print(f\"Epoch {epoch+1} - Val MAE: {val_mae:.2f}\")\n",
    "\n",
    "            if val_mae < best_mae:\n",
    "                best_mae = val_mae\n",
    "                best_state = self.model.state_dict()\n",
    "\n",
    "        if best_state:\n",
    "            self.model.load_state_dict(best_state)\n",
    "\n",
    "    def predict(self, test_articles):\n",
    "        test_articles = enrich_articles(test_articles)\n",
    "        bert = torch.load(\"sloberta_embeddings_val.pt\", map_location=\"cpu\").numpy()\n",
    "\n",
    "        topics, subtopics = zip(*[extract_topics_from_url(a['url']) for a in test_articles])\n",
    "        subtopics = [s if s != \"NO_SUBTOPIC\" else \"none\" for s in subtopics]\n",
    "        topic_ids = [self.topic_enc.classes_[0] if t not in self.topic_enc.classes_ else t for t in topics]\n",
    "        subtopic_ids = [self.subtopic_enc.classes_[0] if s not in self.subtopic_enc.classes_ else s for s in subtopics]\n",
    "        topic_ids = self.topic_enc.transform(topic_ids)\n",
    "        subtopic_ids = self.subtopic_enc.transform(subtopic_ids)\n",
    "\n",
    "        hour_ids = np.array([a['hour'] for a in test_articles], dtype=np.int64)\n",
    "        weekday_ids = np.array([a['day_of_week'] for a in test_articles], dtype=np.int64)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(bert, dtype=torch.float32)\n",
    "            t = torch.tensor(topic_ids, dtype=torch.long)\n",
    "            st = torch.tensor(subtopic_ids, dtype=torch.long)\n",
    "            h = torch.tensor(hour_ids, dtype=torch.long)\n",
    "            wd = torch.tensor(weekday_ids, dtype=torch.long)\n",
    "\n",
    "            preds = self.model(x, t, st, h, wd).numpy()\n",
    "            return preds\n",
    "\n",
    "# --- 6. Main ---\n",
    "if __name__ == '__main__':\n",
    "    train = load(\"../data/rtvslo_train.json\")\n",
    "    test = load(\"../data/rtvslo_test.json\")\n",
    "\n",
    "    m = RTVSloBottleneck()\n",
    "    m.fit(train)\n",
    "    preds = m.predict(test)\n",
    "\n",
    "    np.savetxt(\"predictions.txt\", preds, fmt=\"%.4f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce0fdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š MAE between predictions and true values: 28.77\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. Load your predictions\n",
    "preds = np.loadtxt(\"predictions.txt\")\n",
    "\n",
    "# 2. Load your true y-values from dataset_val.json\n",
    "with open(\"../data/rtvslo_validation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_articles = json.load(f)\n",
    "\n",
    "# 3. Extract true n_comments\n",
    "y_true = np.array([a[\"n_comments\"] for a in val_articles], dtype=np.float32)\n",
    "\n",
    "# 4. Check lengths\n",
    "assert len(preds) == len(y_true), f\"Length mismatch: preds={len(preds)}, y_true={len(y_true)}\"\n",
    "\n",
    "# 5. Calculate MAE\n",
    "mae = mean_absolute_error(y_true, preds)\n",
    "\n",
    "print(f\"ðŸ“Š MAE between predictions and true values: {mae:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
