{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EMBEDDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load multilingual SBERT model (MPNet backbone, supports Slovene)\n",
        "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "model.eval()\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_json(\"rtvslo_validation.json\")  # assumes a list of article dicts\n",
        "\n",
        "df['full_text'] = df['title'] + \" \" + df['lead'] + \" \" + df['paragraphs'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "# Encode all articles\n",
        "embeddings = []\n",
        "for text in tqdm(df['full_text']):\n",
        "    emb = model.encode(text, convert_to_tensor=True)  # returns a torch.Tensor\n",
        "    embeddings.append(emb)\n",
        "\n",
        "# Stack into one tensor\n",
        "embeddings_tensor = torch.stack(embeddings)\n",
        "torch.save(embeddings_tensor, \"text_embeddings_val.pt\")\n",
        "\n",
        "# Also save the targets\n",
        "targets = torch.tensor(df['n_comments'].values, dtype=torch.float32)\n",
        "torch.save(targets, \"targets_val.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkj8-QSho3Hf",
        "outputId": "f24699b2-afd2-46bf-c159-efb858fec331"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of CamembertModel were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2218/2218 [01:16<00:00, 29.01it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Slovene SloBERTa model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EMBEDDIA/sloberta\")\n",
        "model = AutoModel.from_pretrained(\"EMBEDDIA/sloberta\").to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_json(\"rtvslo_test.json\")\n",
        "df['full_text'] = df['title'] + \" \" + df['lead'] + \" \" + df['paragraphs'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "# Optional: Mean pooling function\n",
        "def mean_pooling(outputs, attention_mask):\n",
        "    token_embeddings = outputs.last_hidden_state  # (batch, seq_len, hidden)\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, dim=1) / \\\n",
        "           torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
        "\n",
        "# Function to embed a single article using mean pooling\n",
        "def embed_article(text):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512,\n",
        "        return_token_type_ids=False\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        pooled = mean_pooling(outputs, inputs['attention_mask'])\n",
        "    return pooled.squeeze(0)\n",
        "\n",
        "# Embed all articles\n",
        "embeddings = []\n",
        "for text in tqdm(df['full_text']):\n",
        "    emb = embed_article(text)\n",
        "    embeddings.append(emb.cpu())  # move back to CPU to stack later\n",
        "\n",
        "# Stack and save embeddings\n",
        "embeddings_tensor = torch.stack(embeddings)\n",
        "torch.save(embeddings_tensor, \"sloberta_embeddings_final.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BAGGING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## + calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH3zI2YWEcfQ",
        "outputId": "53d08d48-c47c-47a2-d182-113be78792c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîµ Loading data...\n",
            "üîµ Running PCA...\n",
            "‚úÖ PCA done. Shape: (46830, 50)\n",
            "üîµ Preparing features...\n",
            "üîµ Training LightGBM...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttrain's quantile: 0.312468\tvalid's quantile: 0.391089\n",
            "[200]\ttrain's quantile: 0.268099\tvalid's quantile: 0.37803\n",
            "[300]\ttrain's quantile: 0.248439\tvalid's quantile: 0.375506\n",
            "[400]\ttrain's quantile: 0.237404\tvalid's quantile: 0.373446\n",
            "[500]\ttrain's quantile: 0.229766\tvalid's quantile: 0.372943\n",
            "[600]\ttrain's quantile: 0.222879\tvalid's quantile: 0.372449\n",
            "[700]\ttrain's quantile: 0.217853\tvalid's quantile: 0.371897\n",
            "[800]\ttrain's quantile: 0.212485\tvalid's quantile: 0.372245\n",
            "Early stopping, best iteration is:\n",
            "[767]\ttrain's quantile: 0.214405\tvalid's quantile: 0.371737\n",
            "üîµ Predicting...\n",
            "üîµ Calibrating predictions...\n",
            "üîµ Saving predictions...\n",
            "üîµ Evaluating...\n",
            "‚úÖ MAE Raw          : 29.9895\n",
            "‚úÖ MAE Calibrated   : 28.9768\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Imports ---\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import lightgbm as lgb\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# --- 2. Utility Functions ---\n",
        "def load_json(fn):\n",
        "    with open(fn, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def extract_topics_subtopics(articles):\n",
        "    topics, subtopics = [], []\n",
        "    for a in articles:\n",
        "        parts = a['url'].split('/')\n",
        "        topic = parts[3] if len(parts) > 3 else 'none'\n",
        "        subtopic = parts[4] if len(parts) > 4 else 'none'\n",
        "        topics.append(topic)\n",
        "        subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
        "    return topics, subtopics\n",
        "\n",
        "def enrich_articles(articles):\n",
        "    for a in articles:\n",
        "        dt = pd.to_datetime(a['date'])\n",
        "        a['year'] = dt.year\n",
        "        a['month'] = dt.month\n",
        "        a['day_of_week'] = dt.weekday()\n",
        "        a['hour'] = dt.hour\n",
        "    return articles\n",
        "\n",
        "def process_date_features(articles):\n",
        "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
        "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
        "    day_of_week = np.array([a['day_of_week'] for a in articles])\n",
        "    hour = np.array([a['hour'] for a in articles])\n",
        "\n",
        "    year_scaler = StandardScaler()\n",
        "    month_scaler = StandardScaler()\n",
        "    years_scaled = year_scaler.fit_transform(years)\n",
        "    months_scaled = month_scaler.fit_transform(months)\n",
        "\n",
        "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
        "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
        "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
        "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
        "\n",
        "    date_feats = np.concatenate([years_scaled, months_scaled, day_sin[:, None], day_cos[:, None], hour_sin[:, None], hour_cos[:, None]], axis=1)\n",
        "    return date_feats\n",
        "\n",
        "def prepare_features(articles, bert_reduced, topic_enc=None, subtopic_enc=None, fit_encoders=False):\n",
        "    topics, subtopics = extract_topics_subtopics(articles)\n",
        "\n",
        "    if fit_encoders:\n",
        "        topic_enc = LabelEncoder().fit(topics)\n",
        "        subtopic_enc = LabelEncoder().fit(subtopics)\n",
        "\n",
        "    topics = [t if t in topic_enc.classes_ else topic_enc.classes_[0] for t in topics]\n",
        "    subtopics = [s if s in subtopic_enc.classes_ else subtopic_enc.classes_[0] for s in subtopics]\n",
        "\n",
        "    topic_ids = topic_enc.transform(topics)\n",
        "    subtopic_ids = subtopic_enc.transform(subtopics)\n",
        "\n",
        "    time_feats = process_date_features(articles)\n",
        "\n",
        "    features = np.hstack([bert_reduced, topic_ids[:, None], subtopic_ids[:, None], time_feats])\n",
        "    return features, topic_enc, subtopic_enc\n",
        "\n",
        "def train_lightgbm(X_train, y_train, X_val, y_val, weights_train, weights_val):\n",
        "    params = {\n",
        "        'objective': 'quantile',     # quantile regression (median)\n",
        "        'alpha': 0.5,                # 0.5 = median\n",
        "        'learning_rate': 0.05,\n",
        "        'num_leaves': 128,\n",
        "        'max_depth': -1,\n",
        "        'min_data_in_leaf': 30,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'device_type': 'cpu',\n",
        "        'verbosity': -1,\n",
        "        'seed': 42\n",
        "    }\n",
        "\n",
        "    train_set = lgb.Dataset(X_train, label=y_train, weight=weights_train)\n",
        "    val_set = lgb.Dataset(X_val, label=y_val, weight=weights_val)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_set,\n",
        "        valid_sets=[train_set, val_set],\n",
        "        valid_names=['train', 'valid'],\n",
        "        num_boost_round=5000,\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=100),\n",
        "            lgb.log_evaluation(period=100)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def calibrate_predictions(preds, train_targets_sorted, pred_train_sorted):\n",
        "    \"\"\"Postprocessing: Match percentiles between train and predictions.\"\"\"\n",
        "    ranks = np.searchsorted(pred_train_sorted, preds, side='left') / len(pred_train_sorted)\n",
        "    calibrated = np.quantile(train_targets_sorted, ranks)\n",
        "    return calibrated\n",
        "\n",
        "# --- 3. Load and Prepare Data ---\n",
        "print(\"üîµ Loading data...\")\n",
        "train_articles = load_json(\"rtvslo_train.json\")\n",
        "val_articles = load_json(\"rtvslo_validation.json\")\n",
        "\n",
        "train_articles = enrich_articles(train_articles)\n",
        "val_articles = enrich_articles(val_articles)\n",
        "\n",
        "bert_train = torch.load(\"sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n",
        "bert_val = torch.load(\"sloberta_embeddings_val.pt\", map_location=\"cpu\").numpy()\n",
        "\n",
        "# --- 4. Dimensionality Reduction ---\n",
        "print(\"üîµ Running PCA...\")\n",
        "pca_model = PCA(n_components=50, random_state=42)\n",
        "bert_train_reduced = pca_model.fit_transform(bert_train)\n",
        "bert_val_reduced = pca_model.transform(bert_val)\n",
        "\n",
        "print(f\"‚úÖ PCA done. Shape: {bert_train_reduced.shape}\")\n",
        "\n",
        "# --- 5. Prepare Features\n",
        "print(\"üîµ Preparing features...\")\n",
        "X_train, topic_enc, subtopic_enc = prepare_features(train_articles, bert_train_reduced, fit_encoders=True)\n",
        "X_val, _, _ = prepare_features(val_articles, bert_val_reduced, topic_enc, subtopic_enc)\n",
        "\n",
        "y_train_raw = np.array([a['n_comments'] for a in train_articles], dtype=np.float32)\n",
        "np.savetxt(\"train_targets.txt\", y_train_raw, fmt=\"%.4f\")\n",
        "\n",
        "y_val_raw = np.array([a['n_comments'] for a in val_articles], dtype=np.float32)\n",
        "\n",
        "y_train = np.log1p(y_train_raw)\n",
        "y_val = np.log1p(y_val_raw)\n",
        "\n",
        "# Weights: higher for larger original n_comments\n",
        "weights_train = np.sqrt(y_train_raw + 1)\n",
        "weights_val = np.sqrt(y_val_raw + 1)\n",
        "\n",
        "# --- 6. Train LightGBM\n",
        "print(\"üîµ Training LightGBM...\")\n",
        "model = train_lightgbm(X_train, y_train, X_val, y_val, weights_train, weights_val)\n",
        "\n",
        "# --- 7. Predict\n",
        "print(\"üîµ Predicting...\")\n",
        "val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "\n",
        "# Undo log1p\n",
        "val_preds_exp = np.clip(np.expm1(val_preds), 0, None)\n",
        "\n",
        "# --- 8. Postprocessing Calibration\n",
        "print(\"üîµ Calibrating predictions...\")\n",
        "pred_train = model.predict(X_train, num_iteration=model.best_iteration)\n",
        "train_preds_exp = np.clip(np.expm1(pred_train), 0, None)\n",
        "\n",
        "train_targets_sorted = np.sort(y_train_raw)\n",
        "pred_train_sorted = np.sort(train_preds_exp)\n",
        "\n",
        "val_preds_calibrated = calibrate_predictions(val_preds_exp, train_targets_sorted, pred_train_sorted)\n",
        "\n",
        "# --- 9. Save and Evaluate\n",
        "print(\"üîµ Saving predictions...\")\n",
        "np.savetxt(\"predictions_val_raw.txt\", val_preds_exp, fmt=\"%.4f\")\n",
        "np.savetxt(\"predictions_val_calibrated.txt\", val_preds_calibrated, fmt=\"%.4f\")\n",
        "\n",
        "print(\"üîµ Evaluating...\")\n",
        "mae_raw = mean_absolute_error(y_val_raw, val_preds_exp)\n",
        "mae_calibrated = mean_absolute_error(y_val_raw, val_preds_calibrated)\n",
        "\n",
        "print(f\"‚úÖ MAE Raw          : {mae_raw:.4f}\")\n",
        "print(f\"‚úÖ MAE Calibrated   : {mae_calibrated:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ignlDfafaHuA",
        "outputId": "ca45ad80-13ac-4519-fc79-a166679315fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Training model 1/70...\n",
            "Epoch 10/70 - Validation loss: 0.4572\n",
            "Epoch 20/70 - Validation loss: 0.4427\n",
            "Epoch 30/70 - Validation loss: 0.4370\n",
            "Epoch 40/70 - Validation loss: 0.4420\n",
            "Epoch 50/70 - Validation loss: 0.4338\n",
            "Epoch 60/70 - Validation loss: 0.4317\n",
            "Epoch 70/70 - Validation loss: 0.4329\n",
            "\n",
            "üöÄ Training model 2/70...\n",
            "Epoch 10/70 - Validation loss: 0.4610\n",
            "Epoch 20/70 - Validation loss: 0.4479\n",
            "Epoch 30/70 - Validation loss: 0.4452\n",
            "Epoch 40/70 - Validation loss: 0.4339\n",
            "Epoch 50/70 - Validation loss: 0.4396\n",
            "Epoch 60/70 - Validation loss: 0.4395\n",
            "Epoch 70/70 - Validation loss: 0.4395\n",
            "\n",
            "üöÄ Training model 3/70...\n",
            "Epoch 10/70 - Validation loss: 0.4614\n",
            "Epoch 20/70 - Validation loss: 0.4607\n",
            "Epoch 30/70 - Validation loss: 0.4514\n",
            "Epoch 40/70 - Validation loss: 0.4396\n",
            "Epoch 50/70 - Validation loss: 0.4495\n",
            "Epoch 60/70 - Validation loss: 0.4346\n",
            "Epoch 70/70 - Validation loss: 0.4347\n",
            "\n",
            "üöÄ Training model 4/70...\n",
            "Epoch 10/70 - Validation loss: 0.4661\n",
            "Epoch 20/70 - Validation loss: 0.4540\n",
            "Epoch 30/70 - Validation loss: 0.4381\n",
            "Epoch 40/70 - Validation loss: 0.4334\n",
            "Epoch 50/70 - Validation loss: 0.4345\n",
            "Epoch 60/70 - Validation loss: 0.4319\n",
            "Epoch 70/70 - Validation loss: 0.4321\n",
            "\n",
            "üöÄ Training model 5/70...\n",
            "Epoch 10/70 - Validation loss: 0.4708\n",
            "Epoch 20/70 - Validation loss: 0.4539\n",
            "Epoch 30/70 - Validation loss: 0.4381\n",
            "Epoch 40/70 - Validation loss: 0.4329\n",
            "Epoch 50/70 - Validation loss: 0.4315\n",
            "Epoch 60/70 - Validation loss: 0.4302\n",
            "Epoch 70/70 - Validation loss: 0.4300\n",
            "\n",
            "üöÄ Training model 6/70...\n",
            "Epoch 10/70 - Validation loss: 0.4674\n",
            "Epoch 20/70 - Validation loss: 0.4559\n",
            "Epoch 30/70 - Validation loss: 0.4415\n",
            "Epoch 40/70 - Validation loss: 0.4445\n",
            "Epoch 50/70 - Validation loss: 0.4366\n",
            "Epoch 60/70 - Validation loss: 0.4354\n",
            "Epoch 70/70 - Validation loss: 0.4371\n",
            "\n",
            "üöÄ Training model 7/70...\n",
            "Epoch 10/70 - Validation loss: 0.4661\n",
            "Epoch 20/70 - Validation loss: 0.4512\n",
            "Epoch 30/70 - Validation loss: 0.4427\n",
            "Epoch 40/70 - Validation loss: 0.4379\n",
            "Epoch 50/70 - Validation loss: 0.4311\n",
            "Epoch 60/70 - Validation loss: 0.4314\n",
            "Epoch 70/70 - Validation loss: 0.4283\n",
            "\n",
            "üöÄ Training model 8/70...\n",
            "Epoch 10/70 - Validation loss: 0.4611\n",
            "Epoch 20/70 - Validation loss: 0.4500\n",
            "Epoch 30/70 - Validation loss: 0.4301\n",
            "Epoch 40/70 - Validation loss: 0.4290\n",
            "Epoch 50/70 - Validation loss: 0.4305\n",
            "Epoch 60/70 - Validation loss: 0.4281\n",
            "Epoch 70/70 - Validation loss: 0.4268\n",
            "\n",
            "üöÄ Training model 9/70...\n",
            "Epoch 10/70 - Validation loss: 0.4763\n",
            "Epoch 20/70 - Validation loss: 0.4470\n",
            "Epoch 30/70 - Validation loss: 0.4434\n",
            "Epoch 40/70 - Validation loss: 0.4434\n",
            "Epoch 50/70 - Validation loss: 0.4494\n",
            "Epoch 60/70 - Validation loss: 0.4413\n",
            "Epoch 70/70 - Validation loss: 0.4285\n",
            "\n",
            "üöÄ Training model 10/70...\n",
            "Epoch 10/70 - Validation loss: 0.4573\n",
            "Epoch 20/70 - Validation loss: 0.4601\n",
            "Epoch 30/70 - Validation loss: 0.4471\n",
            "Epoch 40/70 - Validation loss: 0.4380\n",
            "Epoch 50/70 - Validation loss: 0.4337\n",
            "Epoch 60/70 - Validation loss: 0.4354\n",
            "Epoch 70/70 - Validation loss: 0.4288\n",
            "\n",
            "üöÄ Training model 11/70...\n",
            "Epoch 10/70 - Validation loss: 0.4562\n",
            "Epoch 20/70 - Validation loss: 0.4423\n",
            "Epoch 30/70 - Validation loss: 0.4384\n",
            "Epoch 40/70 - Validation loss: 0.4417\n",
            "Epoch 50/70 - Validation loss: 0.4359\n",
            "Epoch 60/70 - Validation loss: 0.4335\n",
            "Epoch 70/70 - Validation loss: 0.4352\n",
            "\n",
            "üöÄ Training model 12/70...\n",
            "Epoch 10/70 - Validation loss: 0.4559\n",
            "Epoch 20/70 - Validation loss: 0.4468\n",
            "Epoch 30/70 - Validation loss: 0.4519\n",
            "Epoch 40/70 - Validation loss: 0.4448\n",
            "Epoch 50/70 - Validation loss: 0.4359\n",
            "Epoch 60/70 - Validation loss: 0.4362\n",
            "Epoch 70/70 - Validation loss: 0.4365\n",
            "\n",
            "üöÄ Training model 13/70...\n",
            "Epoch 10/70 - Validation loss: 0.4790\n",
            "Epoch 20/70 - Validation loss: 0.4463\n",
            "Epoch 30/70 - Validation loss: 0.4385\n",
            "Epoch 40/70 - Validation loss: 0.4425\n",
            "Epoch 50/70 - Validation loss: 0.4377\n",
            "Epoch 60/70 - Validation loss: 0.4399\n",
            "Epoch 70/70 - Validation loss: 0.4395\n",
            "\n",
            "üöÄ Training model 14/70...\n",
            "Epoch 10/70 - Validation loss: 0.4580\n",
            "Epoch 20/70 - Validation loss: 0.4525\n",
            "Epoch 30/70 - Validation loss: 0.4495\n",
            "Epoch 40/70 - Validation loss: 0.4371\n",
            "Epoch 50/70 - Validation loss: 0.4346\n",
            "Epoch 60/70 - Validation loss: 0.4374\n",
            "Epoch 70/70 - Validation loss: 0.4365\n",
            "\n",
            "üöÄ Training model 15/70...\n",
            "Epoch 10/70 - Validation loss: 0.4789\n",
            "Epoch 20/70 - Validation loss: 0.4483\n",
            "Epoch 30/70 - Validation loss: 0.4417\n",
            "Epoch 40/70 - Validation loss: 0.4397\n",
            "Epoch 50/70 - Validation loss: 0.4402\n",
            "Epoch 60/70 - Validation loss: 0.4379\n",
            "Epoch 70/70 - Validation loss: 0.4381\n",
            "\n",
            "üöÄ Training model 16/70...\n",
            "Epoch 10/70 - Validation loss: 0.4564\n",
            "Epoch 20/70 - Validation loss: 0.4569\n",
            "Epoch 30/70 - Validation loss: 0.4576\n",
            "Epoch 40/70 - Validation loss: 0.4401\n",
            "Epoch 50/70 - Validation loss: 0.4378\n",
            "Epoch 60/70 - Validation loss: 0.4394\n",
            "Epoch 70/70 - Validation loss: 0.4394\n",
            "\n",
            "üöÄ Training model 17/70...\n",
            "Epoch 10/70 - Validation loss: 0.4554\n",
            "Epoch 20/70 - Validation loss: 0.4430\n",
            "Epoch 30/70 - Validation loss: 0.4383\n",
            "Epoch 40/70 - Validation loss: 0.4341\n",
            "Epoch 50/70 - Validation loss: 0.4343\n",
            "Epoch 60/70 - Validation loss: 0.4317\n",
            "Epoch 70/70 - Validation loss: 0.4338\n",
            "\n",
            "üöÄ Training model 18/70...\n",
            "Epoch 10/70 - Validation loss: 0.4576\n",
            "Epoch 20/70 - Validation loss: 0.4392\n",
            "Epoch 30/70 - Validation loss: 0.4363\n",
            "Epoch 40/70 - Validation loss: 0.4384\n",
            "Epoch 50/70 - Validation loss: 0.4381\n",
            "Epoch 60/70 - Validation loss: 0.4367\n",
            "Epoch 70/70 - Validation loss: 0.4393\n",
            "\n",
            "üöÄ Training model 19/70...\n",
            "Epoch 10/70 - Validation loss: 0.4568\n",
            "Epoch 20/70 - Validation loss: 0.4476\n",
            "Epoch 30/70 - Validation loss: 0.4525\n",
            "Epoch 40/70 - Validation loss: 0.4510\n",
            "Epoch 50/70 - Validation loss: 0.4414\n",
            "Epoch 60/70 - Validation loss: 0.4370\n",
            "Epoch 70/70 - Validation loss: 0.4394\n",
            "\n",
            "üöÄ Training model 20/70...\n",
            "Epoch 10/70 - Validation loss: 0.4667\n",
            "Epoch 20/70 - Validation loss: 0.4450\n",
            "Epoch 30/70 - Validation loss: 0.4436\n",
            "Epoch 40/70 - Validation loss: 0.4364\n",
            "Epoch 50/70 - Validation loss: 0.4381\n",
            "Epoch 60/70 - Validation loss: 0.4385\n",
            "Epoch 70/70 - Validation loss: 0.4385\n",
            "‚úÖ Done!\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Imports ---\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 2. Utility Functions ---\n",
        "def load(fn):\n",
        "    with open(fn, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def process_date_features(articles):\n",
        "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
        "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
        "    day_of_week = np.array([a['day_of_week'] for a in articles])\n",
        "    hour = np.array([a['hour'] for a in articles])\n",
        "\n",
        "    year_scaler = StandardScaler()\n",
        "    month_scaler = StandardScaler()\n",
        "    years_scaled = year_scaler.fit_transform(years)\n",
        "    months_scaled = month_scaler.fit_transform(months)\n",
        "\n",
        "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
        "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
        "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
        "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
        "\n",
        "    date_feats = np.concatenate([years_scaled, months_scaled, day_sin[:, None], day_cos[:, None], hour_sin[:, None], hour_cos[:, None]], axis=1)\n",
        "    return date_feats\n",
        "\n",
        "def extract_topics_from_url(url):\n",
        "    parts = url.split('/')\n",
        "    topic = parts[3] if len(parts) > 3 else 'none'\n",
        "    subtopic = parts[4] if len(parts) > 4 else 'none'\n",
        "    return topic, subtopic\n",
        "\n",
        "# --- 3. Dataset ---\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, bert_vectors, topic_ids, subtopic_ids, date_feats, targets):\n",
        "        self.X = bert_vectors\n",
        "        self.topic_ids = topic_ids\n",
        "        self.subtopic_ids = subtopic_ids\n",
        "        self.date_feats = date_feats\n",
        "        self.y = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.topic_ids[idx], self.subtopic_ids[idx], self.date_feats[idx], self.y[idx]\n",
        "\n",
        "# --- 4. Model ---\n",
        "class MLPWithEmbeddings(nn.Module):\n",
        "    def __init__(self, input_dim, num_topics, num_subtopics):\n",
        "        super().__init__()\n",
        "        self.topic_embedding = nn.Embedding(num_topics, 16)\n",
        "        self.subtopic_embedding = nn.Embedding(num_subtopics, 24)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim + 16 + 24 + 6, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "    def forward(self, x_text, topic_ids, subtopic_ids, date_feats):\n",
        "        topic_embed = self.topic_embedding(topic_ids)\n",
        "        subtopic_embed = self.subtopic_embedding(subtopic_ids)\n",
        "        x = torch.cat([x_text, topic_embed, subtopic_embed, date_feats], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# --- 5. Main Trainer Class ---\n",
        "class RTVSloBERT:\n",
        "    def __init__(self, batch_size=170, epochs=70, learning_rate=1e-4, l2_lambda=1e-3, eval_split=0.05, model_id=0):\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.eval_split = eval_split\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_id = model_id\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        for a in train_data:\n",
        "            dt = pd.to_datetime(a['date'])\n",
        "            a['year'] = dt.year\n",
        "            a['month'] = dt.month\n",
        "            a['day_of_week'] = dt.weekday()\n",
        "            a['hour'] = dt.hour\n",
        "\n",
        "        raw_targets = [a['n_comments'] for a in train_data]\n",
        "        targets = [np.log1p(t) for t in raw_targets]\n",
        "        bert_vectors = torch.load(\"sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n",
        "\n",
        "        topics = []\n",
        "        subtopics = []\n",
        "        for a in train_data:\n",
        "            topic, subtopic = extract_topics_from_url(a['url'])\n",
        "            topics.append(topic)\n",
        "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
        "\n",
        "        date_feats = process_date_features(train_data)\n",
        "\n",
        "        self.topic_encoder = LabelEncoder().fit(topics)\n",
        "        self.subtopic_encoder = LabelEncoder().fit(subtopics)\n",
        "\n",
        "        topic_ids = self.topic_encoder.transform(topics)\n",
        "        subtopic_ids = self.subtopic_encoder.transform(subtopics)\n",
        "\n",
        "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, date_train, date_val, y_train, y_val = train_test_split(\n",
        "            bert_vectors, topic_ids, subtopic_ids, date_feats, targets, test_size=self.eval_split, random_state=42\n",
        "        )\n",
        "\n",
        "        train_dataset = NewsDataset(\n",
        "            torch.tensor(X_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_train, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "        val_dataset = NewsDataset(\n",
        "            torch.tensor(X_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "        input_dim = bert_vectors.shape[1]\n",
        "        self.model = MLPWithEmbeddings(input_dim, len(self.topic_encoder.classes_), len(self.subtopic_encoder.classes_)).to(self.device)\n",
        "\n",
        "        self._train(self.model, train_loader, val_loader)\n",
        "\n",
        "    def _train(self, model, train_loader, val_loader):\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate, weight_decay=self.l2_lambda)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "        criterion = nn.HuberLoss(delta=5.0)\n",
        "\n",
        "        best_val_mae = float('inf')\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            model.train()\n",
        "            for x_batch, topic_ids, subtopic_ids, date_feats, y_batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats).squeeze()\n",
        "                loss = criterion(y_pred, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            val_losses = []\n",
        "            with torch.no_grad():\n",
        "                for x_batch, topic_ids, subtopic_ids, date_feats, y_batch in val_loader:\n",
        "                    y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats).squeeze()\n",
        "                    loss = criterion(y_pred, y_batch)\n",
        "                    val_losses.append(loss.item())\n",
        "\n",
        "            val_loss = np.mean(val_losses)\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_mae:\n",
        "                best_val_mae = val_loss\n",
        "                best_model_state = model.state_dict()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{self.epochs} - Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save snapshot\n",
        "        os.makedirs(\"snapshots\", exist_ok=True)\n",
        "        torch.save(best_model_state, f\"snapshots/model_{self.model_id:03d}.pt\")\n",
        "\n",
        "    def predict(self, test_data, bert_vectors):\n",
        "        for a in test_data:\n",
        "            dt = pd.to_datetime(a['date'])\n",
        "            a['year'] = dt.year\n",
        "            a['month'] = dt.month\n",
        "            a['day_of_week'] = dt.weekday()\n",
        "            a['hour'] = dt.hour\n",
        "\n",
        "        topics = []\n",
        "        subtopics = []\n",
        "        for a in test_data:\n",
        "            topic, subtopic = extract_topics_from_url(a['url'])\n",
        "            topics.append(topic)\n",
        "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
        "\n",
        "        date_feats = process_date_features(test_data)\n",
        "\n",
        "        topic_ids = [self.topic_encoder.classes_[0] if t not in self.topic_encoder.classes_ else t for t in topics]\n",
        "        subtopic_ids = [self.subtopic_encoder.classes_[0] if s not in self.subtopic_encoder.classes_ else s for s in subtopics]\n",
        "\n",
        "        topic_ids = self.topic_encoder.transform(topic_ids)\n",
        "        subtopic_ids = self.subtopic_encoder.transform(subtopic_ids)\n",
        "\n",
        "        X = torch.tensor(bert_vectors, dtype=torch.float32).to(self.device)\n",
        "        topic_ids = torch.tensor(topic_ids, dtype=torch.long).to(self.device)\n",
        "        subtopic_ids = torch.tensor(subtopic_ids, dtype=torch.long).to(self.device)\n",
        "        date_feats = torch.tensor(date_feats, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = self.model(X, topic_ids, subtopic_ids, date_feats).squeeze().cpu().numpy()\n",
        "            return np.clip(np.expm1(preds), 0, None)\n",
        "\n",
        "# --- 6. Main Loop (Ensemble 70√ó) ---\n",
        "if __name__ == '__main__':\n",
        "    train = load(\"rtvslo_train.json\")\n",
        "    validation = load(\"rtvslo_validation.json\")\n",
        "    test = load(\"rtvslo_test.json\")\n",
        "\n",
        "    bert_vectors_validation = torch.load(\"sloberta_embeddings_val.pt\", map_location=\"cpu\").numpy()\n",
        "    bert_vectors_test = torch.load(\"sloberta_embeddings_final.pt\", map_location=\"cpu\").numpy()\n",
        "\n",
        "    preds_validation = []\n",
        "    preds_test = []\n",
        "\n",
        "    for seed in range(20):\n",
        "        print(f\"\\nüöÄ Training model {seed+1}/70...\")\n",
        "        model = RTVSloBERT(model_id=seed)\n",
        "        model.fit(train)\n",
        "\n",
        "        val_preds = model.predict(validation, bert_vectors_validation)\n",
        "        test_preds = model.predict(test, bert_vectors_test)\n",
        "\n",
        "        preds_validation.append(val_preds)\n",
        "        preds_test.append(test_preds)\n",
        "\n",
        "    # Average all predictions\n",
        "    final_val_preds = np.median(preds_validation, axis=0)\n",
        "    final_test_preds = np.median(preds_test, axis=0)\n",
        "\n",
        "    np.savetxt(\"final_predictions_val.txt\", final_val_preds, fmt=\"%.4f\")\n",
        "    np.savetxt(\"final_predictions_test.txt\", final_test_preds, fmt=\"%.4f\")\n",
        "\n",
        "    print(\"‚úÖ Done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEMANTIC FLAGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elNy5lnTlxz5",
        "outputId": "5b1c4190-85d3-4789-f982-9a40280bedba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Val Loss: 0.5442\n",
            "Epoch 2 - Val Loss: 0.4991\n",
            "Epoch 3 - Val Loss: 0.5044\n",
            "Epoch 4 - Val Loss: 0.4735\n",
            "Epoch 5 - Val Loss: 0.4660\n",
            "Epoch 6 - Val Loss: 0.4719\n",
            "Epoch 7 - Val Loss: 0.4594\n",
            "Epoch 8 - Val Loss: 0.4543\n",
            "Epoch 9 - Val Loss: 0.4592\n",
            "Epoch 10 - Val Loss: 0.4543\n",
            "Epoch 11 - Val Loss: 0.4579\n",
            "Epoch 12 - Val Loss: 0.4537\n",
            "Epoch 13 - Val Loss: 0.4685\n",
            "Epoch 14 - Val Loss: 0.4496\n",
            "Epoch 15 - Val Loss: 0.4418\n",
            "Epoch 16 - Val Loss: 0.4492\n",
            "Epoch 17 - Val Loss: 0.4438\n",
            "Epoch 18 - Val Loss: 0.4541\n",
            "Epoch 19 - Val Loss: 0.4439\n",
            "Epoch 20 - Val Loss: 0.4465\n",
            "Epoch 21 - Val Loss: 0.4634\n",
            "Epoch 22 - Val Loss: 0.4441\n",
            "Epoch 23 - Val Loss: 0.4455\n",
            "Epoch 24 - Val Loss: 0.4410\n",
            "Epoch 25 - Val Loss: 0.4375\n",
            "Epoch 26 - Val Loss: 0.4373\n",
            "Epoch 27 - Val Loss: 0.4407\n",
            "Epoch 28 - Val Loss: 0.4363\n",
            "Epoch 29 - Val Loss: 0.4357\n",
            "Epoch 30 - Val Loss: 0.4379\n",
            "Epoch 31 - Val Loss: 0.4455\n",
            "Epoch 32 - Val Loss: 0.4461\n",
            "Epoch 33 - Val Loss: 0.4317\n",
            "Epoch 34 - Val Loss: 0.4419\n",
            "Epoch 35 - Val Loss: 0.4372\n",
            "Epoch 36 - Val Loss: 0.4441\n",
            "Epoch 37 - Val Loss: 0.4390\n",
            "Epoch 38 - Val Loss: 0.4331\n",
            "Epoch 39 - Val Loss: 0.4449\n",
            "Epoch 40 - Val Loss: 0.4362\n",
            "Epoch 41 - Val Loss: 0.4368\n",
            "Epoch 42 - Val Loss: 0.4369\n",
            "Epoch 43 - Val Loss: 0.4339\n",
            "Epoch 44 - Val Loss: 0.4375\n",
            "Epoch 45 - Val Loss: 0.4406\n",
            "Epoch 46 - Val Loss: 0.4351\n",
            "Epoch 47 - Val Loss: 0.4335\n",
            "Epoch 48 - Val Loss: 0.4332\n",
            "Epoch 49 - Val Loss: 0.4347\n",
            "Epoch 50 - Val Loss: 0.4351\n",
            "Epoch 51 - Val Loss: 0.4346\n",
            "Epoch 52 - Val Loss: 0.4337\n",
            "Epoch 53 - Val Loss: 0.4320\n",
            "Epoch 54 - Val Loss: 0.4338\n",
            "Epoch 55 - Val Loss: 0.4344\n",
            "Epoch 56 - Val Loss: 0.4363\n",
            "Epoch 57 - Val Loss: 0.4366\n",
            "Epoch 58 - Val Loss: 0.4347\n",
            "Epoch 59 - Val Loss: 0.4344\n",
            "Epoch 60 - Val Loss: 0.4355\n",
            "Epoch 61 - Val Loss: 0.4342\n",
            "Epoch 62 - Val Loss: 0.4358\n",
            "Epoch 63 - Val Loss: 0.4332\n",
            "Epoch 64 - Val Loss: 0.4333\n",
            "Epoch 65 - Val Loss: 0.4344\n",
            "Epoch 66 - Val Loss: 0.4323\n",
            "Epoch 67 - Val Loss: 0.4337\n",
            "Epoch 68 - Val Loss: 0.4344\n",
            "Epoch 69 - Val Loss: 0.4344\n",
            "Epoch 70 - Val Loss: 0.4344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 1675.71it/s]\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Imports ---\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 2. Utility Functions ---\n",
        "def load(fn):\n",
        "    with open(fn, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def process_date_features(articles):\n",
        "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
        "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
        "    day_of_week = np.array([a['day_of_week'] for a in articles])\n",
        "    hour = np.array([a['hour'] for a in articles])\n",
        "\n",
        "    year_scaler = StandardScaler()\n",
        "    month_scaler = StandardScaler()\n",
        "    years_scaled = year_scaler.fit_transform(years)\n",
        "    months_scaled = month_scaler.fit_transform(months)\n",
        "\n",
        "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
        "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
        "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
        "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
        "\n",
        "    date_feats = np.concatenate(\n",
        "        [years_scaled, months_scaled, day_sin[:, None], day_cos[:, None], hour_sin[:, None], hour_cos[:, None]],\n",
        "        axis=1\n",
        "    )\n",
        "    return date_feats\n",
        "\n",
        "def extract_topics_from_url(url):\n",
        "    parts = url.split('/')\n",
        "    topic = parts[3] if len(parts) > 3 else 'none'\n",
        "    subtopic = parts[4] if len(parts) > 4 else 'none'\n",
        "    return topic, subtopic\n",
        "\n",
        "def extract_semantic_flags(articles):\n",
        "    flags = []\n",
        "    for a in articles:\n",
        "        url = a.get('url', '').lower()\n",
        "        is_politics = any(word in url for word in ['politics', 'elections', 'parliament', 'vlada', 'politika'])\n",
        "        is_ukraine_russia = any(word in url for word in ['ukraine', 'russia', 'ukrajina', 'rusija'])\n",
        "        is_israel_palestine = any(word in url for word in ['israel', 'palestine', 'gaza', 'hamas'])\n",
        "        is_sport = any(word in url for word in ['sport', 'nogomet', 'ko≈°arka', 'tenis', 'hokej'])\n",
        "        flags.append([is_politics, is_ukraine_russia, is_israel_palestine, is_sport])\n",
        "    return np.array(flags, dtype=np.float32)\n",
        "\n",
        "# --- 3. Dataset ---\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, X, topic_ids, subtopic_ids, date_feats, semantic_flags, targets):\n",
        "        self.X = X\n",
        "        self.topic_ids = topic_ids\n",
        "        self.subtopic_ids = subtopic_ids\n",
        "        self.date_feats = date_feats\n",
        "        self.semantic_flags = semantic_flags\n",
        "        self.y = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.topic_ids[idx], self.subtopic_ids[idx], self.date_feats[idx], self.semantic_flags[idx], self.y[idx]\n",
        "\n",
        "# --- 4. Model ---\n",
        "class MLPWithEmbeddings(nn.Module):\n",
        "    def __init__(self, input_dim, num_topics, num_subtopics):\n",
        "        super().__init__()\n",
        "        self.topic_embedding = nn.Embedding(num_topics, 16)\n",
        "        self.subtopic_embedding = nn.Embedding(num_subtopics, 24)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim + 16 + 24 + 6 + 4, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "    def forward(self, x_text, topic_ids, subtopic_ids, date_feats, semantic_flags):\n",
        "        topic_embed = self.topic_embedding(topic_ids)\n",
        "        subtopic_embed = self.subtopic_embedding(subtopic_ids)\n",
        "        x = torch.cat([x_text, topic_embed, subtopic_embed, date_feats, semantic_flags], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# --- 5. Main Trainer ---\n",
        "class RTVSloBERT:\n",
        "    def __init__(self, batch_size=170, epochs=70, learning_rate=1e-4, l2_lambda=1e-3, eval_split=0.05):\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.eval_split = eval_split\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def fit(self, train_data, save_dir=\"snapshots\"):\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        for a in train_data:\n",
        "            dt = pd.to_datetime(a['date'])\n",
        "            a['year'] = dt.year\n",
        "            a['month'] = dt.month\n",
        "            a['day_of_week'] = dt.weekday()\n",
        "            a['hour'] = dt.hour\n",
        "\n",
        "        raw_targets = np.array([a['n_comments'] for a in train_data])\n",
        "        targets = np.log1p(raw_targets)\n",
        "\n",
        "        bert_vectors = torch.load(\"sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n",
        "\n",
        "        topics = []\n",
        "        subtopics = []\n",
        "        for a in train_data:\n",
        "            topic, subtopic = extract_topics_from_url(a['url'])\n",
        "            topics.append(topic)\n",
        "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
        "\n",
        "        date_feats = process_date_features(train_data)\n",
        "        semantic_flags = extract_semantic_flags(train_data)\n",
        "\n",
        "        self.topic_encoder = LabelEncoder().fit(topics)\n",
        "        self.subtopic_encoder = LabelEncoder().fit(subtopics)\n",
        "\n",
        "        topic_ids = self.topic_encoder.transform(topics)\n",
        "        subtopic_ids = self.subtopic_encoder.transform(subtopics)\n",
        "\n",
        "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, date_train, date_val, semantic_train, semantic_val, y_train, y_val = train_test_split(\n",
        "            bert_vectors, topic_ids, subtopic_ids, date_feats, semantic_flags, targets, test_size=self.eval_split, random_state=42\n",
        "        )\n",
        "\n",
        "        train_dataset = NewsDataset(\n",
        "            torch.tensor(X_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(semantic_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_train, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "\n",
        "        val_dataset = NewsDataset(\n",
        "            torch.tensor(X_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(semantic_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "        input_dim = bert_vectors.shape[1]\n",
        "        self.model = MLPWithEmbeddings(input_dim, len(self.topic_encoder.classes_), len(self.subtopic_encoder.classes_)).to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.l2_lambda)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "        criterion = nn.HuberLoss(delta=5.0)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for x_batch, topic_ids, subtopic_ids, date_feats, semantic_flags, y_batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = self.model(x_batch, topic_ids, subtopic_ids, date_feats, semantic_flags).squeeze()\n",
        "                loss = criterion(y_pred, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            self.model.eval()\n",
        "            val_losses = []\n",
        "            with torch.no_grad():\n",
        "                for x_batch, topic_ids, subtopic_ids, date_feats, semantic_flags, y_batch in val_loader:\n",
        "                    y_pred = self.model(x_batch, topic_ids, subtopic_ids, date_feats, semantic_flags).squeeze()\n",
        "                    loss = criterion(y_pred, y_batch)\n",
        "                    val_losses.append(loss.item())\n",
        "\n",
        "            val_loss = np.mean(val_losses)\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "            # Save snapshot\n",
        "            torch.save(self.model.state_dict(), os.path.join(save_dir, f\"snapshot_epoch{epoch+1}.pt\"))\n",
        "\n",
        "    def predict(self, articles, bert_vectors_path):\n",
        "        bert_vectors = torch.load(bert_vectors_path, map_location=\"cpu\").to(self.device)\n",
        "\n",
        "        for a in articles:\n",
        "            dt = pd.to_datetime(a['date'])\n",
        "            a['year'] = dt.year\n",
        "            a['month'] = dt.month\n",
        "            a['day_of_week'] = dt.weekday()\n",
        "            a['hour'] = dt.hour\n",
        "\n",
        "        topics = []\n",
        "        subtopics = []\n",
        "        for a in articles:\n",
        "            topic, subtopic = extract_topics_from_url(a['url'])\n",
        "            topics.append(topic)\n",
        "            subtopics.append(subtopic if subtopic != \"NO_SUBTOPIC\" else \"none\")\n",
        "\n",
        "        date_feats = torch.tensor(process_date_features(articles), dtype=torch.float32).to(self.device)\n",
        "        semantic_flags = torch.tensor(extract_semantic_flags(articles), dtype=torch.float32).to(self.device)\n",
        "\n",
        "        topic_ids = [self.topic_encoder.classes_[0] if t not in self.topic_encoder.classes_ else t for t in topics]\n",
        "        subtopic_ids = [self.subtopic_encoder.classes_[0] if s not in self.subtopic_encoder.classes_ else s for s in subtopics]\n",
        "\n",
        "        topic_ids = torch.tensor(self.topic_encoder.transform(topic_ids), dtype=torch.long).to(self.device)\n",
        "        subtopic_ids = torch.tensor(self.subtopic_encoder.transform(subtopic_ids), dtype=torch.long).to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for i in tqdm(range(0, bert_vectors.shape[0], self.batch_size)):\n",
        "                xb = bert_vectors[i:i+self.batch_size]\n",
        "                tb = topic_ids[i:i+self.batch_size]\n",
        "                stb = subtopic_ids[i:i+self.batch_size]\n",
        "                db = date_feats[i:i+self.batch_size]\n",
        "                sb = semantic_flags[i:i+self.batch_size]\n",
        "                yb = self.model(xb, tb, stb, db, sb).squeeze()\n",
        "                preds.append(yb)\n",
        "\n",
        "        preds = torch.cat(preds).cpu().numpy()\n",
        "        preds = np.expm1(np.clip(preds, 0, None))\n",
        "        return preds\n",
        "\n",
        "# --- 6. Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    train_data = load(\"rtvslo_train.json\")\n",
        "    val_data = load(\"rtvslo_validation.json\")\n",
        "\n",
        "    model = RTVSloBERT()\n",
        "    model.fit(train_data)\n",
        "\n",
        "    preds_val = model.predict(val_data, \"sloberta_embeddings_val.pt\")\n",
        "    np.savetxt(\"predictions_val_semantic_noweights.txt\", preds_val, fmt=\"%.4f\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CONTEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62uL90v74933",
        "outputId": "752a0591-4413-470f-ce3e-91864a22df8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Validation Loss: 0.7560\n",
            "Epoch 2 - Validation Loss: 0.6287\n",
            "Epoch 3 - Validation Loss: 0.6443\n",
            "Epoch 4 - Validation Loss: 0.5695\n",
            "Epoch 5 - Validation Loss: 0.5657\n",
            "Epoch 6 - Validation Loss: 0.5712\n",
            "Epoch 7 - Validation Loss: 0.5596\n",
            "Epoch 8 - Validation Loss: 0.5676\n",
            "Epoch 9 - Validation Loss: 0.5372\n",
            "Epoch 10 - Validation Loss: 0.5434\n",
            "Epoch 11 - Validation Loss: 0.5410\n",
            "Epoch 12 - Validation Loss: 0.6468\n",
            "Epoch 13 - Validation Loss: 0.5878\n",
            "Epoch 14 - Validation Loss: 0.5256\n",
            "Epoch 15 - Validation Loss: 0.5226\n",
            "Epoch 16 - Validation Loss: 0.5778\n",
            "Epoch 17 - Validation Loss: 0.5413\n",
            "Epoch 18 - Validation Loss: 0.5528\n",
            "Epoch 19 - Validation Loss: 0.8377\n",
            "Epoch 20 - Validation Loss: 0.5065\n",
            "Epoch 21 - Validation Loss: 0.6112\n",
            "Epoch 22 - Validation Loss: 0.5119\n",
            "Epoch 23 - Validation Loss: 0.5352\n",
            "Epoch 24 - Validation Loss: 0.5259\n",
            "Epoch 25 - Validation Loss: 0.5080\n",
            "Epoch 26 - Validation Loss: 0.4983\n",
            "Epoch 27 - Validation Loss: 0.5150\n",
            "Epoch 28 - Validation Loss: 0.4926\n",
            "Epoch 29 - Validation Loss: 0.4929\n",
            "Epoch 30 - Validation Loss: 0.5160\n",
            "Epoch 31 - Validation Loss: 0.5196\n",
            "Epoch 32 - Validation Loss: 0.5389\n",
            "Epoch 33 - Validation Loss: 0.5618\n",
            "Epoch 34 - Validation Loss: 0.6127\n",
            "Epoch 35 - Validation Loss: 0.4742\n",
            "Epoch 36 - Validation Loss: 0.4951\n",
            "Epoch 37 - Validation Loss: 0.4695\n",
            "Epoch 38 - Validation Loss: 0.4821\n",
            "Epoch 39 - Validation Loss: 0.4751\n",
            "Epoch 40 - Validation Loss: 0.5049\n",
            "Epoch 41 - Validation Loss: 0.4914\n",
            "Epoch 42 - Validation Loss: 0.4647\n",
            "Epoch 43 - Validation Loss: 0.4638\n",
            "Epoch 44 - Validation Loss: 0.5053\n",
            "Epoch 45 - Validation Loss: 0.4799\n",
            "Epoch 46 - Validation Loss: 0.5219\n",
            "Epoch 47 - Validation Loss: 0.5514\n",
            "Epoch 48 - Validation Loss: 0.5075\n",
            "Epoch 49 - Validation Loss: 0.4677\n",
            "Epoch 50 - Validation Loss: 0.4882\n",
            "Epoch 51 - Validation Loss: 0.4603\n",
            "Epoch 52 - Validation Loss: 0.4598\n",
            "Epoch 53 - Validation Loss: 0.4831\n",
            "Epoch 54 - Validation Loss: 0.4851\n",
            "Epoch 55 - Validation Loss: 0.4870\n",
            "Epoch 56 - Validation Loss: 0.4666\n",
            "Epoch 57 - Validation Loss: 0.4711\n",
            "Epoch 58 - Validation Loss: 0.5197\n",
            "Epoch 59 - Validation Loss: 0.4641\n",
            "Epoch 60 - Validation Loss: 0.4535\n",
            "Epoch 61 - Validation Loss: 0.4519\n",
            "Epoch 62 - Validation Loss: 0.4524\n",
            "Epoch 63 - Validation Loss: 0.4951\n",
            "Epoch 64 - Validation Loss: 0.4546\n",
            "Epoch 65 - Validation Loss: 0.4508\n",
            "Epoch 66 - Validation Loss: 0.4502\n",
            "Epoch 67 - Validation Loss: 0.4719\n",
            "Epoch 68 - Validation Loss: 0.4535\n",
            "Epoch 69 - Validation Loss: 0.4591\n",
            "Epoch 70 - Validation Loss: 0.4541\n",
            "Epoch 71 - Validation Loss: 0.4634\n",
            "Epoch 72 - Validation Loss: 0.4544\n",
            "Epoch 73 - Validation Loss: 0.4514\n",
            "Epoch 74 - Validation Loss: 0.4483\n",
            "Epoch 75 - Validation Loss: 0.4497\n",
            "Epoch 76 - Validation Loss: 0.4572\n",
            "Epoch 77 - Validation Loss: 0.4547\n",
            "Epoch 78 - Validation Loss: 0.4513\n",
            "Epoch 79 - Validation Loss: 0.4477\n",
            "Epoch 80 - Validation Loss: 0.4505\n",
            "Epoch 81 - Validation Loss: 0.4569\n",
            "Epoch 82 - Validation Loss: 0.4490\n",
            "Epoch 83 - Validation Loss: 0.4531\n",
            "Epoch 84 - Validation Loss: 0.4581\n",
            "Epoch 85 - Validation Loss: 0.4511\n",
            "Epoch 86 - Validation Loss: 0.4460\n",
            "Epoch 87 - Validation Loss: 0.4455\n",
            "Epoch 88 - Validation Loss: 0.4547\n",
            "Epoch 89 - Validation Loss: 0.4463\n",
            "Epoch 90 - Validation Loss: 0.4475\n",
            "Epoch 91 - Validation Loss: 0.4528\n",
            "Epoch 92 - Validation Loss: 0.4480\n",
            "Epoch 93 - Validation Loss: 0.4463\n",
            "Epoch 94 - Validation Loss: 0.4476\n",
            "Epoch 95 - Validation Loss: 0.4472\n",
            "Epoch 96 - Validation Loss: 0.4474\n",
            "Epoch 97 - Validation Loss: 0.4458\n",
            "Epoch 98 - Validation Loss: 0.4502\n",
            "Epoch 99 - Validation Loss: 0.4464\n",
            "Epoch 100 - Validation Loss: 0.4456\n",
            "Epoch 101 - Validation Loss: 0.4454\n",
            "Epoch 102 - Validation Loss: 0.4477\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 1564.33it/s]\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Imports ---\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- 2. Utility Functions ---\n",
        "def load(fn):\n",
        "    with open(fn, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def enrich_articles(articles):\n",
        "    \"\"\"Extract date features + semantic groups + article context features.\"\"\"\n",
        "    # Step 1: Extract datetime features\n",
        "    for a in articles:\n",
        "        dt = pd.to_datetime(a['date'])\n",
        "        a['year'] = dt.year\n",
        "        a['month'] = dt.month\n",
        "        a['day_of_week'] = dt.weekday()\n",
        "        a['hour'] = dt.hour\n",
        "        a['timestamp'] = dt.timestamp()\n",
        "\n",
        "    # Step 2: Semantic group flags\n",
        "    for a in articles:\n",
        "        url = a.get('url', '').lower()\n",
        "        a['is_politics'] = int(any(word in url for word in ['politics', 'elections', 'parliament', 'vlada', 'politika']))\n",
        "        a['is_ukraine_russia'] = int(any(word in url for word in ['ukraine', 'russia', 'ukrajina', 'rusija']))\n",
        "        a['is_israel_palestine'] = int(any(word in url for word in ['israel', 'palestine', 'gaza', 'hamas']))\n",
        "        a['is_sport'] = int(any(word in url for word in ['sport', 'nogomet', 'ko≈°arka', 'tenis', 'hokej']))\n",
        "\n",
        "    # Step 3: Sort articles\n",
        "    articles.sort(key=lambda a: a['timestamp'])\n",
        "\n",
        "    # Step 4: Context window features\n",
        "    timestamps = np.array([a['timestamp'] for a in articles])\n",
        "    n_comments = np.array([a['n_comments'] for a in articles])\n",
        "\n",
        "    for i, a in enumerate(articles):\n",
        "        # Last 1h, 6h, 12h windows\n",
        "        t = a['timestamp']\n",
        "        a['articles_last_1h'] = np.sum((t - timestamps[:i]) <= 3600)\n",
        "        a['articles_last_6h'] = np.sum((t - timestamps[:i]) <= 6*3600)\n",
        "        a['articles_last_12h'] = np.sum((t - timestamps[:i]) <= 12*3600)\n",
        "        # Total comments in last 6h\n",
        "        mask_6h = (t - timestamps[:i]) <= 6*3600\n",
        "        a['total_comments_last_6h'] = np.sum(n_comments[:i][mask_6h]) if np.any(mask_6h) else 0.0\n",
        "\n",
        "    return articles\n",
        "\n",
        "def process_date_features(articles):\n",
        "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
        "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
        "    day_of_week = np.array([a['day_of_week'] for a in articles])\n",
        "    hour = np.array([a['hour'] for a in articles])\n",
        "\n",
        "    year_scaler = StandardScaler()\n",
        "    month_scaler = StandardScaler()\n",
        "    years_scaled = year_scaler.fit_transform(years)\n",
        "    months_scaled = month_scaler.fit_transform(months)\n",
        "\n",
        "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
        "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
        "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
        "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
        "\n",
        "    date_feats = np.concatenate([years_scaled, months_scaled, day_sin[:, None], day_cos[:, None], hour_sin[:, None], hour_cos[:, None]], axis=1)\n",
        "    return date_feats\n",
        "\n",
        "def extract_topics_from_url(url):\n",
        "    parts = url.split('/')\n",
        "    topic = parts[3] if len(parts) > 3 else 'none'\n",
        "    subtopic = parts[4] if len(parts) > 4 else 'none'\n",
        "    return topic, subtopic\n",
        "\n",
        "# --- 3. Dataset ---\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, X, topic_ids, subtopic_ids, date_feats, semantic_flags, context_feats, targets):\n",
        "        self.X = X\n",
        "        self.topic_ids = topic_ids\n",
        "        self.subtopic_ids = subtopic_ids\n",
        "        self.date_feats = date_feats\n",
        "        self.semantic_flags = semantic_flags\n",
        "        self.context_feats = context_feats\n",
        "        self.y = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.topic_ids[idx], self.subtopic_ids[idx], self.date_feats[idx], self.semantic_flags[idx], self.context_feats[idx], self.y[idx]\n",
        "\n",
        "# --- 4. Model ---\n",
        "class MLPWithContext(nn.Module):\n",
        "    def __init__(self, input_dim, num_topics, num_subtopics):\n",
        "        super().__init__()\n",
        "        self.topic_embedding = nn.Embedding(num_topics, 16)\n",
        "        self.subtopic_embedding = nn.Embedding(num_subtopics, 24)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim + 16 + 24 + 6 + 4 + 4, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "    def forward(self, x_text, topic_ids, subtopic_ids, date_feats, semantic_flags, context_feats):\n",
        "        topic_embed = self.topic_embedding(topic_ids)\n",
        "        subtopic_embed = self.subtopic_embedding(subtopic_ids)\n",
        "        x = torch.cat([x_text, topic_embed, subtopic_embed, date_feats, semantic_flags, context_feats], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# --- 5. Main Trainer ---\n",
        "class RTVSloBERTContext:\n",
        "    def __init__(self, batch_size=170, epochs=150, learning_rate=1e-4, l2_lambda=1e-3, eval_split=0.05):\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.eval_split = eval_split\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        os.makedirs(\"snapshots\", exist_ok=True)\n",
        "        train_data = enrich_articles(train_data)\n",
        "\n",
        "        raw_targets = np.array([a['n_comments'] for a in train_data])\n",
        "        targets = np.log1p(raw_targets)\n",
        "\n",
        "        bert_vectors = torch.load(\"sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n",
        "\n",
        "        topics, subtopics = zip(*[extract_topics_from_url(a['url']) for a in train_data])\n",
        "        subtopics = [s if s != \"NO_SUBTOPIC\" else \"none\" for s in subtopics]\n",
        "\n",
        "        date_feats = process_date_features(train_data)\n",
        "        semantic_flags = np.array([[a['is_politics'], a['is_ukraine_russia'], a['is_israel_palestine'], a['is_sport']] for a in train_data], dtype=np.float32)\n",
        "        context_feats = np.array([[a['articles_last_1h'], a['articles_last_6h'], a['articles_last_12h'], a['total_comments_last_6h']] for a in train_data], dtype=np.float32)\n",
        "\n",
        "        self.topic_encoder = LabelEncoder().fit(topics)\n",
        "        self.subtopic_encoder = LabelEncoder().fit(subtopics)\n",
        "\n",
        "        topic_ids = self.topic_encoder.transform(topics)\n",
        "        subtopic_ids = self.subtopic_encoder.transform(subtopics)\n",
        "\n",
        "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, date_train, date_val, semantic_train, semantic_val, context_train, context_val, y_train, y_val = train_test_split(\n",
        "            bert_vectors, topic_ids, subtopic_ids, date_feats, semantic_flags, context_feats, targets, test_size=self.eval_split, random_state=42\n",
        "        )\n",
        "\n",
        "        train_dataset = NewsDataset(\n",
        "            torch.tensor(X_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(semantic_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(context_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_train, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "\n",
        "        val_dataset = NewsDataset(\n",
        "            torch.tensor(X_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(semantic_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(context_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "        input_dim = bert_vectors.shape[1]\n",
        "        self.model = MLPWithContext(input_dim, len(self.topic_encoder.classes_), len(self.subtopic_encoder.classes_)).to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.l2_lambda)\n",
        "        criterion = nn.HuberLoss(delta=5.0)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        patience_limit = 15\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for xb, topicb, subtopicb, dateb, semanticb, contextb, yb in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                preds = self.model(xb, topicb, subtopicb, dateb, semanticb, contextb).squeeze()\n",
        "                loss = criterion(preds, yb)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            val_losses = []\n",
        "            with torch.no_grad():\n",
        "                for xb, topicb, subtopicb, dateb, semanticb, contextb, yb in val_loader:\n",
        "                    preds = self.model(xb, topicb, subtopicb, dateb, semanticb, contextb).squeeze()\n",
        "                    val_losses.append(criterion(preds, yb).item())\n",
        "\n",
        "            val_loss = np.mean(val_losses)\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1} - Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "            if val_loss < best_loss - 1e-4:\n",
        "                best_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                torch.save(self.model.state_dict(), f\"snapshots/best_model.pt\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience_limit:\n",
        "                    print(\"Early stopping triggered.\")\n",
        "                    break\n",
        "\n",
        "    def predict(self, articles, bert_path):\n",
        "        articles = enrich_articles(articles)\n",
        "        bert_vectors = torch.load(bert_path, map_location=\"cpu\").numpy()\n",
        "\n",
        "        topics, subtopics = zip(*[extract_topics_from_url(a['url']) for a in articles])\n",
        "        subtopics = [s if s != \"NO_SUBTOPIC\" else \"none\" for s in subtopics]\n",
        "\n",
        "        date_feats = process_date_features(articles)\n",
        "        semantic_flags = np.array([[a['is_politics'], a['is_ukraine_russia'], a['is_israel_palestine'], a['is_sport']] for a in articles], dtype=np.float32)\n",
        "        context_feats = np.array([[a['articles_last_1h'], a['articles_last_6h'], a['articles_last_12h'], a['total_comments_last_6h']] for a in articles], dtype=np.float32)\n",
        "\n",
        "        topic_ids = [self.topic_encoder.classes_[0] if t not in self.topic_encoder.classes_ else t for t in topics]\n",
        "        subtopic_ids = [self.subtopic_encoder.classes_[0] if s not in self.subtopic_encoder.classes_ else s for s in subtopics]\n",
        "\n",
        "        topic_ids = self.topic_encoder.transform(topic_ids)\n",
        "        subtopic_ids = self.subtopic_encoder.transform(subtopic_ids)\n",
        "\n",
        "        X = torch.tensor(bert_vectors, dtype=torch.float32).to(self.device)\n",
        "        topic_ids = torch.tensor(topic_ids, dtype=torch.long).to(self.device)\n",
        "        subtopic_ids = torch.tensor(subtopic_ids, dtype=torch.long).to(self.device)\n",
        "        date_feats = torch.tensor(date_feats, dtype=torch.float32).to(self.device)\n",
        "        semantic_flags = torch.tensor(semantic_flags, dtype=torch.float32).to(self.device)\n",
        "        context_feats = torch.tensor(context_feats, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        self.model.load_state_dict(torch.load(\"snapshots/best_model.pt\"))\n",
        "        self.model.eval()\n",
        "\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for i in tqdm(range(0, len(X), self.batch_size)):\n",
        "                xb = X[i:i+self.batch_size]\n",
        "                topicb = topic_ids[i:i+self.batch_size]\n",
        "                subtopicb = subtopic_ids[i:i+self.batch_size]\n",
        "                dateb = date_feats[i:i+self.batch_size]\n",
        "                semanticb = semantic_flags[i:i+self.batch_size]\n",
        "                contextb = context_feats[i:i+self.batch_size]\n",
        "                preds.append(self.model(xb, topicb, subtopicb, dateb, semanticb, contextb).squeeze())\n",
        "\n",
        "        preds = torch.cat(preds).cpu().numpy()\n",
        "        preds = np.expm1(np.clip(preds, 0, None))\n",
        "        return preds\n",
        "\n",
        "# --- 6. Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    train_articles = load(\"rtvslo_train.json\")\n",
        "    val_articles = load(\"rtvslo_validation.json\")\n",
        "\n",
        "    model = RTVSloBERTContext()\n",
        "    model.fit(train_articles)\n",
        "\n",
        "    preds_val = model.predict(val_articles, \"sloberta_embeddings_val.pt\")\n",
        "    np.savetxt(\"predictions_val_context.txt\", preds_val, fmt=\"%.4f\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ADDITIONAL MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj6rWbCJCSyN",
        "outputId": "77fdaec1-b33b-4130-a961-b9efa80d3713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Validation Loss: 0.5582\n",
            "Epoch 2 - Validation Loss: 0.5190\n",
            "Epoch 3 - Validation Loss: 0.5029\n",
            "Epoch 4 - Validation Loss: 0.4843\n",
            "Epoch 5 - Validation Loss: 0.4804\n",
            "Epoch 6 - Validation Loss: 0.4678\n",
            "Epoch 7 - Validation Loss: 0.4593\n",
            "Epoch 8 - Validation Loss: 0.4636\n",
            "Epoch 9 - Validation Loss: 0.4623\n",
            "Epoch 10 - Validation Loss: 0.4570\n",
            "Epoch 11 - Validation Loss: 0.4557\n",
            "Epoch 12 - Validation Loss: 0.4525\n",
            "Epoch 13 - Validation Loss: 0.4486\n",
            "Epoch 14 - Validation Loss: 0.4487\n",
            "Epoch 15 - Validation Loss: 0.4518\n",
            "Epoch 16 - Validation Loss: 0.4737\n",
            "Epoch 17 - Validation Loss: 0.4532\n",
            "Epoch 18 - Validation Loss: 0.4515\n",
            "Epoch 19 - Validation Loss: 0.4468\n",
            "Epoch 20 - Validation Loss: 0.4455\n",
            "Epoch 21 - Validation Loss: 0.4363\n",
            "Epoch 22 - Validation Loss: 0.4398\n",
            "Epoch 23 - Validation Loss: 0.4402\n",
            "Epoch 24 - Validation Loss: 0.4449\n",
            "Epoch 25 - Validation Loss: 0.4435\n",
            "Epoch 26 - Validation Loss: 0.4271\n",
            "Epoch 27 - Validation Loss: 0.4272\n",
            "Epoch 28 - Validation Loss: 0.4346\n",
            "Epoch 29 - Validation Loss: 0.4364\n",
            "Epoch 30 - Validation Loss: 0.4341\n",
            "Epoch 31 - Validation Loss: 0.4398\n",
            "Epoch 32 - Validation Loss: 0.4353\n",
            "Epoch 33 - Validation Loss: 0.4320\n",
            "Epoch 34 - Validation Loss: 0.4273\n",
            "Epoch 35 - Validation Loss: 0.4274\n",
            "Epoch 36 - Validation Loss: 0.4239\n",
            "Epoch 37 - Validation Loss: 0.4299\n",
            "Epoch 38 - Validation Loss: 0.4344\n",
            "Epoch 39 - Validation Loss: 0.4303\n",
            "Epoch 40 - Validation Loss: 0.4320\n",
            "Epoch 41 - Validation Loss: 0.4278\n",
            "Epoch 42 - Validation Loss: 0.4393\n",
            "Epoch 43 - Validation Loss: 0.4281\n",
            "Epoch 44 - Validation Loss: 0.4252\n",
            "Epoch 45 - Validation Loss: 0.4249\n",
            "Epoch 46 - Validation Loss: 0.4277\n",
            "Epoch 47 - Validation Loss: 0.4300\n",
            "Epoch 48 - Validation Loss: 0.4282\n",
            "Epoch 49 - Validation Loss: 0.4255\n",
            "Epoch 50 - Validation Loss: 0.4268\n",
            "Epoch 51 - Validation Loss: 0.4270\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 1304.31it/s]\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Imports ---\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- 2. Utility functions ---\n",
        "def load(fn):\n",
        "    with open(fn, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def enrich_articles(articles):\n",
        "    for a in articles:\n",
        "        dt = pd.to_datetime(a['date'])\n",
        "        a['year'] = dt.year\n",
        "        a['month'] = dt.month\n",
        "        a['day_of_week'] = dt.weekday()\n",
        "        a['hour'] = dt.hour\n",
        "        a['timestamp'] = dt.timestamp()\n",
        "\n",
        "        url = a.get('url', '').lower()\n",
        "        a['is_politics'] = int(any(word in url for word in ['politics', 'elections', 'parliament', 'vlada', 'politika']))\n",
        "        a['is_ukraine_russia'] = int(any(word in url for word in ['ukraine', 'russia', 'ukrajina', 'rusija']))\n",
        "        a['is_israel_palestine'] = int(any(word in url for word in ['israel', 'palestine', 'gaza', 'hamas']))\n",
        "        a['is_sport'] = int(any(word in url for word in ['sport', 'nogomet', 'ko≈°arka', 'tenis', 'hokej']))\n",
        "\n",
        "    articles.sort(key=lambda a: a['timestamp'])\n",
        "\n",
        "    timestamps = np.array([a['timestamp'] for a in articles])\n",
        "    n_comments = np.array([a['n_comments'] for a in articles])\n",
        "\n",
        "    for i, a in enumerate(articles):\n",
        "        t = a['timestamp']\n",
        "        a['articles_last_1h'] = np.sum((t - timestamps[:i]) <= 3600)\n",
        "        a['articles_last_6h'] = np.sum((t - timestamps[:i]) <= 6*3600)\n",
        "        a['articles_last_12h'] = np.sum((t - timestamps[:i]) <= 12*3600)\n",
        "        mask_6h = (t - timestamps[:i]) <= 6*3600\n",
        "        a['total_comments_last_6h'] = np.sum(n_comments[:i][mask_6h]) if np.any(mask_6h) else 0.0\n",
        "\n",
        "    return articles\n",
        "\n",
        "def process_date_features(articles):\n",
        "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
        "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
        "    day_of_week = np.array([a['day_of_week'] for a in articles])\n",
        "    hour = np.array([a['hour'] for a in articles])\n",
        "\n",
        "    year_scaler = StandardScaler()\n",
        "    month_scaler = StandardScaler()\n",
        "    years_scaled = year_scaler.fit_transform(years)\n",
        "    months_scaled = month_scaler.fit_transform(months)\n",
        "\n",
        "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
        "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
        "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
        "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
        "\n",
        "    return np.concatenate([years_scaled, months_scaled, day_sin[:, None], day_cos[:, None], hour_sin[:, None], hour_cos[:, None]], axis=1)\n",
        "\n",
        "def extract_topics_from_url(url):\n",
        "    parts = url.split('/')\n",
        "    topic = parts[3] if len(parts) > 3 else 'none'\n",
        "    subtopic = parts[4] if len(parts) > 4 else 'none'\n",
        "    return topic, subtopic\n",
        "\n",
        "# --- 3. Dataset ---\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, X, topic_ids, subtopic_ids, date_feats, semantic_flags, context_feats, targets):\n",
        "        self.X = X\n",
        "        self.topic_ids = topic_ids\n",
        "        self.subtopic_ids = subtopic_ids\n",
        "        self.date_feats = date_feats\n",
        "        self.semantic_flags = semantic_flags\n",
        "        self.context_feats = context_feats\n",
        "        self.y = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.topic_ids[idx], self.subtopic_ids[idx], self.date_feats[idx], self.semantic_flags[idx], self.context_feats[idx], self.y[idx]\n",
        "\n",
        "# --- 4. Model ---\n",
        "class MLPBlock(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8)\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.mlp(x)\n",
        "\n",
        "class MLPWithContextFeatures(nn.Module):\n",
        "    def __init__(self, input_dim, num_topics, num_subtopics):\n",
        "        super().__init__()\n",
        "        self.topic_embedding = nn.Embedding(num_topics, 16)\n",
        "        self.subtopic_embedding = nn.Embedding(num_subtopics, 24)\n",
        "        self.context_mlp = MLPBlock(8)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim + 16 + 24 + 6 + 8, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "    def forward(self, x_text, topic_ids, subtopic_ids, date_feats, semantic_flags, context_feats):\n",
        "        topic_embed = self.topic_embedding(topic_ids)\n",
        "        subtopic_embed = self.subtopic_embedding(subtopic_ids)\n",
        "        context_input = torch.cat([semantic_flags, context_feats], dim=-1)\n",
        "        context_out = self.context_mlp(context_input)\n",
        "\n",
        "        x = torch.cat([x_text, topic_embed, subtopic_embed, date_feats, context_out], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# --- 5. Trainer ---\n",
        "class RTVSloImprovedModel:\n",
        "    def __init__(self, batch_size=170, epochs=150, learning_rate=1e-4, l2_lambda=1e-3, eval_split=0.05):\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.eval_split = eval_split\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        os.makedirs(\"snapshots\", exist_ok=True)\n",
        "        train_data = enrich_articles(train_data)\n",
        "\n",
        "        raw_targets = np.array([a['n_comments'] for a in train_data])\n",
        "        targets = np.log1p(raw_targets)\n",
        "\n",
        "        bert_vectors = torch.load(\"sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n",
        "\n",
        "        topics, subtopics = zip(*[extract_topics_from_url(a['url']) for a in train_data])\n",
        "        subtopics = [s if s != \"NO_SUBTOPIC\" else \"none\" for s in subtopics]\n",
        "\n",
        "        date_feats = process_date_features(train_data)\n",
        "        semantic_flags = np.array([[a['is_politics'], a['is_ukraine_russia'], a['is_israel_palestine'], a['is_sport']] for a in train_data], dtype=np.float32)\n",
        "        context_feats = np.array([[a['articles_last_1h'], a['articles_last_6h'], a['articles_last_12h'], a['total_comments_last_6h']] for a in train_data], dtype=np.float32)\n",
        "\n",
        "        self.topic_encoder = LabelEncoder().fit(topics)\n",
        "        self.subtopic_encoder = LabelEncoder().fit(subtopics)\n",
        "\n",
        "        topic_ids = self.topic_encoder.transform(topics)\n",
        "        subtopic_ids = self.subtopic_encoder.transform(subtopics)\n",
        "\n",
        "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, date_train, date_val, semantic_train, semantic_val, context_train, context_val, y_train, y_val = train_test_split(\n",
        "            bert_vectors, topic_ids, subtopic_ids, date_feats, semantic_flags, context_feats, targets, test_size=self.eval_split, random_state=42\n",
        "        )\n",
        "\n",
        "        train_dataset = NewsDataset(\n",
        "            torch.tensor(X_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(semantic_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(context_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_train, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "\n",
        "        val_dataset = NewsDataset(\n",
        "            torch.tensor(X_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(semantic_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(context_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "        input_dim = bert_vectors.shape[1]\n",
        "        self.model = MLPWithContextFeatures(input_dim, len(self.topic_encoder.classes_), len(self.subtopic_encoder.classes_)).to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.l2_lambda)\n",
        "        criterion = nn.HuberLoss(delta=5.0)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        patience_limit = 15\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for xb, topicb, subtopicb, dateb, semanticb, contextb, yb in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                preds = self.model(xb, topicb, subtopicb, dateb, semanticb, contextb).squeeze()\n",
        "                loss = criterion(preds, yb)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            self.model.eval()\n",
        "            val_losses = []\n",
        "            with torch.no_grad():\n",
        "                for xb, topicb, subtopicb, dateb, semanticb, contextb, yb in val_loader:\n",
        "                    preds = self.model(xb, topicb, subtopicb, dateb, semanticb, contextb).squeeze()\n",
        "                    val_losses.append(criterion(preds, yb).item())\n",
        "\n",
        "            val_loss = np.mean(val_losses)\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1} - Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "            if val_loss < best_loss - 1e-4:\n",
        "                best_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                torch.save(self.model.state_dict(), \"snapshots/best_improved_model.pt\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience_limit:\n",
        "                    print(\"Early stopping triggered.\")\n",
        "                    break\n",
        "\n",
        "    def predict(self, articles, bert_path):\n",
        "        articles = enrich_articles(articles)\n",
        "        bert_vectors = torch.load(bert_path, map_location=\"cpu\").numpy()\n",
        "\n",
        "        topics, subtopics = zip(*[extract_topics_from_url(a['url']) for a in articles])\n",
        "        subtopics = [s if s != \"NO_SUBTOPIC\" else \"none\" for s in subtopics]\n",
        "\n",
        "        date_feats = process_date_features(articles)\n",
        "        semantic_flags = np.array([[a['is_politics'], a['is_ukraine_russia'], a['is_israel_palestine'], a['is_sport']] for a in articles], dtype=np.float32)\n",
        "        context_feats = np.array([[a['articles_last_1h'], a['articles_last_6h'], a['articles_last_12h'], a['total_comments_last_6h']] for a in articles], dtype=np.float32)\n",
        "\n",
        "        topic_ids = [self.topic_encoder.classes_[0] if t not in self.topic_encoder.classes_ else t for t in topics]\n",
        "        subtopic_ids = [self.subtopic_encoder.classes_[0] if s not in self.subtopic_encoder.classes_ else s for s in subtopics]\n",
        "\n",
        "        topic_ids = torch.tensor(self.topic_encoder.transform(topic_ids), dtype=torch.long).to(self.device)\n",
        "        subtopic_ids = torch.tensor(self.subtopic_encoder.transform(subtopic_ids), dtype=torch.long).to(self.device)\n",
        "\n",
        "        X = torch.tensor(bert_vectors, dtype=torch.float32).to(self.device)\n",
        "        date_feats = torch.tensor(date_feats, dtype=torch.float32).to(self.device)\n",
        "        semantic_flags = torch.tensor(semantic_flags, dtype=torch.float32).to(self.device)\n",
        "        context_feats = torch.tensor(context_feats, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        self.model.load_state_dict(torch.load(\"snapshots/best_improved_model.pt\"))\n",
        "        self.model.eval()\n",
        "\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for i in tqdm(range(0, len(X), self.batch_size)):\n",
        "                xb = X[i:i+self.batch_size]\n",
        "                topicb = topic_ids[i:i+self.batch_size]\n",
        "                subtopicb = subtopic_ids[i:i+self.batch_size]\n",
        "                dateb = date_feats[i:i+self.batch_size]\n",
        "                semanticb = semantic_flags[i:i+self.batch_size]\n",
        "                contextb = context_feats[i:i+self.batch_size]\n",
        "                preds.append(self.model(xb, topicb, subtopicb, dateb, semanticb, contextb).squeeze())\n",
        "\n",
        "        preds = torch.cat(preds).cpu().numpy()\n",
        "        preds = np.expm1(np.clip(preds, 0, None))\n",
        "        return preds\n",
        "\n",
        "# --- 6. Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    train_articles = load(\"rtvslo_train.json\")\n",
        "    val_articles = load(\"rtvslo_validation.json\")\n",
        "\n",
        "    model = RTVSloImprovedModel()\n",
        "    model.fit(train_articles)\n",
        "\n",
        "    preds_val = model.predict(val_articles, \"sloberta_embeddings_val.pt\")\n",
        "    np.savetxt(\"predictions_val_attention.txt\", preds_val, fmt=\"%.4f\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MORE SEMANTIC FLAGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8dOwTo8xD1N8",
        "outputId": "6f2b675b-5641-4c5e-dcfa-a4cf5083c3e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Training model 1/20...\n",
            "Early stopping at epoch 62\n",
            "\n",
            "üöÄ Training model 2/20...\n",
            "Early stopping at epoch 39\n",
            "\n",
            "üöÄ Training model 3/20...\n",
            "Early stopping at epoch 60\n",
            "\n",
            "üöÄ Training model 4/20...\n",
            "Early stopping at epoch 55\n",
            "\n",
            "üöÄ Training model 5/20...\n",
            "Early stopping at epoch 48\n",
            "\n",
            "üöÄ Training model 6/20...\n",
            "Early stopping at epoch 63\n",
            "\n",
            "üöÄ Training model 7/20...\n",
            "Early stopping at epoch 76\n",
            "\n",
            "üöÄ Training model 8/20...\n",
            "Early stopping at epoch 52\n",
            "\n",
            "üöÄ Training model 9/20...\n",
            "Early stopping at epoch 67\n",
            "\n",
            "üöÄ Training model 10/20...\n",
            "Early stopping at epoch 61\n",
            "\n",
            "üöÄ Training model 11/20...\n",
            "Early stopping at epoch 63\n",
            "\n",
            "üöÄ Training model 12/20...\n",
            "Early stopping at epoch 81\n",
            "\n",
            "üöÄ Training model 13/20...\n",
            "Early stopping at epoch 50\n",
            "\n",
            "üöÄ Training model 14/20...\n",
            "Early stopping at epoch 62\n",
            "\n",
            "üöÄ Training model 15/20...\n",
            "Early stopping at epoch 62\n",
            "\n",
            "üöÄ Training model 16/20...\n",
            "Early stopping at epoch 53\n",
            "\n",
            "üöÄ Training model 17/20...\n",
            "Early stopping at epoch 50\n",
            "\n",
            "üöÄ Training model 18/20...\n",
            "Early stopping at epoch 62\n",
            "\n",
            "üöÄ Training model 19/20...\n",
            "Early stopping at epoch 54\n",
            "\n",
            "üöÄ Training model 20/20...\n",
            "Early stopping at epoch 40\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_ef96d2ee-e7f3-4266-9bfe-9f6725cb833a\", \"final_predictions_val.txt\", 16956)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b5deeefd-9f36-4d39-8745-71d72495a0da\", \"final_predictions_test.txt\", 16956)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Done!\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Imports ---\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# --- 2. Utility Functions ---\n",
        "def load(fn):\n",
        "    with open(fn, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def process_date_features(articles):\n",
        "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
        "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
        "    day_of_week = np.array([a['day_of_week'] for a in articles])\n",
        "    hour = np.array([a['hour'] for a in articles])\n",
        "\n",
        "    year_scaler = StandardScaler()\n",
        "    month_scaler = StandardScaler()\n",
        "    years_scaled = year_scaler.fit_transform(years)\n",
        "    months_scaled = month_scaler.fit_transform(months)\n",
        "\n",
        "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
        "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
        "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
        "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
        "\n",
        "    return np.concatenate([years_scaled, months_scaled, day_sin[:, None], day_cos[:, None], hour_sin[:, None], hour_cos[:, None]], axis=1)\n",
        "\n",
        "def extract_topics_from_url(url):\n",
        "    parts = url.split('/')\n",
        "    topic = parts[3] if len(parts) > 3 else 'none'\n",
        "    subtopic = parts[4] if len(parts) > 4 else 'none'\n",
        "    return topic, subtopic\n",
        "\n",
        "def assign_semantic_cluster_features(articles):\n",
        "    cluster_keywords = {\n",
        "        \"israel_conflict\": ['izraelski', 'gaza', 'palestinec', 'hamas', 'izrael', 'vojna'],\n",
        "        \"sports_general\": ['tekma', 'liga', 'olimpija', 'prvenstvo', 'gol', 'ko≈°arka', 'nogomet', 'reprezentanca'],\n",
        "        \"crime\": ['policija', 'kazniv', 'sodi≈°ƒçe', 'napad', 'dejanje', 'zapor'],\n",
        "        \"culture\": ['muzej', 'kultura', 'umetnost', 'razstava', 'galerija', 'film', 'glasba'],\n",
        "        \"cycling\": ['dirka', 'etapa', 'kolesar', 'pogaƒçar', 'rogliƒç'],\n",
        "        \"economy\": ['zakon', 'evro', 'ministrstvo', 'vlada', 'gospodarstvo', 'trg', 'energija'],\n",
        "        \"climate_tourism\": ['voda', 'turizem', 'temperatura', 'vreme', 'okolje'],\n",
        "        \"nba_hockey\": ['donƒçiƒá', 'nba', 'dallas', 'hokej', 'rangers'],\n",
        "        \"politics_eu\": ['stranka', 'volitev', 'parlament', 'evropski', 'predsednik'],\n",
        "        \"ukraine_russia\": ['ruski', 'ukrajina', 'rusija', 'zelenski', 'vojna'],\n",
        "        \"trump_related\": ['trump', 'biden', 'epstein', 'predsedni≈°ki'],\n",
        "        \"disasters\": ['po≈æar', 'nesreƒça', 're≈°evalec', 'gasilec', 'letalo'],\n",
        "        \"winter_sports\": ['smuƒçanje', 'skoki', 'pokal', 'olimpijski'],\n",
        "        \"energy_market\": ['elektriƒçen', 'energija', 'avtomobil', 'bencin', 'trg'],\n",
        "        \"arts_music\": ['koncert', 'album', 'pesem', 'glasben', 'festival', 'nagrada'],\n",
        "    }\n",
        "\n",
        "    clusters = list(cluster_keywords.keys())\n",
        "    semantic_features = np.zeros((len(articles), len(clusters)), dtype=np.float32)\n",
        "\n",
        "    for i, article in enumerate(articles):\n",
        "        text = (article.get('title', '') + ' ' + article.get('lead', '')).lower()\n",
        "        for j, (cluster_name, keywords) in enumerate(cluster_keywords.items()):\n",
        "            if any(kw in text for kw in keywords):\n",
        "                semantic_features[i, j] = 1.0\n",
        "                break\n",
        "    return semantic_features\n",
        "\n",
        "# --- 3. Dataset ---\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, bert_vectors, topic_ids, subtopic_ids, date_feats, semantic_feats, targets):\n",
        "        self.X = bert_vectors\n",
        "        self.topic_ids = topic_ids\n",
        "        self.subtopic_ids = subtopic_ids\n",
        "        self.date_feats = date_feats\n",
        "        self.semantic_feats = semantic_feats\n",
        "        self.y = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.topic_ids[idx], self.subtopic_ids[idx], self.date_feats[idx], self.semantic_feats[idx], self.y[idx]\n",
        "\n",
        "# --- 4. Model ---\n",
        "class SemanticMLPBlock(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.mlp(x)\n",
        "\n",
        "class MLPWithImprovedSemantic(nn.Module):\n",
        "    def __init__(self, input_dim, num_topics, num_subtopics, semantic_dim):\n",
        "        super().__init__()\n",
        "        self.topic_embedding = nn.Embedding(num_topics, 16)\n",
        "        self.subtopic_embedding = nn.Embedding(num_subtopics, 24)\n",
        "        self.semantic_mlp = SemanticMLPBlock(semantic_dim)\n",
        "\n",
        "        total_input_dim = input_dim + 16 + 24 + 6 + 8\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(total_input_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "    def forward(self, x_text, topic_ids, subtopic_ids, date_feats, semantic_feats):\n",
        "        topic_embed = self.topic_embedding(topic_ids)\n",
        "        subtopic_embed = self.subtopic_embedding(subtopic_ids)\n",
        "        semantic_out = self.semantic_mlp(semantic_feats)\n",
        "\n",
        "        x = torch.cat([x_text, topic_embed, subtopic_embed, date_feats, semantic_out], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# --- 5. Main Trainer Class ---\n",
        "class RTVSloBERTImproved:\n",
        "    def __init__(self, batch_size=170, epochs=150, learning_rate=1e-4, l2_lambda=1e-3, eval_split=0.05, model_id=0):\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.eval_split = eval_split\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_id = model_id\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        for a in train_data:\n",
        "            dt = pd.to_datetime(a['date'])\n",
        "            a['year'] = dt.year\n",
        "            a['month'] = dt.month\n",
        "            a['day_of_week'] = dt.weekday()\n",
        "            a['hour'] = dt.hour\n",
        "\n",
        "        raw_targets = [a['n_comments'] for a in train_data]\n",
        "        targets = [np.log1p(t) for t in raw_targets]\n",
        "        bert_vectors = torch.load(\"sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n",
        "\n",
        "        topics, subtopics = zip(*[extract_topics_from_url(a['url']) for a in train_data])\n",
        "        subtopics = [s if s != \"NO_SUBTOPIC\" else \"none\" for s in subtopics]\n",
        "\n",
        "        date_feats = process_date_features(train_data)\n",
        "        semantic_feats = assign_semantic_cluster_features(train_data)\n",
        "\n",
        "        self.topic_encoder = LabelEncoder().fit(topics)\n",
        "        self.subtopic_encoder = LabelEncoder().fit(subtopics)\n",
        "\n",
        "        topic_ids = self.topic_encoder.transform(topics)\n",
        "        subtopic_ids = self.subtopic_encoder.transform(subtopics)\n",
        "\n",
        "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, date_train, date_val, semantic_train, semantic_val, y_train, y_val = train_test_split(\n",
        "            bert_vectors, topic_ids, subtopic_ids, date_feats, semantic_feats, targets, test_size=self.eval_split, random_state=42\n",
        "        )\n",
        "\n",
        "        train_dataset = NewsDataset(\n",
        "            torch.tensor(X_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(semantic_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_train, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "        val_dataset = NewsDataset(\n",
        "            torch.tensor(X_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(semantic_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "        input_dim = bert_vectors.shape[1]\n",
        "        semantic_dim = semantic_feats.shape[1]\n",
        "        self.model = MLPWithImprovedSemantic(input_dim, len(self.topic_encoder.classes_), len(self.subtopic_encoder.classes_), semantic_dim).to(self.device)\n",
        "\n",
        "        self._train(self.model, train_loader, val_loader)\n",
        "\n",
        "    def _train(self, model, train_loader, val_loader):\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate, weight_decay=self.l2_lambda)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "        criterion = nn.HuberLoss(delta=5.0)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        patience_limit = 15\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            model.train()\n",
        "            for x_batch, topic_ids, subtopic_ids, date_feats, semantic_feats, y_batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats, semantic_feats).squeeze()\n",
        "                loss = criterion(y_pred, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            val_losses = []\n",
        "            with torch.no_grad():\n",
        "                for x_batch, topic_ids, subtopic_ids, date_feats, semantic_feats, y_batch in val_loader:\n",
        "                    y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats, semantic_feats).squeeze()\n",
        "                    val_losses.append(criterion(y_pred, y_batch).item())\n",
        "\n",
        "            val_loss = np.mean(val_losses)\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss - 1e-4:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                best_model_state = model.state_dict()\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience_limit:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        os.makedirs(\"snapshots\", exist_ok=True)\n",
        "        torch.save(best_model_state, f\"snapshots/model_{self.model_id:03d}.pt\")\n",
        "\n",
        "    def predict(self, test_data, bert_vectors):\n",
        "        for a in test_data:\n",
        "            dt = pd.to_datetime(a['date'])\n",
        "            a['year'] = dt.year\n",
        "            a['month'] = dt.month\n",
        "            a['day_of_week'] = dt.weekday()\n",
        "            a['hour'] = dt.hour\n",
        "\n",
        "        topics, subtopics = zip(*[extract_topics_from_url(a['url']) for a in test_data])\n",
        "        subtopics = [s if s != \"NO_SUBTOPIC\" else \"none\" for s in subtopics]\n",
        "\n",
        "        date_feats = process_date_features(test_data)\n",
        "        semantic_feats = assign_semantic_cluster_features(test_data)\n",
        "\n",
        "        topic_ids = [self.topic_encoder.classes_[0] if t not in self.topic_encoder.classes_ else t for t in topics]\n",
        "        subtopic_ids = [self.subtopic_encoder.classes_[0] if s not in self.subtopic_encoder.classes_ else s for s in subtopics]\n",
        "\n",
        "        topic_ids = torch.tensor(self.topic_encoder.transform(topic_ids), dtype=torch.long).to(self.device)\n",
        "        subtopic_ids = torch.tensor(self.subtopic_encoder.transform(subtopic_ids), dtype=torch.long).to(self.device)\n",
        "\n",
        "        X = torch.tensor(bert_vectors, dtype=torch.float32).to(self.device)\n",
        "        date_feats = torch.tensor(date_feats, dtype=torch.float32).to(self.device)\n",
        "        semantic_feats = torch.tensor(semantic_feats, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = self.model(X, topic_ids, subtopic_ids, date_feats, semantic_feats).squeeze().cpu().numpy()\n",
        "            return np.clip(np.expm1(preds), 0, None)\n",
        "\n",
        "# --- 6. Main Loop (Ensemble) ---\n",
        "if __name__ == '__main__':\n",
        "    train = load(\"rtvslo_train.json\")\n",
        "    validation = load(\"rtvslo_validation.json\")\n",
        "    test = load(\"rtvslo_test.json\")\n",
        "\n",
        "    bert_vectors_validation = torch.load(\"sloberta_embeddings_val.pt\", map_location=\"cpu\").numpy()\n",
        "    bert_vectors_test = torch.load(\"sloberta_embeddings_final.pt\", map_location=\"cpu\").numpy()\n",
        "\n",
        "    preds_validation = []\n",
        "    preds_test = []\n",
        "\n",
        "    for seed in range(20):\n",
        "        print(f\"\\nüöÄ Training model {seed+1}/20...\")\n",
        "        model = RTVSloBERTImproved(model_id=seed)\n",
        "        model.fit(train)\n",
        "\n",
        "        val_preds = model.predict(validation, bert_vectors_validation)\n",
        "        test_preds = model.predict(test, bert_vectors_test)\n",
        "\n",
        "        preds_validation.append(val_preds)\n",
        "        preds_test.append(test_preds)\n",
        "\n",
        "    final_val_preds = np.median(preds_validation, axis=0)\n",
        "    final_test_preds = np.median(preds_test, axis=0)\n",
        "\n",
        "    np.savetxt(\"final_predictions_val.txt\", final_val_preds, fmt=\"%.4f\")\n",
        "    np.savetxt(\"final_predictions_test.txt\", final_test_preds, fmt=\"%.4f\")\n",
        "\n",
        "    files.download(\"final_predictions_val.txt\")\n",
        "    files.download(\"final_predictions_test.txt\")\n",
        "\n",
        "    print(\"‚úÖ Done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AVERAGE?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Imports ---\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# --- 2. Utility Functions ---\n",
        "def load(fn):\n",
        "    with open(fn, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def process_date_features(articles):\n",
        "    years = np.array([a['year'] for a in articles]).reshape(-1, 1)\n",
        "    months = np.array([a['month'] for a in articles]).reshape(-1, 1)\n",
        "    day_of_week = np.array([a['day_of_week'] for a in articles])\n",
        "    hour = np.array([a['hour'] for a in articles])\n",
        "\n",
        "    year_scaler = StandardScaler()\n",
        "    month_scaler = StandardScaler()\n",
        "    years_scaled = year_scaler.fit_transform(years)\n",
        "    months_scaled = month_scaler.fit_transform(months)\n",
        "\n",
        "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
        "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
        "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
        "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
        "\n",
        "    return np.concatenate([years_scaled, months_scaled, day_sin[:, None], day_cos[:, None], hour_sin[:, None], hour_cos[:, None]], axis=1)\n",
        "\n",
        "def extract_topics_from_url(url):\n",
        "    parts = url.split('/')\n",
        "    topic = parts[3] if len(parts) > 3 else 'none'\n",
        "    subtopic = parts[4] if len(parts) > 4 else 'none'\n",
        "    return topic, subtopic\n",
        "\n",
        "def assign_semantic_cluster_features(articles):\n",
        "    cluster_keywords = {\n",
        "        \"israel_conflict\": ['izraelski', 'gaza', 'palestinec', 'hamas', 'izrael', 'vojna'],\n",
        "        \"sports_general\": ['tekma', 'liga', 'olimpija', 'prvenstvo', 'gol', 'ko≈°arka', 'nogomet', 'reprezentanca'],\n",
        "        \"crime\": ['policija', 'kazniv', 'sodi≈°ƒçe', 'napad', 'dejanje', 'zapor'],\n",
        "        \"culture\": ['muzej', 'kultura', 'umetnost', 'razstava', 'galerija', 'film', 'glasba'],\n",
        "        \"cycling\": ['dirka', 'etapa', 'kolesar', 'pogaƒçar', 'rogliƒç'],\n",
        "        \"economy\": ['zakon', 'evro', 'ministrstvo', 'vlada', 'gospodarstvo', 'trg', 'energija'],\n",
        "        \"climate_tourism\": ['voda', 'turizem', 'temperatura', 'vreme', 'okolje'],\n",
        "        \"nba_hockey\": ['donƒçiƒá', 'nba', 'dallas', 'hokej', 'rangers'],\n",
        "        \"politics_eu\": ['stranka', 'volitev', 'parlament', 'evropski', 'predsednik'],\n",
        "        \"ukraine_russia\": ['ruski', 'ukrajina', 'rusija', 'zelenski', 'vojna'],\n",
        "        \"trump_related\": ['trump', 'biden', 'epstein', 'predsedni≈°ki'],\n",
        "        \"disasters\": ['po≈æar', 'nesreƒça', 're≈°evalec', 'gasilec', 'letalo'],\n",
        "        \"winter_sports\": ['smuƒçanje', 'skoki', 'pokal', 'olimpijski'],\n",
        "        \"energy_market\": ['elektriƒçen', 'energija', 'avtomobil', 'bencin', 'trg'],\n",
        "        \"arts_music\": ['koncert', 'album', 'pesem', 'glasben', 'festival', 'nagrada'],\n",
        "    }\n",
        "\n",
        "    clusters = list(cluster_keywords.keys())\n",
        "    semantic_features = np.zeros((len(articles), len(clusters)), dtype=np.float32)\n",
        "\n",
        "    for i, article in enumerate(articles):\n",
        "        text = (article.get('title', '') + ' ' + article.get('lead', '')).lower()\n",
        "        for j, (cluster_name, keywords) in enumerate(cluster_keywords.items()):\n",
        "            if any(kw in text for kw in keywords):\n",
        "                semantic_features[i, j] = 1.0\n",
        "                break\n",
        "    return semantic_features\n",
        "\n",
        "# --- 3. Dataset ---\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, bert_vectors, topic_ids, subtopic_ids, date_feats, semantic_feats, targets):\n",
        "        self.X = bert_vectors\n",
        "        self.topic_ids = topic_ids\n",
        "        self.subtopic_ids = subtopic_ids\n",
        "        self.date_feats = date_feats\n",
        "        self.semantic_feats = semantic_feats\n",
        "        self.y = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.topic_ids[idx], self.subtopic_ids[idx], self.date_feats[idx], self.semantic_feats[idx], self.y[idx]\n",
        "\n",
        "# --- 4. Model ---\n",
        "class SemanticMLPBlock(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.mlp(x)\n",
        "\n",
        "class MLPWithImprovedSemantic(nn.Module):\n",
        "    def __init__(self, input_dim, num_topics, num_subtopics, semantic_dim):\n",
        "        super().__init__()\n",
        "        self.topic_embedding = nn.Embedding(num_topics, 16)\n",
        "        self.subtopic_embedding = nn.Embedding(num_subtopics, 24)\n",
        "        self.semantic_mlp = SemanticMLPBlock(semantic_dim)\n",
        "\n",
        "        total_input_dim = input_dim + 16 + 24 + 6 + 8  # BERT + topic + subtopic + date + semantic compressed\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(total_input_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "    def forward(self, x_text, topic_ids, subtopic_ids, date_feats, semantic_feats):\n",
        "        topic_embed = self.topic_embedding(topic_ids)\n",
        "        subtopic_embed = self.subtopic_embedding(subtopic_ids)\n",
        "        semantic_out = self.semantic_mlp(semantic_feats)\n",
        "\n",
        "        x = torch.cat([x_text, topic_embed, subtopic_embed, date_feats, semantic_out], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# --- 5. Trainer Class ---\n",
        "class RTVSloBERTImproved:\n",
        "    def __init__(self, batch_size=170, epochs=150, learning_rate=1e-4, l2_lambda=1e-3, eval_split=0.05, model_id=0):\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.eval_split = eval_split\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_id = model_id\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        for a in train_data:\n",
        "            dt = pd.to_datetime(a['date'])\n",
        "            a['year'] = dt.year\n",
        "            a['month'] = dt.month\n",
        "            a['day_of_week'] = dt.weekday()\n",
        "            a['hour'] = dt.hour\n",
        "\n",
        "        raw_targets = [a['n_comments'] for a in train_data]\n",
        "        targets = [np.log1p(t) for t in raw_targets]\n",
        "        bert_vectors = torch.load(\"sloberta_embeddings.pt\", map_location=\"cpu\").numpy()\n",
        "\n",
        "        topics, subtopics = zip(*[extract_topics_from_url(a['url']) for a in train_data])\n",
        "        subtopics = [s if s != \"NO_SUBTOPIC\" else \"none\" for s in subtopics]\n",
        "\n",
        "        date_feats = process_date_features(train_data)\n",
        "        semantic_feats = assign_semantic_cluster_features(train_data)\n",
        "\n",
        "        self.topic_encoder = LabelEncoder().fit(topics)\n",
        "        self.subtopic_encoder = LabelEncoder().fit(subtopics)\n",
        "\n",
        "        topic_ids = self.topic_encoder.transform(topics)\n",
        "        subtopic_ids = self.subtopic_encoder.transform(subtopics)\n",
        "\n",
        "        X_train, X_val, topic_train, topic_val, subtopic_train, subtopic_val, date_train, date_val, semantic_train, semantic_val, y_train, y_val = train_test_split(\n",
        "            bert_vectors, topic_ids, subtopic_ids, date_feats, semantic_feats, targets, test_size=self.eval_split, random_state=42\n",
        "        )\n",
        "\n",
        "        train_dataset = NewsDataset(\n",
        "            torch.tensor(X_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_train, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(semantic_train, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_train, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "        val_dataset = NewsDataset(\n",
        "            torch.tensor(X_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(topic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(subtopic_val, dtype=torch.long).to(self.device),\n",
        "            torch.tensor(date_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(semantic_val, dtype=torch.float32).to(self.device),\n",
        "            torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "        input_dim = bert_vectors.shape[1]\n",
        "        semantic_dim = semantic_feats.shape[1]\n",
        "        self.model = MLPWithImprovedSemantic(input_dim, len(self.topic_encoder.classes_), len(self.subtopic_encoder.classes_), semantic_dim).to(self.device)\n",
        "\n",
        "        self._train(self.model, train_loader, val_loader)\n",
        "\n",
        "    def _train(self, model, train_loader, val_loader):\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate, weight_decay=self.l2_lambda)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "        criterion = nn.HuberLoss(delta=5.0)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        patience_limit = 15\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            model.train()\n",
        "            for x_batch, topic_ids, subtopic_ids, date_feats, semantic_feats, y_batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats, semantic_feats).squeeze()\n",
        "                loss = criterion(y_pred, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            val_losses = []\n",
        "            with torch.no_grad():\n",
        "                for x_batch, topic_ids, subtopic_ids, date_feats, semantic_feats, y_batch in val_loader:\n",
        "                    y_pred = model(x_batch, topic_ids, subtopic_ids, date_feats, semantic_feats).squeeze()\n",
        "                    val_losses.append(criterion(y_pred, y_batch).item())\n",
        "\n",
        "            val_loss = np.mean(val_losses)\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss - 1e-4:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                best_model_state = model.state_dict()\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience_limit:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        os.makedirs(\"snapshots\", exist_ok=True)\n",
        "        torch.save(best_model_state, f\"snapshots/model_{self.model_id:03d}.pt\")\n",
        "\n",
        "    def predict(self, test_data, bert_vectors):\n",
        "        for a in test_data:\n",
        "            dt = pd.to_datetime(a['date'])\n",
        "            a['year'] = dt.year\n",
        "            a['month'] = dt.month\n",
        "            a['day_of_week'] = dt.weekday()\n",
        "            a['hour'] = dt.hour\n",
        "\n",
        "        topics, subtopics = zip(*[extract_topics_from_url(a['url']) for a in test_data])\n",
        "        subtopics = [s if s != \"NO_SUBTOPIC\" else \"none\" for s in subtopics]\n",
        "\n",
        "        date_feats = process_date_features(test_data)\n",
        "        semantic_feats = assign_semantic_cluster_features(test_data)\n",
        "\n",
        "        topic_ids = [self.topic_encoder.classes_[0] if t not in self.topic_encoder.classes_ else t for t in topics]\n",
        "        subtopic_ids = [self.subtopic_encoder.classes_[0] if s not in self.subtopic_encoder.classes_ else s for s in subtopics]\n",
        "\n",
        "        topic_ids = torch.tensor(self.topic_encoder.transform(topic_ids), dtype=torch.long).to(self.device)\n",
        "        subtopic_ids = torch.tensor(self.subtopic_encoder.transform(subtopic_ids), dtype=torch.long).to(self.device)\n",
        "\n",
        "        X = torch.tensor(bert_vectors, dtype=torch.float32).to(self.device)\n",
        "        date_feats = torch.tensor(date_feats, dtype=torch.float32).to(self.device)\n",
        "        semantic_feats = torch.tensor(semantic_feats, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = self.model(X, topic_ids, subtopic_ids, date_feats, semantic_feats).squeeze().cpu().numpy()\n",
        "            return np.clip(np.expm1(preds), 0, None)\n",
        "\n",
        "# --- 6. Main Ensemble Loop ---\n",
        "if __name__ == '__main__':\n",
        "    train = load(\"rtvslo_train.json\")\n",
        "    validation = load(\"rtvslo_validation.json\")\n",
        "    test = load(\"rtvslo_test.json\")\n",
        "\n",
        "    bert_vectors_validation = torch.load(\"sloberta_embeddings_val.pt\", map_location=\"cpu\").numpy()\n",
        "    bert_vectors_test = torch.load(\"sloberta_embeddings_final.pt\", map_location=\"cpu\").numpy()\n",
        "\n",
        "    preds_validation = []\n",
        "    preds_test = []\n",
        "\n",
        "    for seed in range(20):\n",
        "        print(f\"\\nüöÄ Training model {seed+1}/20...\")\n",
        "        model = RTVSloBERTImproved(model_id=seed)\n",
        "        model.fit(train)\n",
        "\n",
        "        val_preds = model.predict(validation, bert_vectors_validation)\n",
        "        test_preds = model.predict(test, bert_vectors_test)\n",
        "\n",
        "        preds_validation.append(val_preds)\n",
        "        preds_test.append(test_preds)\n",
        "\n",
        "    final_val_preds = np.median(preds_validation, axis=0)\n",
        "    final_test_preds = np.median(preds_test, axis=0)\n",
        "\n",
        "    np.savetxt(\"final_predictions_val.txt\", final_val_preds, fmt=\"%.4f\")\n",
        "    np.savetxt(\"final_predictions_test.txt\", final_test_preds, fmt=\"%.4f\")\n",
        "\n",
        "    files.download(\"final_predictions_val.txt\")\n",
        "    files.download(\"final_predictions_test.txt\")\n",
        "\n",
        "    print(\"‚úÖ Done!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
